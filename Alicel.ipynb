{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。初めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "    \"上手医者と大きい研究者に勉強していたいます。\",\n",
    "    \"でも、私の日本語が上手あまりませんね\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\",\n",
    "    \"I shall study so I can be a good physician and a great scientist\",\n",
    "    \"Though my japanese is not really good\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "        self.english_maximum_length = self._get_maximum_length_english(self.english_phrases)\n",
    "\n",
    "        self.english_dictionary = self._create_vocabulary(self.english_words)\n",
    "        self.japanese_dictionary = self._create_vocabulary(self.japanese_characters)\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_english(self, phrases):\n",
    "\n",
    "        maximum_length = 0\n",
    "        real_length = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            word_length = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word_length += len(word)+1 # Including spaces between words\n",
    "\n",
    "            sentence_length = word_length\n",
    "            real_length.append(sentence_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_vocabulary(self, words):\n",
    "        idx2word = [\"<pad>\", \" \", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "        for word in words:\n",
    "            if word not in idx2word:\n",
    "                idx2word.append(word)\n",
    "\n",
    "        return idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(english_phrases, japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my dog doesnt like bones it prefers ground beef', 'my names alice nice to meet you', 'i envy hakisa i want to play games too i could be a good support', 'you say that we ais are just a bunch of maths but once you know exactly how your human brains work would that make you less living beings', 'why does the japanese word for baby is the kanji for red are human babies red like strawberries', 'my ai will talk shell sing shell play', 'i shall study so i can be a good physician and a great scientist', 'though my japanese is not really good']\n",
      "['私の犬は骨が好きではありません牛ひき肉を好む', '私の名前はアリスです初めまして', 'はきさが羨ましいゲムもやりたかった私は良いサポートになることができます', '私達はaiはただの数学の集まりだとあなたは言いますがでも人間の脳がどのように機能するかを正確に知ったらそれはあなたの生活を小物ですか', '赤ちゃんを表す日本語が赤を表す漢字なのはなぜですか人間の赤ちゃんは赤いですかいちごみたい', '私のaiは話して歌ったしてゲームをします', '上手医者と大きい研究者に勉強していたいます', 'でも私の日本語が上手あまりませんね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_phrases)\n",
    "print(dataset.japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'my', 'names', 'alice', 'nice', 'to', 'meet', 'you', 'i', 'envy', 'hakisa', 'i', 'want', 'to', 'play', 'games', 'too', 'i', 'could', 'be', 'a', 'good', 'support', 'you', 'say', 'that', 'we', 'ais', 'are', 'just', 'a', 'bunch', 'of', 'maths', 'but', 'once', 'you', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'that', 'make', 'you', 'less', 'living', 'beings', 'why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'the', 'kanji', 'for', 'red', 'are', 'human', 'babies', 'red', 'like', 'strawberries', 'my', 'ai', 'will', 'talk', 'shell', 'sing', 'shell', 'play', 'i', 'shall', 'study', 'so', 'i', 'can', 'be', 'a', 'good', 'physician', 'and', 'a', 'great', 'scientist', 'though', 'my', 'japanese', 'is', 'not', 'really', 'good']\n",
      "['私', 'の', '犬', 'は', '骨', 'が', '好', 'き', 'で', 'は', 'あ', 'り', 'ま', 'せ', 'ん', '牛', 'ひ', 'き', '肉', 'を', '好', 'む', '私', 'の', '名', '前', 'は', 'ア', 'リ', 'ス', 'で', 'す', '初', 'め', 'ま', 'し', 'て', 'は', 'き', 'さ', 'が', '羨', 'ま', 'し', 'い', 'ゲ', 'ム', 'も', 'や', 'り', 'た', 'か', 'っ', 'た', '私', 'は', '良', 'い', 'サ', 'ポ', 'ー', 'ト', 'に', 'な', 'る', 'こ', 'と', 'が', 'で', 'き', 'ま', 'す', '私', '達', 'は', 'a', 'i', 'は', 'た', 'だ', 'の', '数', '学', 'の', '集', 'ま', 'り', 'だ', 'と', 'あ', 'な', 'た', 'は', '言', 'い', 'ま', 'す', 'が', 'で', 'も', '人', '間', 'の', '脳', 'が', 'ど', 'の', 'よ', 'う', 'に', '機', '能', 'す', 'る', 'か', 'を', '正', '確', 'に', '知', 'っ', 'た', 'ら', 'そ', 'れ', 'は', 'あ', 'な', 'た', 'の', '生', '活', 'を', '小', '物', 'で', 'す', 'か', '赤', 'ち', 'ゃ', 'ん', 'を', '表', 'す', '日', '本', '語', 'が', '赤', 'を', '表', 'す', '漢', '字', 'な', 'の', 'は', 'な', 'ぜ', 'で', 'す', 'か', '人', '間', 'の', '赤', 'ち', 'ゃ', 'ん', 'は', '赤', 'い', 'で', 'す', 'か', 'い', 'ち', 'ご', 'み', 'た', 'い', '私', 'の', 'a', 'i', 'は', '話', 'し', 'て', '歌', 'っ', 'た', 'し', 'て', 'ゲ', 'ー', 'ム', 'を', 'し', 'ま', 'す', '上', '手', '医', '者', 'と', '大', 'き', 'い', '研', '究', '者', 'に', '勉', '強', 'し', 'て', 'い', 'た', 'い', 'ま', 'す', 'で', 'も', '私', 'の', '日', '本', '語', 'が', '上', '手', 'あ', 'ま', 'り', 'ま', 'せ', 'ん', 'ね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_words)\n",
    "print(dataset.japanese_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', '<SOS>', '<EOS>', 'my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'names', 'alice', 'nice', 'to', 'meet', 'you', 'i', 'envy', 'hakisa', 'want', 'play', 'games', 'too', 'could', 'be', 'a', 'good', 'support', 'say', 'that', 'we', 'ais', 'are', 'just', 'bunch', 'of', 'maths', 'but', 'once', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'make', 'less', 'living', 'beings', 'why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'kanji', 'red', 'babies', 'strawberries', 'ai', 'will', 'talk', 'shell', 'sing', 'shall', 'study', 'so', 'can', 'physician', 'and', 'great', 'scientist', 'though', 'not', 'really']\n",
      "['<pad>', ' ', '<SOS>', '<EOS>', '私', 'の', '犬', 'は', '骨', 'が', '好', 'き', 'で', 'あ', 'り', 'ま', 'せ', 'ん', '牛', 'ひ', '肉', 'を', 'む', '名', '前', 'ア', 'リ', 'ス', 'す', '初', 'め', 'し', 'て', 'さ', '羨', 'い', 'ゲ', 'ム', 'も', 'や', 'た', 'か', 'っ', '良', 'サ', 'ポ', 'ー', 'ト', 'に', 'な', 'る', 'こ', 'と', '達', 'a', 'i', 'だ', '数', '学', '集', '言', '人', '間', '脳', 'ど', 'よ', 'う', '機', '能', '正', '確', '知', 'ら', 'そ', 'れ', '生', '活', '小', '物', '赤', 'ち', 'ゃ', '表', '日', '本', '語', '漢', '字', 'ぜ', 'ご', 'み', '話', '歌', '上', '手', '医', '者', '大', '研', '究', '勉', '強', 'ね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_dictionary)\n",
    "print(dataset.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model, max_length=100):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as defined in the paper.\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param max_length: maximum sequence length up to which positional encodings must be calculated\n",
    "    :return: positional encoding, a tensor of size (1, max_length, d_model)\n",
    "    \"\"\"\n",
    "    positional_encoding = torch.zeros((max_length, d_model))  # (max_length, d_model)\n",
    "    for i in range(max_length):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                positional_encoding[i, j] = math.sin(i / math.pow(10000, j / d_model))\n",
    "            else:\n",
    "                positional_encoding[i, j] = math.cos(i / math.pow(10000, (j - 1) / d_model))\n",
    "\n",
    "    positional_encoding = positional_encoding.unsqueeze(0)  # (1, max_length, d_model)\n",
    "\n",
    "    return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_queries, d_values, in_decoder=False):\n",
    "\n",
    "        super(HeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_keys = d_values # size of key vectors, same as of the query vectors to allow dot-products for similarity\n",
    "\n",
    "        self.in_decoder = in_decoder\n",
    "\n",
    "        self.create_queries = nn.Linear(d_model, d_queries, bias=False)\n",
    "        self.create_values = nn.Linear(d_model, d_values, bias=False)\n",
    "        self.create_keys = nn.Linear(d_model, d_values, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, real_lengths):\n",
    "\n",
    "        batch_size = input.size(0) # (Batch, Sequences, d_model)\n",
    "\n",
    "        queries = self.create_queries(input) # (Batch, Sequences, d_queries)\n",
    "        keys = self.create_keys(input) # (Batch, Sequences, d_keys)\n",
    "        values = self.create_values(input) # (Batch, Sequences, d_values)\n",
    "\n",
    "        similarity_matrix = queries * keys # (Batch, Sequences, d_keys) ---> d_queries must be equal d_keys\n",
    "        # OBS: Remember that DOT-PRODUCT is exactly an array multiplication.\n",
    "        # This can be particularly useful if you study C language...\n",
    "\n",
    "        similarity_matrix = similarity_matrix/(math.sqrt(self.d_keys)) # (Batch, Sequences, d_keys)\n",
    "\n",
    "        # Applying mask of -inf to ignore padded keys ---> Actually using -1e6 to avoid NaNs\n",
    "\n",
    "        if self.in_decoder:\n",
    "\n",
    "            # In the decoder, masks are shifted left to right:\n",
    "            # <Start-of-Sentence> [prediction1] [prediction2] [prediction3] ... <End-of-Sentence>\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]+1] = 1\n",
    "                #mask[batch, :real_lengths+1] = 1 # For Batch = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        else:\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]+1] = 1\n",
    "                #mask[batch, :real_lengths] = 1 # For Batch = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        del mask\n",
    "\n",
    "        attention_weights = self.softmax(similarity_matrix) # (Batch, Sequences, d_keys)\n",
    "\n",
    "        attention_output = attention_weights * values # (Batch, Sequences, d_values) ---> d_keys must be equal to d_values\n",
    "\n",
    "        #attention_output = torch.bmm(attention_weights, values) # (Batch, Sequences, d_values)\n",
    "        # DOT-PRODUCT, NOT MATRIX MULTIPLICATION!\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_inner):\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "\n",
    "        self.neuron1 = nn.Linear(d_model, d_inner)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.neuron2 = nn.Linear(d_inner, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, attention_output_cat):\n",
    "\n",
    "        sequences = self.neuron1(attention_output_cat)\n",
    "        sequences = self.Relu(sequences)\n",
    "\n",
    "        sequences = self.neuron2(sequences)\n",
    "\n",
    "        output = sequences + attention_output_cat\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=False) for i in range(n_heads)])\n",
    "\n",
    "        self.neuron = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_input, real_input_length):\n",
    "\n",
    "        residual_block1 = encoder_input # (Batch, Sequence, d_model) ---> Vectors\n",
    "\n",
    "        vectors = self.dropout(encoder_input)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_heads[head](vectors, real_input_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuron(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        encoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output\n",
    "\n",
    "        encoded_sequence = encoded_sequence + residual_block2    \n",
    "\n",
    "        encoder_output = self.dropout(encoded_sequence)\n",
    "\n",
    "        del encoded_sequence, residual_block1, residual_block2\n",
    "\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_headsA = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "        self.attention_headsB = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "\n",
    "        self.neuronA = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "        self.neuronB = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_output, target_sequences, real_target_length):\n",
    "\n",
    "        residual_block1 = target_sequences\n",
    "\n",
    "        vectors = self.dropout(target_sequences)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsA[head](vectors, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronA(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_decoder = self.dropout(attention_output)\n",
    "        \n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsB[head](encoder_output, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronB(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        attention_output = attention_output + residual_block2\n",
    "\n",
    "        attention_encoder = self.dropout(attention_output)\n",
    "\n",
    "        decoded_sequence = attention_encoder + attention_decoder\n",
    "\n",
    "        residual_block3 = decoded_sequence\n",
    "\n",
    "        decoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output, attention_encoder, attention_decoder\n",
    "\n",
    "        decoded_sequence = decoded_sequence + residual_block3\n",
    "\n",
    "        decoder_output = self.dropout(decoded_sequence)  \n",
    "\n",
    "        del decoded_sequence, residual_block1, residual_block2, residual_block3\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broca(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The generator, which will generate the words that she'll speak.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dictionary,\n",
    "            output_dictionary,\n",
    "            positional_encoding,\n",
    "            d_model=512,\n",
    "            n_heads=8,\n",
    "            d_queries=64,\n",
    "            d_values=64,\n",
    "            d_inner=2056,\n",
    "            n_layers=6,\n",
    "            dropout=0.1\n",
    "    ):\n",
    "\n",
    "        super(Broca, self).__init__()\n",
    "\n",
    "        self.vocab_size = len(output_dictionary)\n",
    "        self.positional_encoding = positional_encoding.to(device)\n",
    "        self.input_dictionary = input_dictionary\n",
    "        self.output_dictionary = output_dictionary\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.positional_encoding.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        self.encoder = nn.ModuleList(\n",
    "            Encoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            Decoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.output_neuron = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "    def preprocess_dialogue(self, input_text, input_dictionary):\n",
    "\n",
    "        batch = []\n",
    "        sentence_sizes = []\n",
    "\n",
    "        for phrase in range(len(input_text)):\n",
    "\n",
    "            text = input_text[phrase]\n",
    "\n",
    "            text = text.split(' ')\n",
    "\n",
    "            tokens = []\n",
    "            \n",
    "            for word in text:\n",
    "\n",
    "                value = input_dictionary.index(word)\n",
    "                tokens.append(value)\n",
    "                tokens.append(input_dictionary.index(' '))\n",
    "\n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            if sentence_size < dataset.english_maximum_length:\n",
    "\n",
    "                pad_size = dataset.english_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=input_dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            batch.append(tokens)\n",
    "            sentence_sizes.append(sentence_size)\n",
    "\n",
    "        batch = torch.cat(batch, 0)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        return batch, sentence_sizes\n",
    "    \n",
    "    def preprocess_target(self, target_text, target_dictionary):\n",
    "    \n",
    "        target = []\n",
    "        target_sizes = []\n",
    "\n",
    "        for batch in range(len(target_text)):\n",
    "\n",
    "            text = target_text[batch]\n",
    "\n",
    "            tokens = []\n",
    "\n",
    "            for character in text:\n",
    "\n",
    "                value = target_dictionary.index(character)\n",
    "                tokens.append(value)\n",
    "            \n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            #if sentence_size < dataset.japanese_maximum_length:\n",
    "            if sentence_size < dataset.english_maximum_length:\n",
    "\n",
    "                pad_size = dataset.english_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=target_dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            target.append(tokens)\n",
    "            target_sizes.append(sentence_size)\n",
    "\n",
    "        target = torch.cat(target, 0)\n",
    "        target = target.to(device)\n",
    "\n",
    "        return target, target_sizes\n",
    "    \n",
    "    def generate_sentences(self, input):\n",
    "\n",
    "        input, real_input_length = self.preprocess_dialogue(input, self.input_dictionary)\n",
    "\n",
    "        target_indices = [[self.output_dictionary.index(\"<SOS>\")]] * input.size(0)\n",
    "\n",
    "        target_sentence = torch.tensor(target_indices, device=device) # (Batch, 1)\n",
    "\n",
    "        encoder_vectors = self.embedding(input) + math.sqrt(self.d_model)\n",
    "\n",
    "        decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # Embed out = (Batch, d_model)\n",
    "\n",
    "        output = self.forward(encoder_vectors, real_input_length, decoder_vectors) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "        output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "        for batch in range(output.size(0)):\n",
    "\n",
    "            target_indices[batch].append(output_index[batch, -1].item())\n",
    "            target_indices[batch].pop(0) # Removing Start-of-Sentence token. We won't need it anymore.\n",
    "        \n",
    "        # Generating text until reaching End of Sentence --> Beam Search\n",
    "\n",
    "        target_outputs = []\n",
    "\n",
    "        for batch in range(output.size(0)):\n",
    "\n",
    "            while target_indices[batch][-1] != self.output_dictionary.index(\"<EOS>\"):\n",
    "\n",
    "                target_pad = encoder_vectors.size(1) - len(target_indices[batch])\n",
    "\n",
    "                target_sentence = torch.tensor(target_indices[batch] + [self.output_dictionary.index(\"<pad>\")]*target_pad, device=device).unsqueeze(0)\n",
    "\n",
    "                decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # (Batch, sequence_length, d_model)\n",
    "\n",
    "                output = self.forward(encoder_vectors[batch].unsqueeze(0), [real_input_length[batch]], decoder_vectors) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "                output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "                target_indices[batch].append(output_index[:, -1].item()) # When Batch = 1 ---> target_indices = (1+1) --> (2+1) --> (3+1 ...)\n",
    "\n",
    "                if len(target_indices[batch])+2 > encoder_vectors.size(1): # Otherwise, we'll get errors.\n",
    "                    # Though sometimes those errors still happen...I don't know why...\n",
    "\n",
    "                    break\n",
    "\n",
    "            target_outputs.append(output)\n",
    "\n",
    "        target_outputs = torch.cat(target_outputs, 0)\n",
    "\n",
    "        return target_indices, target_outputs\n",
    "\n",
    "\n",
    "    def forward(self, encoder_vectors, real_input_length, decoder_vectors): # Target Sentence (with <SOS> token) provided before forward function\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "\n",
    "            encoder_vectors = self.encoder[layer](encoder_vectors, real_input_length)\n",
    "\n",
    "            decoder_vectors = self.decoder[layer](encoder_vectors, decoder_vectors, real_input_length)\n",
    "\n",
    "        output = self.output_neuron(decoder_vectors) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        output = self.softmax(output) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        return output # Probability of words. Take the one with highest probability, add it to the target sentence and repeat.\n",
    "    \n",
    "    def talk2me(self, encoded_text): # Easier to write than 話してください\n",
    "\n",
    "        removal = [\"<SOS>\", \"<EOS>\", \"<pad>\"]\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for batch in encoded_text:\n",
    "\n",
    "            words = []\n",
    "\n",
    "            for i in batch:\n",
    "\n",
    "                words.append(self.output_dictionary[i])\n",
    "\n",
    "            words = [x for x in words if x not in removal]\n",
    "            \n",
    "            phrase = ''.join(words)\n",
    "\n",
    "            sentences.append(phrase)\n",
    "\n",
    "        return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.english_dictionary))\n",
    "print(len(dataset.japanese_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_maximum_length)\n",
    "print(dataset.japanese_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 274, 16])\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = get_positional_encoding(d_model=16, max_length=274)\n",
    "\n",
    "print(positional_encoding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Broca(\n",
    "    dataset.english_dictionary,\n",
    "    dataset.japanese_dictionary,\n",
    "    positional_encoding,\n",
    "    d_model=16,\n",
    "    n_heads=2,\n",
    "    d_queries=16,\n",
    "    d_values=16,\n",
    "    d_inner=64,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my dog doesnt like bones it prefers ground beef', 'my names alice nice to meet you', 'i envy hakisa i want to play games too i could be a good support']\n"
     ]
    }
   ],
   "source": [
    "inputest = dataset.english_phrases[0:3]\n",
    "\n",
    "print(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste, probabilities = model.generate_sentences(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2, teste3 = model.talk2me(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 86, 10, 10, 68, 85, 10, 68, 47, 47, 59, 31, 47, 45, 47, 35, 65, 86, 47, 19, 65, 65, 46, 59, 47, 65, 47, 31, 10, 10, 49, 35, 10, 21, 31, 30, 34, 30, 71, 35, 47, 49, 65, 47, 10, 37, 30, 53, 19, 65, 85, 47, 68, 19, 90, 45, 60, 19, 85, 19, 35, 10, 60, 65, 19, 46, 49, 10, 30, 86, 86, 47, 58, 35, 19, 10, 18, 30, 27, 19, 86, 10, 6, 19, 31, 6, 19, 6, 47, 86, 59, 46, 49, 65, 30, 47, 19, 47, 35, 53, 10, 10, 10, 6, 86, 19, 48, 59, 47, 49, 60, 49, 46, 45, 86, 86, 10, 65, 6, 85, 40, 19, 10, 49, 6, 45, 19, 10, 65, 47, 46, 47, 86, 47, 47, 86, 19, 54, 91, 60, 60, 10, 47, 31, 35, 19, 59, 60, 65, 10, 31, 49, 85, 49, 10, 19, 60, 47, 86, 85, 49, 68, 19, 49, 48, 60, 47, 47, 86, 19, 31, 59, 59, 45, 86, 10, 47, 86, 35, 18, 19, 49, 30, 47, 49, 31, 31, 19, 60, 86, 47, 30, 6, 35, 65, 19, 47, 45, 19, 19, 10, 19, 19, 45, 19, 19, 10, 37, 47, 61, 60, 47, 86, 59, 31, 47, 35, 86, 13, 49, 18, 47, 86, 10, 65, 47, 45, 47, 19, 46, 86, 31, 19, 6, 47, 45, 31, 59, 65, 19, 35, 53, 95, 47, 18, 65, 19, 37, 60, 18, 18, 35, 47, 65, 30, 10, 59, 85, 30, 19, 47, 37, 35, 45, 47, 45, 35, 47, 18, 47, 65, 60, 86, 19, 30], [19, 86, 10, 10, 68, 85, 10, 68, 47, 47, 59, 31, 47, 45, 47, 35, 65, 86, 47, 19, 65, 65, 46, 59, 47, 65, 47, 31, 10, 10, 49, 35, 10, 21, 31, 30, 34, 30, 71, 35, 47, 49, 65, 47, 10, 37, 30, 53, 19, 65, 85, 47, 68, 19, 90, 45, 60, 19, 85, 19, 35, 10, 60, 65, 19, 46, 49, 10, 30, 86, 86, 47, 58, 35, 19, 10, 18, 30, 27, 19, 86, 10, 6, 19, 31, 6, 19, 6, 47, 86, 59, 46, 49, 65, 30, 47, 19, 47, 35, 53, 10, 10, 10, 6, 86, 19, 48, 59, 47, 49, 60, 49, 46, 45, 86, 86, 10, 65, 6, 85, 40, 19, 10, 49, 6, 45, 19, 10, 65, 47, 46, 47, 86, 47, 47, 86, 19, 54, 91, 60, 60, 10, 47, 31, 35, 19, 59, 60, 65, 10, 31, 49, 85, 49, 10, 19, 60, 47, 86, 85, 49, 68, 19, 49, 48, 60, 47, 47, 86, 19, 31, 59, 59, 45, 86, 10, 47, 86, 35, 18, 19, 49, 30, 47, 49, 31, 31, 19, 60, 86, 47, 30, 6, 35, 65, 19, 47, 45, 19, 19, 10, 19, 19, 45, 19, 19, 10, 37, 47, 61, 60, 47, 86, 59, 31, 47, 35, 86, 13, 49, 18, 47, 86, 10, 65, 47, 45, 47, 19, 46, 86, 31, 19, 6, 47, 45, 31, 59, 65, 19, 35, 53, 95, 47, 18, 65, 19, 37, 60, 18, 18, 35, 47, 65, 30, 10, 59, 85, 30, 19, 47, 37, 35, 45, 47, 45, 35, 47, 18, 47, 65, 60, 86, 19, 30], [19, 86, 10, 10, 68, 85, 10, 68, 47, 47, 59, 31, 47, 45, 47, 35, 65, 86, 47, 19, 65, 65, 46, 59, 47, 65, 47, 31, 10, 10, 49, 35, 10, 21, 31, 30, 34, 30, 71, 35, 47, 49, 65, 47, 10, 37, 30, 53, 19, 65, 85, 47, 68, 19, 90, 45, 60, 19, 85, 19, 35, 10, 60, 65, 19, 46, 49, 10, 30, 86, 86, 47, 58, 35, 19, 10, 18, 30, 27, 19, 86, 10, 6, 19, 31, 6, 19, 6, 47, 86, 59, 46, 49, 65, 30, 47, 19, 47, 35, 53, 10, 10, 10, 6, 86, 19, 48, 59, 47, 49, 60, 49, 46, 45, 86, 86, 10, 65, 6, 85, 40, 19, 10, 49, 6, 45, 19, 10, 65, 47, 46, 47, 86, 47, 47, 86, 19, 54, 91, 60, 60, 10, 47, 31, 35, 19, 59, 60, 65, 10, 31, 49, 85, 49, 10, 19, 60, 47, 86, 85, 49, 68, 19, 49, 48, 60, 47, 47, 86, 19, 31, 59, 59, 45, 86, 10, 47, 86, 35, 18, 19, 49, 30, 47, 49, 31, 31, 19, 60, 86, 47, 30, 6, 35, 65, 19, 47, 45, 19, 19, 10, 19, 19, 45, 19, 19, 10, 37, 47, 61, 60, 47, 86, 59, 31, 47, 35, 86, 13, 49, 18, 47, 86, 10, 65, 47, 45, 47, 19, 46, 86, 31, 19, 6, 47, 45, 31, 59, 65, 19, 35, 53, 95, 47, 18, 65, 19, 37, 60, 18, 18, 35, 47, 65, 30, 10, 59, 85, 30, 19, 47, 37, 35, 45, 47, 45, 35, 47, 18, 47, 65, 60, 86, 19, 30]]\n",
      "['ひ漢好好能語好能トト集しトポトいよ漢トひよよー集トよトし好好ない好をしめ羨め知いトなよト好ムめ達ひよ語ト能ひみポ言ひ語ひい好言よひーな好め漢漢ト学いひ好牛めスひ漢好犬ひし犬ひ犬ト漢集ーなよめトひトい達好好好犬漢ひに集トな言なーポ漢漢好よ犬語たひ好な犬ポひ好よトート漢トト漢ひa話言言好トしいひ集言よ好しな語な好ひ言ト漢語な能ひなに言トト漢ひし集集ポ漢好ト漢い牛ひなめトなししひ言漢トめ犬いよひトポひひ好ひひポひひ好ムト人言ト漢集しトい漢あな牛ト漢好よトポトひー漢しひ犬トポし集よひい達医ト牛よひム言牛牛いトよめ好集語めひトムいポトポいト牛トよ言漢ひめ', 'ひ漢好好能語好能トト集しトポトいよ漢トひよよー集トよトし好好ない好をしめ羨め知いトなよト好ムめ達ひよ語ト能ひみポ言ひ語ひい好言よひーな好め漢漢ト学いひ好牛めスひ漢好犬ひし犬ひ犬ト漢集ーなよめトひトい達好好好犬漢ひに集トな言なーポ漢漢好よ犬語たひ好な犬ポひ好よトート漢トト漢ひa話言言好トしいひ集言よ好しな語な好ひ言ト漢語な能ひなに言トト漢ひし集集ポ漢好ト漢い牛ひなめトなししひ言漢トめ犬いよひトポひひ好ひひポひひ好ムト人言ト漢集しトい漢あな牛ト漢好よトポトひー漢しひ犬トポし集よひい達医ト牛よひム言牛牛いトよめ好集語めひトムいポトポいト牛トよ言漢ひめ', 'ひ漢好好能語好能トト集しトポトいよ漢トひよよー集トよトし好好ない好をしめ羨め知いトなよト好ムめ達ひよ語ト能ひみポ言ひ語ひい好言よひーな好め漢漢ト学いひ好牛めスひ漢好犬ひし犬ひ犬ト漢集ーなよめトひトい達好好好犬漢ひに集トな言なーポ漢漢好よ犬語たひ好な犬ポひ好よトート漢トト漢ひa話言言好トしいひ集言よ好しな語な好ひ言ト漢語な能ひなに言トト漢ひし集集ポ漢好ト漢い牛ひなめトなししひ言漢トめ犬いよひトポひひ好ひひポひひ好ムト人言ト漢集しトい漢あな牛ト漢好よトポトひー漢しひ犬トポし集よひい達医ト牛よひム言牛牛いトよめ好集語めひトムいポトポいト牛トよ言漢ひめ']\n",
      "['ひ', '漢', '好', '好', '能', '語', '好', '能', 'ト', 'ト', '集', 'し', 'ト', 'ポ', 'ト', 'い', 'よ', '漢', 'ト', 'ひ', 'よ', 'よ', 'ー', '集', 'ト', 'よ', 'ト', 'し', '好', '好', 'な', 'い', '好', 'を', 'し', 'め', '羨', 'め', '知', 'い', 'ト', 'な', 'よ', 'ト', '好', 'ム', 'め', '達', 'ひ', 'よ', '語', 'ト', '能', 'ひ', 'み', 'ポ', '言', 'ひ', '語', 'ひ', 'い', '好', '言', 'よ', 'ひ', 'ー', 'な', '好', 'め', '漢', '漢', 'ト', '学', 'い', 'ひ', '好', '牛', 'め', 'ス', 'ひ', '漢', '好', '犬', 'ひ', 'し', '犬', 'ひ', '犬', 'ト', '漢', '集', 'ー', 'な', 'よ', 'め', 'ト', 'ひ', 'ト', 'い', '達', '好', '好', '好', '犬', '漢', 'ひ', 'に', '集', 'ト', 'な', '言', 'な', 'ー', 'ポ', '漢', '漢', '好', 'よ', '犬', '語', 'た', 'ひ', '好', 'な', '犬', 'ポ', 'ひ', '好', 'よ', 'ト', 'ー', 'ト', '漢', 'ト', 'ト', '漢', 'ひ', 'a', '話', '言', '言', '好', 'ト', 'し', 'い', 'ひ', '集', '言', 'よ', '好', 'し', 'な', '語', 'な', '好', 'ひ', '言', 'ト', '漢', '語', 'な', '能', 'ひ', 'な', 'に', '言', 'ト', 'ト', '漢', 'ひ', 'し', '集', '集', 'ポ', '漢', '好', 'ト', '漢', 'い', '牛', 'ひ', 'な', 'め', 'ト', 'な', 'し', 'し', 'ひ', '言', '漢', 'ト', 'め', '犬', 'い', 'よ', 'ひ', 'ト', 'ポ', 'ひ', 'ひ', '好', 'ひ', 'ひ', 'ポ', 'ひ', 'ひ', '好', 'ム', 'ト', '人', '言', 'ト', '漢', '集', 'し', 'ト', 'い', '漢', 'あ', 'な', '牛', 'ト', '漢', '好', 'よ', 'ト', 'ポ', 'ト', 'ひ', 'ー', '漢', 'し', 'ひ', '犬', 'ト', 'ポ', 'し', '集', 'よ', 'ひ', 'い', '達', '医', 'ト', '牛', 'よ', 'ひ', 'ム', '言', '牛', '牛', 'い', 'ト', 'よ', 'め', '好', '集', '語', 'め', 'ひ', 'ト', 'ム', 'い', 'ポ', 'ト', 'ポ', 'い', 'ト', '牛', 'ト', 'よ', '言', '漢', 'ひ', 'め']\n"
     ]
    }
   ],
   "source": [
    "print(teste)\n",
    "print(teste2)\n",
    "print(teste3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 274, 103])\n",
      "tensor([ -7.4077,  -7.5822,  -7.4569,  -8.9240,  -8.7341,  -5.5174,  -8.8402,\n",
      "         -7.4989, -15.3325,  -7.1464, -11.0713,  -8.8875,  -8.5916,  -8.5472,\n",
      "         -6.2259,  -9.5910, -10.3222,  -5.3836,  -9.8709,  -7.0499,  -7.1175,\n",
      "        -12.8834,  -5.0934,  -7.3825, -10.3312,  -7.0444,  -1.3831,  -3.8188,\n",
      "        -12.8234,  -8.2651,  -6.8028,  -6.1243,  -2.9476,  -9.1630,  -7.8622,\n",
      "         -7.4962,  -4.3223,  -7.3711,  -1.5454,  -8.2204,  -3.7890,  -7.7943,\n",
      "         -7.5050,  -9.1047,  -5.6103,  -7.5185,  -9.5259,  -6.4132,  -3.7595,\n",
      "         -8.7571,  -8.5637,  -6.0815,  -7.5027,  -9.3357,  -7.5818, -11.0212,\n",
      "         -8.4136,  -8.3371,  -5.7022,  -6.8867,  -7.2171,  -5.2773,  -8.0870,\n",
      "         -6.4800,  -8.0784, -10.0333,  -6.5211, -10.6734, -11.5680,  -8.8631,\n",
      "         -7.7689, -10.7498, -10.2519,  -2.5590,  -6.6104,  -8.7322,  -8.5502,\n",
      "        -11.3365, -10.3166,  -6.0734,  -8.8425,  -2.5607,  -6.7292,  -5.0885,\n",
      "         -4.5518,  -7.7464, -11.9960,  -2.1040,  -7.2578,  -8.1020, -10.5526,\n",
      "        -10.1261,  -7.7574,  -8.8951,  -6.5799,  -4.4067,  -8.6050,  -7.1978,\n",
      "         -9.3118,  -9.9658,  -9.5545,  -7.2831,  -3.4801], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probabilities.size())\n",
    "print(probabilities[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1, 3, 4, 2, 0, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "input_texts = dataset.english_phrases\n",
    "target_texts = dataset.japanese_phrases\n",
    "\n",
    "#shuffle = torch.randperm(len(input_texts))\n",
    "\n",
    "batch_shuffle = [i for i in range(len(input_texts))]\n",
    "shuffle(batch_shuffle)\n",
    "\n",
    "print(len(input_texts))\n",
    "print(batch_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-2, betas=(0., 0.999), eps=1e-9)\n",
    "loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15956924946123402\n",
      "1.7552926369521265\n",
      "3.3510587900891546\n",
      "4.9468485606022226\n",
      "6.542642799493576\n",
      "8.138422357420778\n",
      "9.734168085079688\n",
      "11.329860833587446\n",
      "12.925481454865453\n",
      "14.521010802022314\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = 50\n",
    "std = 5000\n",
    "\n",
    "for i in range(1, 100, 10):\n",
    "\n",
    "    A = ((i - mean)**2) / (2*(std**2))\n",
    "\n",
    "    num = math.e ** (-A)\n",
    "    den = std * math.sqrt(math.pi*2)\n",
    "\n",
    "    y = (num/den) * (i/mean) * 1e5\n",
    "\n",
    "    print(y)\n",
    "    plt.scatter(i, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_function(current_epoch, scale=1e5):\n",
    "\n",
    "    MEAN = 50\n",
    "    STD = 5000\n",
    "\n",
    "    A = ((current_epoch - MEAN)**2) / (2*(STD**2))\n",
    "\n",
    "    num = math.e ** (-A)\n",
    "    den = STD * math.sqrt(math.pi*2)\n",
    "\n",
    "    y = (num/den) * (current_epoch/MEAN) * scale\n",
    "\n",
    "    return min(y, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampler(original_target, generated_target, sampler_chance=0.1):\n",
    "\n",
    "    new_target = []\n",
    "\n",
    "    for batch in range(max(len(original_target), len(generated_target))):\n",
    "\n",
    "        target = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            candidateA = original_target[batch]\n",
    "        \n",
    "        except IndexError:\n",
    "\n",
    "            new_target.append(generated_target[batch])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            candidateB = generated_target[batch]\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            new_target.append(original_target[batch])\n",
    "            continue\n",
    "\n",
    "        for item in range(max(len(candidateA), len(candidateB))):\n",
    "\n",
    "            if torch.rand((1,)) < sampler_chance:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    target.append(candidateB[item])\n",
    "\n",
    "                except IndexError:\n",
    "\n",
    "                    target.append(candidateA[item])\n",
    "\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    target.append(candidateA[item])\n",
    "\n",
    "                except IndexError:\n",
    "\n",
    "                    target.append(candidateB[item])\n",
    "        \n",
    "        new_target.append(target)\n",
    "\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration: 10\tSampler chance: 0.04787095866249852\n",
      "Last Loss: 2.9649741649627686\tBatch Loss: 0.2964974343776703\n",
      "Gradients Average: 0.00011130960774607956\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.997438907623291\n",
      "['大まか正字名名字犬名犬名ポー正犬らト歌犬めそ字字大犬犬犬で大犬正犬ト名正小大字大正羨犬正歌名こ名ー歌大そ名犬字名字正字字まー名正大数こ名犬大ま犬よト名ら大字ら字字そトトト名ら字トそ犬らをー名字大ら正犬大こ正言字大トー字名正犬字字そ小名大かー名前字名ト正字名ト赤正正ー犬サそそ名大ト羨名こ字骨名名正か羨大犬かスー字こ正大正ー漢名そ大ト漢羨正をそポ字字まー大生大正らそ正正も正らー名小', '大まか正字名名字犬名犬名ポー正犬らト歌犬めそ字字大犬犬犬で大犬正犬ト名正小大字大正羨犬正歌名こ名ー歌大そ名犬字名字正字字まー名正大数こ名犬大ま犬よト名ら大字ら字字そトトト名ら字トそ犬らをー名字大ら正犬大こ正言字大トー字名正犬字字そ小名大かー名前字名ト正字名ト赤正正ー犬サそそ名大ト羨名こ字骨名名正か羨大犬かスー字こ正大正ー漢名そ大ト漢羨正をそポ字字まー大生大正らそ正正も正らー名小', '大まか正字名名字犬名犬名ポー正犬らト歌犬めそ字字大犬犬犬で大犬正犬ト名正小大字大正羨犬正歌名こ名ー歌大そ名犬字名字正字字まー名正大数こ名犬大ま犬よト名ら大字ら字字そトトト名ら字トそ犬らをー名字大ら正犬大こ正言字大トー字名正犬字字そ小名大かー名前字名ト正字名ト赤正正ー犬サそそ名大ト羨名こ字骨名名正か羨大犬かスー字こ正大正ー漢名そ大ト漢羨正をそポ字字まー大生大正らそ正正も正らー名小']\n",
      "Current Iteration: 20\tSampler chance: 0.09574244007729277\n",
      "Last Loss: 2.787767171859741\tBatch Loss: 0.27874070405960083\n",
      "Gradients Average: 2.9116831683495548e-06\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.8626022338867188\n",
      "['ららら字名ら犬名ら名名名ら犬犬らそ名ら名名名犬ら名小らららら小ら名名名ららららら名字ら名ら名名名らら小名ら名名名名だ名犬らら名小名名正らららららら名犬名ら名ら名らららら名犬ららら名ら字ららか名字ららら名犬名名名名名ら名名ら名犬らららら名ららら名名らららららら犬らららら名名名名名らら名活犬ららら犬名名漢名名名名名ら名ら犬名ら犬ら名名ら名ら名名ら犬ねらら犬らら犬犬ら名ららららら小犬名名犬犬小ら名ら犬らら犬名名ら小ら名犬名名名ら名ららら名名ららら名名ら名犬ら犬ららら名字犬名ら犬名犬字らら名ら名名ら名名ら名らら犬ららら名小', 'ららら字名ら犬名ら名名名ら犬犬らそ名ら名名名犬ら名小らららら小ら名名名ららららら名字ら名ら名名名らら小名ら名名名名だ名犬らら名小名名正らららららら名犬名ら名ら名らららら名犬ららら名ら字ららか名字ららら名犬名名名名名ら名名ら名犬らららら名ららら名名らららららら犬らららら名名名名名らら名活犬ららら犬名名漢名名名名名ら名ら犬名ら犬ら名名ら名ら名名ら犬ねらら犬らら犬犬ら名ららららら小犬名名犬犬小ら名ら犬らら犬名名ら小ら名犬名名名ら名ららら名名ららら名名ら名犬ら犬ららら名字犬名ら犬名犬字らら名ら名名ら名名ら名らら犬ららら名小', 'ららら字名ら犬名ら名名名ら犬犬らそ名ら名名名犬ら名小らららら小ら名名名ららららら名字ら名ら名名名らら小名ら名名名名だ名犬らら名小名名正らららららら名犬名ら名ら名らららら名犬ららら名ら字ららか名字ららら名犬名名名名名ら名名ら名犬らららら名ららら名名らららららら犬らららら名名名名名らら名活犬ららら犬名名漢名名名名名ら名ら犬名ら犬ら名名ら名ら名名ら犬ねらら犬らら犬犬ら名ららららら小犬名名犬犬小ら名ら犬らら犬名名ら小ら名犬名名名ら名ららら名名ららら名名ら名犬ら犬ららら名字犬名ら犬名犬字らら名ら名名ら名名ら名らら犬ららら名小']\n",
      "Current Iteration: 30\tSampler chance: 0.1436143925474734\n",
      "Last Loss: 2.7351715564727783\tBatch Loss: 0.27658697962760925\n",
      "Gradients Average: 6.130104338808451e-06\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.7872207164764404\n",
      "['小名名犬犬名名名名名名名犬そ犬名名名ら犬名名ら犬名名名犬名小小名名活犬名名生名犬や犬活名名活名名活名ト名名犬犬名名名犬すら名名名ら犬名名名犬ら犬名ら名名犬活生名活犬ら活名名犬犬か名物名名名小名名犬名名名ら犬名名字名名犬そ小名活生名名名名名名犬名名小名名名犬ら犬小名活生名名ト名犬犬名名名名名名名名犬名らだ犬名犬トら犬名名名名名名犬犬小活活小犬ら名名犬ら犬名活か活名小られ犬名そま名犬犬正名名犬ら犬犬生犬活名生字ら名小犬ら名名か集名生犬名犬そ名']\n",
      "Current Iteration: 40\tSampler chance: 0.20744430590589177\n",
      "Last Loss: 2.7326831817626953\tBatch Loss: 0.06831707805395126\n",
      "Gradients Average: -7.716035383964481e-07\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.834705352783203\n",
      "['活犬犬名生犬名活名犬名活犬う犬活名犬物名生名名犬犬ら漢名名名正i犬生小ら名名犬名ら犬犬ら小物名活活名名物物犬犬ら名名名犬らら名名ら犬物ら犬ら生名名名犬活ら物生名活名活名犬生犬生物名犬犬犬犬犬犬名犬犬犬名名名犬小物名ら物ら名犬犬生犬ら名ま名ら名名名犬犬ら犬犬生犬名小犬ら名物名名名犬名犬名犬犬名名ら犬名物犬らら名活名犬名活犬犬犬犬生', '活犬犬名生犬名活名犬名活犬う犬活名犬物名生名名犬犬ら漢名名名正i犬生小ら名名犬名ら犬犬ら小物名活活名名物物犬犬ら名名名犬らら名名ら犬物ら犬ら生名名名犬活ら物生名活名活名犬生犬生物名犬犬犬犬犬犬名犬犬犬名名名犬小物名ら物ら名犬犬生犬ら名ま名ら名名名犬犬ら犬犬生犬名小犬ら名物名名名犬名犬名犬犬名名ら犬名物犬らら名活名犬名活犬犬犬犬生', '活犬犬名生犬名活名犬名活犬う犬活名犬物名生名名犬犬ら漢名名名正i犬生小ら名名犬名ら犬犬ら小物名活活名名物物犬犬ら名名名犬らら名名ら犬物ら犬ら生名名名犬活ら物生名活名活名犬生犬生物名犬犬犬犬犬犬名犬犬犬名名名犬小物名ら物ら名犬犬生犬ら名ま名ら名名名犬犬ら犬犬生犬名小犬ら名物名名名犬名犬名犬犬名名ら犬名物犬らら名活名犬名活犬犬犬犬生']\n",
      "Current Iteration: 50\tSampler chance: 0.25531715645602127\n",
      "Last Loss: 2.7599353790283203\tBatch Loss: 0.1101124957203865\n",
      "Gradients Average: 1.3452611256070668e-06\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.7929162979125977\n",
      "['小名生活か生生犬小生ら活ら活小だ生名活名達活生物生正小小活物小生小物生活犬小だ犬小犬生生小活小ら活ら名犬犬ら生だ生小小物名活ら生名活小小ら小活ら小物生小生小だらか生生生犬生ら生生生物小', '小名生活か生生犬小生ら活ら活小だ生名活名達活生物生正小小活物小生小物生活犬小だ犬小犬生生小活小ら活ら名犬犬ら生だ生小小物名活ら生名活小小ら小活ら小物生小生小だらか生生生犬生ら生生生物小', '小名生活か生生犬小生ら活ら活小だ生名活名達活生物生正小小活物小生小物生活犬小だ犬小犬生生小活小ら活ら名犬犬ら生だ生小小物名活ら生名活小小ら小活ら小物生小生小だらか生生生犬生ら生生生物小']\n",
      "Current Iteration: 60\tSampler chance: 0.3031903057314118\n",
      "Last Loss: 2.795510768890381\tBatch Loss: 0.1393965184688568\n",
      "Gradients Average: -6.255461357795866e-07\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.698681116104126\n",
      "['小小ます小小小生み生小活生活む活大小物小生だ小名そだ小ら名活活活小活だ生だん小活生生名小']\n",
      "Current Iteration: 70\tSampler chance: 0.3670215467561561\n",
      "Last Loss: 2.788135051727295\tBatch Loss: 0.03983050212264061\n",
      "Gradients Average: 1.1145189091621432e-06\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.8253519535064697\n",
      "['活生活活活生生活活活生生生生生生か生生活活だ生活活生生生活活活活活活生生だ生活生生生だ活物生生生生生活だ生生生生生生活生活生活活生生活活生活生生生ん活だ生活生生生活生生生生活か活生生生生活生生生活生生生活活活活生生活活生生生活生活活生活生生生生生活生活活生', '活生活活活生生活活活生生生生生生か生生活活だ生活活生生生活活活活活活生生だ生活生生生だ活物生生生生生活だ生生生生生生活生活生活活生生活活生活生生生ん活だ生活生生生活生生生生活か活生生生生活生生生活生生生活活活活生生活活生生生活生活活生活生生生生生活生活活生', '活生活活活生生活活活生生生生生生か生生活活だ生活活生生生活活活活活活生生だ生活生生生だ活物生生生生生活だ生生生生生生活生活生活活生生活活生活生生生ん活だ生活生生生活生生生生活か活生生生生活生生生活生生生活活活活生生活活生生生活生活活生活生生生生生活生活活生']\n",
      "Current Iteration: 80\tSampler chance: 0.4148951919973477\n",
      "Last Loss: 2.7937355041503906\tBatch Loss: 0.07151796668767929\n",
      "Gradients Average: 1.1517580787767656e-05\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.8193206787109375\n",
      "['小小活活物活活活活活小活活活活活活活活活活活活活活活活活活活活活活活活活活小活小小小活活活小小物活活活活活小小活活活小活活活活活活活物小小活活物活活活活小活小活活活活活活活小活だ活物活活活小活小活活活物活活活小活活み活小活小活小活活活活活活活活小活小活小活活物小小活活活活物み活活活小活小活活小活活小活活活活小小活小活活活活活小小活小活小活活活小活活活活活活物活活活活活活活活物物活活活小活活名小活活活活活活活活活活活活活活小活小活活物活活活活活活活活活活活活小み小活小活活活活活活活活活小小活活活活活小活活活活活小小活活活活小活活活活物活活活', '小小活活物活活活活活小活活活活活活活活活活活活活活活活活活活活活活活活活活小活小小小活活活小小物活活活活活小小活活活小活活活活活活活物小小活活物活活活活小活小活活活活活活活小活だ活物活活活小活小活活活物活活活小活活み活小活小活小活活活活活活活活小活小活小活活物小小活活活活物み活活活小活小活活小活活小活活活活小小活小活活活活活小小活小活小活活活小活活活活活活物活活活活活活活活物物活活活小活活名小活活活活活活活活活活活活活活小活小活活物活活活活活活活活活活活活小み小活小活活活活活活活活活小小活活活活活小活活活活活小小活活活活小活活活活物活活活', '小小活活物活活活活活小活活活活活活活活活活活活活活活活活活活活活活活活活活小活小小小活活活小小物活活活活活小小活活活小活活活活活活活物小小活活物活活活活小活小活活活活活活活小活だ活物活活活小活小活活活物活活活小活活み活小活小活小活活活活活活活活小活小活小活活物小小活活活活物み活活活小活小活活小活活小活活活活小小活小活活活活活小小活小活小活活活小活活活活活活物活活活活活活活活物物活活活小活活名小活活活活活活活活活活活活活活小活小活活物活活活活活活活活活活活活小み小活小活活活活活活活活活小小活活活活活小活活活活活小小活活活活小活活活活物活活活']\n",
      "Current Iteration: 90\tSampler chance: 0.4627689636254027\n",
      "Last Loss: 2.4924750328063965\tBatch Loss: 0.09376710653305054\n",
      "Gradients Average: -1.5668865671614185e-07\n",
      "Generating Text...\n",
      "Evaluation Loss: 2.8854048252105713\n",
      "['小だ小ご活活だ小活物小小小活だ活小活小でだ小生生活ま活そ小小か活活だ小小小正活だ活み小活か小生活だだた活活小活活小小活ら活だ小小小らだだだ小小小小活小小だ生活活小小物小だらだらだ活小だ活だだ小小小小小活かららち活小小小だ小らそ活小だ小活活活活小かだ小活小小小活そ小だだ活小小小小だ小活だかだ活小だだ小活小小活活活小だらみ小活活小小だだ小活小小生小活小活だ小小小字ご活小小活活だだ小小生小小みだ名小小小小小小小小小小そ小活小活だ活小小小らだ小']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "\n",
    "    shuffle(batch_shuffle)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in range(0, len(input_texts), BATCH_SIZE):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        current_batch = batch_shuffle[batch:min(len(input_texts)-1, batch+BATCH_SIZE)]\n",
    "\n",
    "        input_text, target_text = [], []\n",
    "\n",
    "        for i in range(len(current_batch)):\n",
    "\n",
    "            input_text.append(input_texts[current_batch[i]])\n",
    "            target_text.append(target_texts[current_batch[i]])\n",
    "\n",
    "        input, real_input_length = model.preprocess_dialogue(input_text, dataset.english_dictionary)\n",
    "        target, target_length = model.preprocess_target(target_text, dataset.japanese_dictionary)\n",
    "\n",
    "        encoder_vectors = model.embedding(input) + math.sqrt(model.d_model)\n",
    "        decoder_vectors = model.embedding(target) + math.sqrt(model.d_model)\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors)\n",
    "\n",
    "        target_vector = torch.zeros_like(output).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "        cost1 = loss(output[:, :66], target_vector[:, :66])\n",
    "\n",
    "        #new_target = schedule_sampler(target_vector, output.detach())\n",
    "        generated_target, _ = model.talk2me(output[:, :66].argmax(-1))\n",
    "        sampler_chance = sampler_function(epoch, scale=1e4)\n",
    "        new_target = schedule_sampler(target_text, generated_target, sampler_chance)\n",
    "        new_target, target_length = model.preprocess_target(new_target, dataset.japanese_dictionary)\n",
    "\n",
    "        decoder_vectors = model.embedding(new_target) + math.sqrt(model.d_model)\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors)\n",
    "\n",
    "        target_vector = torch.zeros_like(output).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "        cost2 = loss(output[:, :66], target_vector[:, :66])\n",
    "\n",
    "        #generated_text, possibilities = model.generate_sentences(input_text)\n",
    "        #target, target_sizes = model.preprocess_target(target_text, dataset.japanese_dictionary)\n",
    "        #target_vector = torch.zeros_like(possibilities).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "        # Sampling the 5 highest plausible choices to use as labels\n",
    "        # Though this doesn't seem to make much sense.\n",
    "        # Best to simply try to predict the next word/sentence (GPT-2 Pre-Training)\n",
    "\n",
    "        #_, true_possibilities = torch.sort(possibilities, dim=-1, descending=True)\n",
    "\n",
    "        #random_idx = torch.randint(0, 5, size=(1,))\n",
    "\n",
    "        #true_possibilities = true_possibilities[:, random_idx.item()]\n",
    "        #true_possibilities = true_possibilities[:, -1]\n",
    "\n",
    "        #cost = loss(possibilities, true_possibilities)\n",
    "\n",
    "        #cost2 = loss(possibilities[:, :66], target[:, :66])\n",
    "\n",
    "        total_cost = cost1 + (cost2 * (epoch/100))\n",
    "\n",
    "        total_cost.backward()\n",
    "\n",
    "        total_loss += total_cost\n",
    "\n",
    "        grads = torch.mean(model.encoder[0].attention_heads[0].create_queries.weight.grad)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 10 == 0:\n",
    "\n",
    "            print(f\"Current Iteration: {iters}\\tSampler chance: {sampler_chance}\")\n",
    "            print(f\"Last Loss: {total_cost.item()}\\tBatch Loss: {total_loss/iters}\")\n",
    "            print(f\"Gradients Average: {grads}\")\n",
    "            print(f\"Generating Text...\")\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            generated_text, possibilities = model.generate_sentences(input_text)\n",
    "\n",
    "            target_vector = torch.zeros_like(possibilities).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "            cost = loss(possibilities[:, :66], target_vector[:, :66])\n",
    "            cost.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            generated_text, _ = model.talk2me(generated_text)\n",
    "\n",
    "            print(f\"Evaluation Loss: {cost.item()}\")\n",
    "            print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Generative Adversarial Networks\n",
    "\n",
    "## We can obtain an AI capable of detecting AI-generated texts, while also making an AI capable of generating realistic texts\n",
    "\n",
    "\n",
    "**TEXT ----> WERNICKE AREA: PROCESS INFORMATION (What does it mean?)**\n",
    "\n",
    "**WERNICKE ------> ASSOCIATIVE CORTEX(TEMPORAL + PARIETAL?) -----> BROCA AREA: GENERATES NEW(?) INFORMATION ---> MOTOR CORTEX (PRECENTRAL GYRUS)**\n",
    "\n",
    "\n",
    "*Question: Can she detect metaphores, implicit messages, sense of humour, poetry? Such things aren't as simple as \"Word ---> Meaning\", afterall.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
