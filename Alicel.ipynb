{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT:\n",
    "\n",
    "In Transformer, the data is in tensor size (Batch, Sequence_length, features)\n",
    "\n",
    "In reality, the Batch dimension is always considered to be 1. The \"Batch\" that is really considered is the sequence_length,\n",
    "that is, the number of tokens the model will be processing at each time\n",
    "\n",
    "In Reinforcement Learning, it's not common to use mini-batches. This may be the case with the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter.ttk import LabeledScale\n",
    "from PyPDF2 import PdfFileReader as PFR\n",
    "FILE_PATH = 'F:/Livros Medicina/Bases farmacológicas terapêutica de Goodman & Gilman 13 ed.pdf'\n",
    "file = open(FILE_PATH, 'rb')\n",
    "Reader = PFR(file)\n",
    "\n",
    "text = [Reader.getPage(i).extractText() for i in range(800, 900)]\n",
    "\n",
    "text = text[850:860]\n",
    "\n",
    "text = [i.lower() for i in text]\n",
    "\n",
    "text = [re.sub('[^a-z0-9\\s\\ç\\á\\ô\\ã\\ê\\é\\â\\ó\\í\\.\\/]', '', x) for x in text]\n",
    "\n",
    "text_total = ' '.join(text)\n",
    "text_total = ' '.join(text_total.split())\n",
    "text_total = text_total.replace(' de ', '').replace(' e ', '').replace(' o ', '').replace('.', '').replace(' em ', '').replace(' a ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。初めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "    \"上手医者と大きい研究者に勉強していたいます。\",\n",
    "    \"でも、私の日本語が上手あまりませんね\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\",\n",
    "    \"I shall study so I can be a good physician and a great scientist\",\n",
    "    \"Though my japanese is not really good\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_PATH = \"D:/Python/Projects/Alice/shinamotaENG.txt\"\n",
    "JAPANESE_PATH = \"D:/Python/Projects/Alice/shinamotaJP.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So, as for my life', 'Or, anyone’s life', 'Sometimes, and often times,', 'they’re judged equally…right?', 'You came into my dark skies', 'bringing along depression', 'with some occasional rain', 'but I’m so leaning onto you', 'what do I do?', '']\n"
     ]
    }
   ],
   "source": [
    "# Note: Pay attention to apostrophes ('ve, 're, 'm)\n",
    "\n",
    "english_phrases = []\n",
    "\n",
    "with open(ENGLISH_PATH, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    for i in f:\n",
    "\n",
    "        english_phrases.append(i)\n",
    "\n",
    "f.close()\n",
    "\n",
    "english_phrases = [i.replace('\\n', '') for i in english_phrases]\n",
    "print(english_phrases[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['僕の命っつったって 誰の命っつったって', '時時々 公平に 裁かれるもんなんでしょ', '暗い空にやってきた 鬱を連れてやってきた', '時々雨 総計に 頼り切りだ どうしよう', '朽ちるまでの愛憎を 朽ちるまでの愛憎を', '飲み込む君 簡単に 微笑む君 どうして', '言葉を書く 曖昧に 言葉を書く 曖昧に', '伝わりきらんないから 君だけをさ 信じて', '捨ててきた夢をあつめて', 'ちょっと ちょっと 間違えたから']\n"
     ]
    }
   ],
   "source": [
    "japanese_phrases = []\n",
    "\n",
    "with open(JAPANESE_PATH, 'r', encoding='utf-8') as f:\n",
    "\n",
    "    for i in f:\n",
    "\n",
    "        japanese_phrases.append(i)\n",
    "\n",
    "f.close()\n",
    "\n",
    "japanese_phrases = [i.replace('\\n', '') for i in japanese_phrases]\n",
    "print(japanese_phrases[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "        self.english_maximum_length = self._get_maximum_length_english(self.english_phrases)\n",
    "\n",
    "        self.vocabulary_maximum_length = max(self.japanese_maximum_length, self.english_maximum_length)\n",
    "\n",
    "        #self.english_dictionary = self._create_vocabulary(self.english_words)\n",
    "        #self.japanese_dictionary = self._create_vocabulary(self.japanese_characters)\n",
    "        self.dictionary = self._create_vocabulary(self.english_words) + self._create_vocabulary(self.japanese_characters)\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "        phrases = [x for x in phrases if x != '']\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_english(self, phrases):\n",
    "\n",
    "        maximum_length = 0\n",
    "        real_length = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            word_length = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word_length += len(word)+1 # Including spaces between words\n",
    "\n",
    "            sentence_length = word_length\n",
    "            real_length.append(sentence_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_vocabulary(self, words):\n",
    "        idx2word = [\"<pad>\", \" \", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "        for word in words:\n",
    "            if word not in idx2word:\n",
    "                idx2word.append(word)\n",
    "\n",
    "        return idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(english_phrases, japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so as for my life', 'or anyones life', 'sometimes and often times', 'theyre judged equallyright', 'you came into my dark skies', 'bringing along depression', 'with some occasional rain', 'but im so leaning onto you', 'what do i do', 'love hate so extreme till its decayed', 'but you take it all in', 'and easily smile why', 'my written words come out ambiguous', 'it doesnt quite convey', 'so i look to you', 'as the only one to trust', 'collecting the dreams i threw away', 'this little mistake that i made', 'oh even if at times i stumble', 'its so cold cold cold cold cold', 'its so cold cold cold cold cold', 'its so cold cold cold cold dont sweet talk', 'oh your your your voice', 'is so far far far far far', 'is so far far far far far', 'oh so far far far far dont hurt me', 'do i care where i am in this line', 'in order for me to be me', 'im here with promises i recollected', 'too painful to swallow the mistake that ive made', 'love hate so extreme till its decayed', 'here you are taking it all in', 'smiling so easily  why', 'flowers to settle me down', 'cause because you gave up', 'oh running after that dream so far away', 'so fast fast fast fast fast', 'so fast fast fast fast fast', 'so fast fast fast fast i cant keep up', 'those feelings i couldnt drive away', 'i hate hate hate hate hate it', 'i hate hate hate hate hate it', 'i hate hate hate hate it  cant i be forgiven', 'oh the dream the dream i had but', 'im scared scared scared scared scared', 'im scared scared scared scared scared', 'im scared scared scared scared dont come close', 'we just keep repeating our secret code', 'its so cold cold cold cold cold', 'its so cold cold cold cold cold', 'its so cold cold cold cold please', 'who cares where i am in this line', 'in order for me to be me']\n",
      "['僕の命っつったって 誰の命っつったって', '時時々 公平に 裁かれるもんなんでしょ', '暗い空にやってきた 鬱を連れてやってきた', '時々雨 総計に 頼り切りだ どうしよう', '朽ちるまでの愛憎を 朽ちるまでの愛憎を', '飲み込む君 簡単に 微笑む君 どうして', '言葉を書く 曖昧に 言葉を書く 曖昧に', '伝わりきらんないから 君だけをさ 信じて', '捨ててきた夢をあつめて', 'ちょっと ちょっと 間違えたから', 'ああ 時に 時に 躓いたって', '寒い 寒い 寒い 寒い 寒い', '寒い 寒い 寒い 寒い 寒い', '寒い 寒い 寒い 寒い 言い寄らないで', 'ああ 君の 君の 君の声が', '遠い 遠い 遠い 遠い 遠い', '遠い 遠い 遠い 遠い 遠い', '遠い 遠い 遠い 遠い 傷つけないで', '何番目でも 何番目でも', '僕が僕であるために', '契り集め 持ってきた 契り集め 持ってきた', 'あの日の間違いを 飲み込むのが苦るしくて', '朽ちるまでの愛憎を 朽ちるまでの愛憎を', '飲み込む君 簡単に 微笑む君 どうして', '子供だましの花ふたつ', 'きっと きっと 諦めたから', 'ああ 遠い夢を追いかけてさ', '早い 早い 早い 早い 早い', '早い 早い 早い 早い 早い', '早い 早い 早い 早い 追いつけないよ', '捨てきれず残した思いが', '憎い 憎い 憎い 憎い 憎い', '憎い 憎い 憎い 憎い 憎い', '憎い 憎い 憎い 憎い 許されないの', 'ああ 夢を 夢を見てたはずが', '怖い 怖い 怖い 怖い 怖い', '怖い 怖い 怖い 怖い 怖い', '怖い 怖い 怖い 怖い 近づかないで', '合言葉を繰り返すだけ', '寒い 寒い 寒い 寒い 寒い', '寒い 寒い 寒い 寒い 寒い', '寒い 寒い 寒い 寒い お願いだから', '何番目でも 何番目でも', '僕が僕であるために']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_phrases)\n",
    "print(dataset.japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'as', 'for', 'my', 'life', 'or', 'anyones', 'life', 'sometimes', 'and', 'often', 'times', 'theyre', 'judged', 'equallyright', 'you', 'came', 'into', 'my', 'dark', 'skies', 'bringing', 'along', 'depression', 'with', 'some', 'occasional', 'rain', 'but', 'im', 'so', 'leaning', 'onto', 'you', 'what', 'do', 'i', 'do', 'love', 'hate', 'so', 'extreme', 'till', 'its', 'decayed', 'but', 'you', 'take', 'it', 'all', 'in', 'and', 'easily', 'smile', 'why', 'my', 'written', 'words', 'come', 'out', 'ambiguous', 'it', 'doesnt', 'quite', 'convey', 'so', 'i', 'look', 'to', 'you', 'as', 'the', 'only', 'one', 'to', 'trust', 'collecting', 'the', 'dreams', 'i', 'threw', 'away', 'this', 'little', 'mistake', 'that', 'i', 'made', 'oh', 'even', 'if', 'at', 'times', 'i', 'stumble', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'cold', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'cold', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'dont', 'sweet', 'talk', 'oh', 'your', 'your', 'your', 'voice', 'is', 'so', 'far', 'far', 'far', 'far', 'far', 'is', 'so', 'far', 'far', 'far', 'far', 'far', 'oh', 'so', 'far', 'far', 'far', 'far', 'dont', 'hurt', 'me', 'do', 'i', 'care', 'where', 'i', 'am', 'in', 'this', 'line', 'in', 'order', 'for', 'me', 'to', 'be', 'me', 'im', 'here', 'with', 'promises', 'i', 'recollected', 'too', 'painful', 'to', 'swallow', 'the', 'mistake', 'that', 'ive', 'made', 'love', 'hate', 'so', 'extreme', 'till', 'its', 'decayed', 'here', 'you', 'are', 'taking', 'it', 'all', 'in', 'smiling', 'so', 'easily', '', 'why', 'flowers', 'to', 'settle', 'me', 'down', 'cause', 'because', 'you', 'gave', 'up', 'oh', 'running', 'after', 'that', 'dream', 'so', 'far', 'away', 'so', 'fast', 'fast', 'fast', 'fast', 'fast', 'so', 'fast', 'fast', 'fast', 'fast', 'fast', 'so', 'fast', 'fast', 'fast', 'fast', 'i', 'cant', 'keep', 'up', 'those', 'feelings', 'i', 'couldnt', 'drive', 'away', 'i', 'hate', 'hate', 'hate', 'hate', 'hate', 'it', 'i', 'hate', 'hate', 'hate', 'hate', 'hate', 'it', 'i', 'hate', 'hate', 'hate', 'hate', 'it', '', 'cant', 'i', 'be', 'forgiven', 'oh', 'the', 'dream', 'the', 'dream', 'i', 'had', 'but', 'im', 'scared', 'scared', 'scared', 'scared', 'scared', 'im', 'scared', 'scared', 'scared', 'scared', 'scared', 'im', 'scared', 'scared', 'scared', 'scared', 'dont', 'come', 'close', 'we', 'just', 'keep', 'repeating', 'our', 'secret', 'code', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'cold', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'cold', 'its', 'so', 'cold', 'cold', 'cold', 'cold', 'please', 'who', 'cares', 'where', 'i', 'am', 'in', 'this', 'line', 'in', 'order', 'for', 'me', 'to', 'be', 'me']\n",
      "['僕', 'の', '命', 'っ', 'つ', 'っ', 'た', 'っ', 'て', '誰', 'の', '命', 'っ', 'つ', 'っ', 'た', 'っ', 'て', '時', '時', '々', '公', '平', 'に', '裁', 'か', 'れ', 'る', 'も', 'ん', 'な', 'ん', 'で', 'し', 'ょ', '暗', 'い', '空', 'に', 'や', 'っ', 'て', 'き', 'た', '鬱', 'を', '連', 'れ', 'て', 'や', 'っ', 'て', 'き', 'た', '時', '々', '雨', '総', '計', 'に', '頼', 'り', '切', 'り', 'だ', 'ど', 'う', 'し', 'よ', 'う', '朽', 'ち', 'る', 'ま', 'で', 'の', '愛', '憎', 'を', '朽', 'ち', 'る', 'ま', 'で', 'の', '愛', '憎', 'を', '飲', 'み', '込', 'む', '君', '簡', '単', 'に', '微', '笑', 'む', '君', 'ど', 'う', 'し', 'て', '言', '葉', 'を', '書', 'く', '曖', '昧', 'に', '言', '葉', 'を', '書', 'く', '曖', '昧', 'に', '伝', 'わ', 'り', 'き', 'ら', 'ん', 'な', 'い', 'か', 'ら', '君', 'だ', 'け', 'を', 'さ', '信', 'じ', 'て', '捨', 'て', 'て', 'き', 'た', '夢', 'を', 'あ', 'つ', 'め', 'て', 'ち', 'ょ', 'っ', 'と', 'ち', 'ょ', 'っ', 'と', '間', '違', 'え', 'た', 'か', 'ら', 'あ', 'あ', '時', 'に', '時', 'に', '躓', 'い', 'た', 'っ', 'て', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '言', 'い', '寄', 'ら', 'な', 'い', 'で', 'あ', 'あ', '君', 'の', '君', 'の', '君', 'の', '声', 'が', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '遠', 'い', '傷', 'つ', 'け', 'な', 'い', 'で', '何', '番', '目', 'で', 'も', '何', '番', '目', 'で', 'も', '僕', 'が', '僕', 'で', 'あ', 'る', 'た', 'め', 'に', '契', 'り', '集', 'め', '持', 'っ', 'て', 'き', 'た', '契', 'り', '集', 'め', '持', 'っ', 'て', 'き', 'た', 'あ', 'の', '日', 'の', '間', '違', 'い', 'を', '飲', 'み', '込', 'む', 'の', 'が', '苦', 'る', 'し', 'く', 'て', '朽', 'ち', 'る', 'ま', 'で', 'の', '愛', '憎', 'を', '朽', 'ち', 'る', 'ま', 'で', 'の', '愛', '憎', 'を', '飲', 'み', '込', 'む', '君', '簡', '単', 'に', '微', '笑', 'む', '君', 'ど', 'う', 'し', 'て', '子', '供', 'だ', 'ま', 'し', 'の', '花', 'ふ', 'た', 'つ', 'き', 'っ', 'と', 'き', 'っ', 'と', '諦', 'め', 'た', 'か', 'ら', 'あ', 'あ', '遠', 'い', '夢', 'を', '追', 'い', 'か', 'け', 'て', 'さ', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '早', 'い', '追', 'い', 'つ', 'け', 'な', 'い', 'よ', '捨', 'て', 'き', 'れ', 'ず', '残', 'し', 'た', '思', 'い', 'が', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '憎', 'い', '許', 'さ', 'れ', 'な', 'い', 'の', 'あ', 'あ', '夢', 'を', '夢', 'を', '見', 'て', 'た', 'は', 'ず', 'が', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '怖', 'い', '近', 'づ', 'か', 'な', 'い', 'で', '合', '言', '葉', 'を', '繰', 'り', '返', 'す', 'だ', 'け', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', '寒', 'い', 'お', '願', 'い', 'だ', 'か', 'ら', '何', '番', '目', 'で', 'も', '何', '番', '目', 'で', 'も', '僕', 'が', '僕', 'で', 'あ', 'る', 'た', 'め', 'に']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_words)\n",
    "print(dataset.japanese_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.english_dictionary)\n",
    "print(dataset.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', '<SOS>', '<EOS>', 'so', 'as', 'for', 'my', 'life', 'or', 'anyones', 'sometimes', 'and', 'often', 'times', 'theyre', 'judged', 'equallyright', 'you', 'came', 'into', 'dark', 'skies', 'bringing', 'along', 'depression', 'with', 'some', 'occasional', 'rain', 'but', 'im', 'leaning', 'onto', 'what', 'do', 'i', 'love', 'hate', 'extreme', 'till', 'its', 'decayed', 'take', 'it', 'all', 'in', 'easily', 'smile', 'why', 'written', 'words', 'come', 'out', 'ambiguous', 'doesnt', 'quite', 'convey', 'look', 'to', 'the', 'only', 'one', 'trust', 'collecting', 'dreams', 'threw', 'away', 'this', 'little', 'mistake', 'that', 'made', 'oh', 'even', 'if', 'at', 'stumble', 'cold', 'dont', 'sweet', 'talk', 'your', 'voice', 'is', 'far', 'hurt', 'me', 'care', 'where', 'am', 'line', 'order', 'be', 'here', 'promises', 'recollected', 'too', 'painful', 'swallow', 'ive', 'are', 'taking', 'smiling', '', 'flowers', 'settle', 'down', 'cause', 'because', 'gave', 'up', 'running', 'after', 'dream', 'fast', 'cant', 'keep', 'those', 'feelings', 'couldnt', 'drive', 'forgiven', 'had', 'scared', 'close', 'we', 'just', 'repeating', 'our', 'secret', 'code', 'please', 'who', 'cares', '<pad>', ' ', '<SOS>', '<EOS>', '僕', 'の', '命', 'っ', 'つ', 'た', 'て', '誰', '時', '々', '公', '平', 'に', '裁', 'か', 'れ', 'る', 'も', 'ん', 'な', 'で', 'し', 'ょ', '暗', 'い', '空', 'や', 'き', '鬱', 'を', '連', '雨', '総', '計', '頼', 'り', '切', 'だ', 'ど', 'う', 'よ', '朽', 'ち', 'ま', '愛', '憎', '飲', 'み', '込', 'む', '君', '簡', '単', '微', '笑', '言', '葉', '書', 'く', '曖', '昧', '伝', 'わ', 'ら', 'け', 'さ', '信', 'じ', '捨', '夢', 'あ', 'め', 'と', '間', '違', 'え', '躓', '寒', '寄', '声', 'が', '遠', '傷', '何', '番', '目', '契', '集', '持', '日', '苦', '子', '供', '花', 'ふ', '諦', '追', '早', 'ず', '残', '思', '許', '見', 'は', '怖', '近', 'づ', '合', '繰', '返', 'す', 'お', '願']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model, max_length=100):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as defined in the paper.\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param max_length: maximum sequence length up to which positional encodings must be calculated\n",
    "    :return: positional encoding, a tensor of size (1, max_length, d_model)\n",
    "    \"\"\"\n",
    "    positional_encoding = torch.zeros((max_length, d_model))  # (max_length, d_model)\n",
    "    for i in range(max_length):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                positional_encoding[i, j] = math.sin(i / math.pow(10000, j / d_model))\n",
    "            else:\n",
    "                positional_encoding[i, j] = math.cos(i / math.pow(10000, (j - 1) / d_model))\n",
    "\n",
    "    positional_encoding = positional_encoding.unsqueeze(0)  # (1, max_length, d_model)\n",
    "\n",
    "    return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_queries, d_values, in_decoder=False):\n",
    "\n",
    "        super(HeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_keys = d_values # size of key vectors, same as of the query vectors to allow dot-products for similarity\n",
    "\n",
    "        self.in_decoder = in_decoder\n",
    "\n",
    "        self.create_queries = nn.Linear(d_model, d_queries, bias=False)\n",
    "        self.create_values = nn.Linear(d_model, d_values, bias=False)\n",
    "        self.create_keys = nn.Linear(d_model, d_values, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, real_lengths):\n",
    "\n",
    "        assert input.size(0) == 1, \"Don't complicate things. Just use a tensor with size (1, Sequence_length, d_model)\"\n",
    "\n",
    "        queries = self.create_queries(input) # (Batch, Sequences, d_queries)\n",
    "        keys = self.create_keys(input) # (Batch, Sequences, d_keys)\n",
    "        values = self.create_values(input) # (Batch, Sequences, d_values)\n",
    "\n",
    "        similarity_matrix = []\n",
    "\n",
    "        for batch in range(input.size(0)): # Actually 1\n",
    "\n",
    "            dot_product = torch.matmul(queries[batch], keys[batch].T)\n",
    "            similarity_matrix.append(dot_product.unsqueeze(0))\n",
    "\n",
    "        del dot_product\n",
    "\n",
    "        similarity_matrix = torch.cat(similarity_matrix, 0) # (Batch, Sequences, Sequences)\n",
    "\n",
    "        similarity_matrix = similarity_matrix/(math.sqrt(self.d_keys))\n",
    "\n",
    "        # Applying mask of -inf to ignore padded keys ---> Actually using -1e6 to avoid NaNs\n",
    "\n",
    "        if self.in_decoder:\n",
    "\n",
    "            # In the decoder, masks are shifted left to right:\n",
    "            # <Start-of-Sentence> [prediction1] [prediction2] [prediction3] ... <End-of-Sentence>\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]+1] = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        else:\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]] = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        del mask\n",
    "\n",
    "        attention_weights = self.softmax(similarity_matrix) # (Batch, Sequences, d_keys)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for batch in range(attention_weights.size(0)):\n",
    "\n",
    "            output = torch.matmul(attention_weights[batch], values[batch])\n",
    "            attention_output.append(output.unsqueeze(0))\n",
    "\n",
    "        del output\n",
    "\n",
    "        attention_output = torch.cat(attention_output, 0)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_inner):\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "\n",
    "        self.neuron1 = nn.Linear(d_model, d_inner)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.neuron2 = nn.Linear(d_inner, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, attention_output_cat):\n",
    "\n",
    "        sequences = self.neuron1(attention_output_cat)\n",
    "        sequences = self.Relu(sequences)\n",
    "\n",
    "        sequences = self.neuron2(sequences)\n",
    "\n",
    "        #output = sequences + attention_output_cat\n",
    "        output = sequences\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=False) for i in range(n_heads)])\n",
    "\n",
    "        self.neuron = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_input, real_input_length):\n",
    "\n",
    "        residual_block1 = encoder_input # (Batch, Sequence, d_model) ---> Vectors\n",
    "\n",
    "        vectors = self.dropout(encoder_input)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_heads[head](vectors, real_input_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuron(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        encoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output\n",
    "\n",
    "        encoded_sequence = encoded_sequence + residual_block2    \n",
    "\n",
    "        encoder_output = self.dropout(encoded_sequence)\n",
    "\n",
    "        del encoded_sequence, residual_block1, residual_block2\n",
    "\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_headsA = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "        self.attention_headsB = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "\n",
    "        self.neuronA = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "        self.neuronB = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_output, target_sequences, real_target_length):\n",
    "\n",
    "        residual_block1 = target_sequences\n",
    "\n",
    "        vectors = self.dropout(target_sequences)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsA[head](vectors, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronA(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_decoder = self.dropout(attention_output)\n",
    "        \n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsB[head](encoder_output, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronB(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        attention_output = attention_output + residual_block2\n",
    "\n",
    "        attention_encoder = self.dropout(attention_output)\n",
    "\n",
    "        decoded_sequence = attention_encoder + attention_decoder\n",
    "\n",
    "        residual_block3 = decoded_sequence\n",
    "\n",
    "        decoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output, attention_encoder, attention_decoder\n",
    "\n",
    "        decoded_sequence = decoded_sequence + residual_block3\n",
    "\n",
    "        decoder_output = self.dropout(decoded_sequence)  \n",
    "\n",
    "        del decoded_sequence, residual_block1, residual_block2, residual_block3\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broca(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The generator, which will generate the words that she'll speak.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dictionary,\n",
    "            output_dictionary,\n",
    "            positional_encoding,\n",
    "            d_model=512,\n",
    "            n_heads=8,\n",
    "            d_queries=64,\n",
    "            d_values=64,\n",
    "            d_inner=2056,\n",
    "            n_layers=6,\n",
    "            dropout=0.1\n",
    "    ):\n",
    "\n",
    "        super(Broca, self).__init__()\n",
    "\n",
    "        self.vocab_size = len(output_dictionary)\n",
    "        self.positional_encoding = positional_encoding.to(device)\n",
    "        self.input_dictionary = input_dictionary\n",
    "        self.output_dictionary = output_dictionary\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.positional_encoding.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        self.encoder = nn.ModuleList(\n",
    "            Encoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            Decoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.output_neuron = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "    '''def preprocess_dialogue(self, input_text, input_dictionary):\n",
    "\n",
    "        batch = []\n",
    "        sentence_sizes = []\n",
    "\n",
    "        for phrase in range(len(input_text)):\n",
    "\n",
    "            text = input_text[phrase]\n",
    "\n",
    "            text = text.split(' ')\n",
    "\n",
    "            tokens = []\n",
    "            \n",
    "            for word in text:\n",
    "\n",
    "                value = input_dictionary.index(word)\n",
    "                tokens.append(value)\n",
    "                tokens.append(input_dictionary.index(' '))\n",
    "\n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            if sentence_size < dataset.english_maximum_length:\n",
    "\n",
    "                pad_size = dataset.english_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=input_dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            batch.append(tokens)\n",
    "            sentence_sizes.append(sentence_size)\n",
    "\n",
    "        batch = torch.cat(batch, 0)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        return batch, sentence_sizes'''\n",
    "\n",
    "    def preprocess_dialogue(self, input_text):\n",
    "\n",
    "        for phrase in range(len(input_text)):\n",
    "\n",
    "            text = input_text[phrase]\n",
    "\n",
    "            text = text.split(' ')\n",
    "\n",
    "            tokens = []\n",
    "            \n",
    "            for word in text:\n",
    "\n",
    "                value = dataset.dictionary.index(word)\n",
    "                tokens.append(value)\n",
    "                tokens.append(dataset.dictionary.index(' '))\n",
    "\n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            if sentence_size < dataset.vocabulary_maximum_length:\n",
    "\n",
    "                pad_size = dataset.vocabulary_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=dataset.dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "\n",
    "        return tokens, sentence_size\n",
    "    \n",
    "    '''def preprocess_target(self, target_text, target_dictionary):\n",
    "    \n",
    "        target = []\n",
    "        target_sizes = []\n",
    "\n",
    "        for batch in range(len(target_text)):\n",
    "\n",
    "            text = target_text[batch]\n",
    "\n",
    "            tokens = []\n",
    "\n",
    "            for character in text:\n",
    "\n",
    "                #value = target_dictionary.index(character)\n",
    "\n",
    "                try: # Second forward passes in the same iteration may mix alphabetic characters with japanese\n",
    "\n",
    "                    value = dataset.dictionary.index(character)\n",
    "                \n",
    "                except: # In this case, ignore the alphabetic ones.\n",
    "\n",
    "                    value = dataset.dictionary.index('')\n",
    "                    \n",
    "                tokens.append(value)\n",
    "            \n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            #if sentence_size < dataset.japanese_maximum_length:\n",
    "            if sentence_size < dataset.vocabulary_maximum_length:\n",
    "\n",
    "                pad_size = dataset.english_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=dataset.dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            target.append(tokens)\n",
    "            target_sizes.append(sentence_size)\n",
    "\n",
    "        target = torch.cat(target, 0)\n",
    "        target = target.to(device)\n",
    "\n",
    "        return target, target_sizes'''\n",
    "    \n",
    "    def generate_sentences(self, input):\n",
    "\n",
    "        #input, real_input_length = self.preprocess_dialogue(input, self.input_dictionary)\n",
    "        input, real_input_length = self.preprocess_dialogue(input)\n",
    "\n",
    "        #target_indices = [[self.output_dictionary.index(\"<SOS>\")]] * input.size(0)\n",
    "\n",
    "        #target_sentence = torch.tensor(target_indices, device=device) # (Batch, 1)\n",
    "\n",
    "        target_indices = [dataset.dictionary.index(\"<SOS>\")]\n",
    "\n",
    "        target_sentence = torch.tensor(target_indices, device=device).unsqueeze(0)\n",
    "\n",
    "        real_target_length = len(target_indices)-1\n",
    "\n",
    "        #real_target_length = [len(target_indices[x]) for x in range(len(target_indices))]\n",
    "\n",
    "        encoder_vectors = self.embedding(input) + math.sqrt(self.d_model)\n",
    "\n",
    "        decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # Embed out = (Batch, d_model)\n",
    "\n",
    "        output = self.forward(encoder_vectors, real_input_length, decoder_vectors, real_target_length) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "        output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "        target_indices.append(output_index[:, -1].item()) # When Batch = 1 ---> target_indices = (1+1) --> (2+1) --> (3+1 ...)\n",
    "\n",
    "        target_indices.pop(0) # Removing Start-of-Sentence token. We won't need it anymore I guess.\n",
    "        \n",
    "        # Generating text until reaching End of Sentence --> Beam Search\n",
    "\n",
    "        while target_indices[-1] != dataset.dictionary.index(\"<EOS>\"):\n",
    "\n",
    "            target_pad = encoder_vectors.size(1) - len(target_indices)\n",
    "\n",
    "            target_sentence = torch.tensor(target_indices + [dataset.dictionary.index(\"<pad>\")]*target_pad, device=device).unsqueeze(0)\n",
    "\n",
    "            decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # (Batch, sequence_length, d_model)\n",
    "\n",
    "            output = self.forward(encoder_vectors, real_input_length, decoder_vectors) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "            output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "            target_indices.append(output_index[:, -1].item()) # When Batch = 1 ---> target_indices = (1+1) --> (2+1) --> (3+1 ...)\n",
    "\n",
    "            if len(target_indices) > encoder_vectors.size(1): # Otherwise, we'll get errors.\n",
    "\n",
    "                break\n",
    "\n",
    "        return target_indices, output[:, -1]\n",
    "\n",
    "        '''for batch in range(output.size(0)):\n",
    "\n",
    "            target_indices[batch].append(output_index[batch, -1].item())\n",
    "            target_indices[batch].pop(0) # Removing Start-of-Sentence token. We won't need it anymore.\n",
    "        \n",
    "        # Generating text until reaching End of Sentence --> Beam Search\n",
    "\n",
    "        target_outputs = []\n",
    "\n",
    "        for batch in range(output.size(0)):\n",
    "\n",
    "            while target_indices[batch][-1] != self.output_dictionary.index(\"<EOS>\"):\n",
    "\n",
    "                target_pad = encoder_vectors.size(1) - len(target_indices[batch])\n",
    "\n",
    "                target_sentence = torch.tensor(target_indices[batch] + [self.output_dictionary.index(\"<pad>\")]*target_pad, device=device).unsqueeze(0)\n",
    "\n",
    "                real_target_length = [len(target_indices[x]) for x in range(len(target_indices))]\n",
    "\n",
    "                decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # (Batch, sequence_length, d_model)\n",
    "\n",
    "                output = self.forward(encoder_vectors[batch].unsqueeze(0), [real_input_length[batch]], decoder_vectors, [real_target_length[batch]]) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "                output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "                target_indices[batch].append(output_index[:, -1].item()) # When Batch = 1 ---> target_indices = (1+1) --> (2+1) --> (3+1 ...)\n",
    "\n",
    "                if len(target_indices[batch])+3 > encoder_vectors.size(1): # Otherwise, we'll get errors.\n",
    "                    # Though sometimes those errors still happen...I don't know why...\n",
    "\n",
    "                    break\n",
    "\n",
    "            target_outputs.append(output)\n",
    "\n",
    "        target_outputs = torch.cat(target_outputs, 0)\n",
    "\n",
    "        return target_indices, target_outputs'''\n",
    "\n",
    "\n",
    "    def forward(self, encoder_vectors, real_input_length, decoder_vectors, real_target_length): # Target Sentence (with <SOS> token) provided before forward function\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "\n",
    "            encoder_vectors = self.encoder[layer](encoder_vectors, real_input_length)\n",
    "\n",
    "            decoder_vectors = self.decoder[layer](encoder_vectors, decoder_vectors, real_target_length)\n",
    "\n",
    "        output = self.output_neuron(decoder_vectors) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        output = self.softmax(output) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        return output # Probability of words. Take the one with highest probability, add it to the target sentence and repeat.\n",
    "    \n",
    "    def talk2me(self, encoded_text): # Easier to write than 話してください\n",
    "\n",
    "        removal = [\"<SOS>\", \"<EOS>\", \"<pad>\"]\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for batch in encoded_text:\n",
    "\n",
    "            words = []\n",
    "\n",
    "            for i in batch:\n",
    "\n",
    "                words.append(self.output_dictionary[i])\n",
    "\n",
    "            words = [x for x in words if x not in removal]\n",
    "            \n",
    "            phrase = ''.join(words)\n",
    "\n",
    "            sentences.append(phrase)\n",
    "\n",
    "        return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.english_dictionary))\n",
    "print(len(dataset.japanese_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_maximum_length)\n",
    "print(dataset.japanese_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 16])\n"
     ]
    }
   ],
   "source": [
    "#positional_encoding = get_positional_encoding(d_model=16, max_length=max(dataset.english_maximum_length, dataset.japanese_maximum_length))\n",
    "\n",
    "positional_encoding = get_positional_encoding(d_model=16, max_length=dataset.vocabulary_maximum_length)\n",
    "\n",
    "print(positional_encoding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Broca(\n",
    "    #dataset.english_dictionary,\n",
    "    #dataset.japanese_dictionary,\n",
    "    dataset.dictionary,\n",
    "    dataset.dictionary,\n",
    "    positional_encoding,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_queries=64,\n",
    "    d_values=64,\n",
    "    d_inner=2048,\n",
    "    n_layers=6,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so as for my life', 'or anyones life', 'sometimes and often times']\n"
     ]
    }
   ],
   "source": [
    "inputest = dataset.english_phrases[0:3]\n",
    "\n",
    "print(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste, probabilities = model.generate_sentences(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2, teste3 = model.talk2me(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[237, 151, 28, 28, 242, 24, 148, 28, 28, 6, 6, 28, 28, 206, 28, 28, 28, 122, 17, 28, 28, 93, 28, 73, 28, 73, 28, 6, 28, 28, 28, 28, 28, 28, 206, 206, 28, 73, 85, 73, 28, 24, 28, 212, 28, 206, 33, 237, 116, 73, 6, 28, 24, 28, 35, 225, 29, 28, 28, 28, 206, 6, 73, 116, 206, 28, 28, 206, 77, 24, 35, 110, 56, 28, 28, 79, 206, 28, 50, 28, 73, 43, 28, 28, 24, 28, 28, 28, 28, 28, 6, 28, 33, 206, 28, 28, 221], [237, 151, 28, 28, 242, 24, 148, 28, 28, 6, 6, 28, 28, 206, 28, 28, 28, 122, 17, 28, 28, 93, 28, 73, 28, 73, 28, 6, 28, 28, 28, 28, 28, 28, 206, 206, 28, 73, 85, 73, 28, 24, 28, 212, 28, 206, 33, 237, 116, 73, 6, 28, 24, 28, 35, 225, 29, 28, 28, 28, 206, 6, 73, 116, 206, 28, 28, 206, 77, 24, 35, 110, 56, 28, 28, 79, 206, 28, 50, 28, 73, 43, 28, 28, 24, 28, 28, 28, 28, 28, 6, 28, 33, 206, 28, 28, 221], [237, 151, 28, 28, 242, 24, 148, 28, 28, 6, 6, 28, 28, 206, 28, 28, 28, 122, 17, 28, 28, 93, 28, 73, 28, 73, 28, 6, 28, 28, 28, 28, 28, 28, 206, 206, 28, 73, 85, 73, 28, 24, 28, 212, 28, 206, 33, 237, 116, 73, 6, 28, 24, 28, 35, 225, 29, 28, 28, 28, 206, 6, 73, 116, 206, 28, 28, 206, 77, 24, 35, 110, 56, 28, 28, 79, 206, 28, 50, 28, 73, 43, 28, 28, 24, 28, 28, 28, 28, 28, 6, 28, 33, 206, 28, 28, 221]]\n",
      "['ずにoccasionaloccasionalはalong々occasionaloccasionalforforoccasionaloccasionalじoccasionaloccasionaloccasionalforgivenequallyrightoccasionaloccasionalbeoccasionalohoccasionalohoccasionalforoccasionaloccasionaloccasionaloccasionaloccasionaloccasionalじじoccasionalohfarohoccasionalalongoccasional間occasionalじontoずcantohforoccasionalalongoccasionaldo契rainoccasionaloccasionaloccasionalじforohcantじoccasionaloccasionalじstumblealongdogavequiteoccasionaloccasionaldontじoccasionalwrittenoccasionalohtakeoccasionaloccasionalalongoccasionaloccasionaloccasionaloccasionaloccasionalforoccasionalontoじoccasionaloccasional傷', 'ずにoccasionaloccasionalはalong々occasionaloccasionalforforoccasionaloccasionalじoccasionaloccasionaloccasionalforgivenequallyrightoccasionaloccasionalbeoccasionalohoccasionalohoccasionalforoccasionaloccasionaloccasionaloccasionaloccasionaloccasionalじじoccasionalohfarohoccasionalalongoccasional間occasionalじontoずcantohforoccasionalalongoccasionaldo契rainoccasionaloccasionaloccasionalじforohcantじoccasionaloccasionalじstumblealongdogavequiteoccasionaloccasionaldontじoccasionalwrittenoccasionalohtakeoccasionaloccasionalalongoccasionaloccasionaloccasionaloccasionaloccasionalforoccasionalontoじoccasionaloccasional傷', 'ずにoccasionaloccasionalはalong々occasionaloccasionalforforoccasionaloccasionalじoccasionaloccasionaloccasionalforgivenequallyrightoccasionaloccasionalbeoccasionalohoccasionalohoccasionalforoccasionaloccasionaloccasionaloccasionaloccasionaloccasionalじじoccasionalohfarohoccasionalalongoccasional間occasionalじontoずcantohforoccasionalalongoccasionaldo契rainoccasionaloccasionaloccasionalじforohcantじoccasionaloccasionalじstumblealongdogavequiteoccasionaloccasionaldontじoccasionalwrittenoccasionalohtakeoccasionaloccasionalalongoccasionaloccasionaloccasionaloccasionaloccasionalforoccasionalontoじoccasionaloccasional傷']\n",
      "['ず', 'に', 'occasional', 'occasional', 'は', 'along', '々', 'occasional', 'occasional', 'for', 'for', 'occasional', 'occasional', 'じ', 'occasional', 'occasional', 'occasional', 'forgiven', 'equallyright', 'occasional', 'occasional', 'be', 'occasional', 'oh', 'occasional', 'oh', 'occasional', 'for', 'occasional', 'occasional', 'occasional', 'occasional', 'occasional', 'occasional', 'じ', 'じ', 'occasional', 'oh', 'far', 'oh', 'occasional', 'along', 'occasional', '間', 'occasional', 'じ', 'onto', 'ず', 'cant', 'oh', 'for', 'occasional', 'along', 'occasional', 'do', '契', 'rain', 'occasional', 'occasional', 'occasional', 'じ', 'for', 'oh', 'cant', 'じ', 'occasional', 'occasional', 'じ', 'stumble', 'along', 'do', 'gave', 'quite', 'occasional', 'occasional', 'dont', 'じ', 'occasional', 'written', 'occasional', 'oh', 'take', 'occasional', 'occasional', 'along', 'occasional', 'occasional', 'occasional', 'occasional', 'occasional', 'for', 'occasional', 'onto', 'じ', 'occasional', 'occasional', '傷']\n"
     ]
    }
   ],
   "source": [
    "print(teste)\n",
    "print(teste2)\n",
    "print(teste3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 96, 252])\n",
      "tensor([-17.6027,  -8.5462, -13.6292, -14.3481, -14.1583, -19.4754, -15.0795,\n",
      "        -14.5429, -10.4098, -10.6503, -10.8837, -12.6462, -11.9500, -13.4707,\n",
      "         -7.1664, -21.4530, -15.4631, -23.8110,  -8.0562,  -6.4433, -10.1797,\n",
      "        -13.5840, -11.1416,  -2.4481, -14.9709,  -9.6160,  -2.9300,  -4.4820,\n",
      "         -9.3233,  -9.3733, -17.5346, -18.0314,  -2.5274,  -3.0909,  -7.3750,\n",
      "        -10.9545, -19.0578, -15.7094, -12.0697,  -6.3057,  -9.5539, -13.9650,\n",
      "         -9.8632, -16.5653, -17.1872, -17.5678, -12.0377, -19.8527, -10.6320,\n",
      "         -5.1805,  -7.5369, -17.5443, -10.5237, -12.0650,  -5.7778, -14.0523,\n",
      "        -15.1287, -20.5018, -17.5003,  -5.3902, -12.8141, -13.7247,  -8.4571,\n",
      "         -8.4310, -10.3895, -16.4738, -15.4045, -12.4513, -11.5450, -11.7634,\n",
      "         -3.5700,  -7.5764, -15.8845, -11.6086,  -8.4543, -13.1408,  -9.3030,\n",
      "        -13.9942,  -6.3783, -16.5325, -18.0355, -15.6293,  -9.0572, -15.1278,\n",
      "        -15.0507, -14.3985, -10.3614, -13.5764, -13.3565,  -8.0503, -10.4671,\n",
      "        -14.4972, -15.7587, -17.3344, -10.7162,  -8.3519, -11.1886, -12.8500,\n",
      "        -16.8032, -18.3527, -14.9289,  -8.8957,  -9.1593, -11.6212, -12.4294,\n",
      "        -18.3110, -14.6278,  -8.1773, -18.9945,  -8.6864, -17.2683, -12.3286,\n",
      "         -2.0661, -11.7409, -15.7892, -11.7006, -14.1161, -20.5705, -21.6851,\n",
      "         -8.9450,  -8.8835,  -5.3146, -15.4097,  -2.9394, -14.9936, -11.9703,\n",
      "         -5.1236,  -7.8656,  -7.7927, -15.3299, -18.6239,  -9.3572, -20.4291,\n",
      "        -18.0591, -12.8748, -17.7877,  -8.2528, -17.1998,  -7.7911, -11.7687,\n",
      "         -7.4081, -14.9214, -11.7016, -12.5900,  -2.0488,  -6.8658, -16.0339,\n",
      "         -6.0475, -14.1305,  -8.4088, -13.2980, -15.6494,  -9.6930,  -8.9826,\n",
      "         -7.4653, -12.8181, -16.1965, -12.8282,  -2.1789, -20.1520, -15.0554,\n",
      "        -14.6847, -16.0878, -15.2936, -19.6864, -11.0075,  -9.6026,  -7.9574,\n",
      "        -17.0328, -17.5708, -13.3671,  -9.4573, -18.7690,  -7.9993, -14.4434,\n",
      "        -13.2118, -22.0609,  -9.9498, -12.2587, -17.0861,  -7.0537,  -6.3510,\n",
      "         -6.9172, -11.5474,  -8.0477, -19.4485,  -6.3033, -17.3935, -14.9041,\n",
      "        -16.1304, -14.5721, -11.9061,  -2.3957, -10.5778,  -7.5509, -19.4868,\n",
      "        -18.1221, -14.1361,  -9.6065, -13.2653, -19.8069,  -6.7528, -22.4048,\n",
      "         -4.7692, -11.0850, -22.1045,  -5.4936,  -7.2318,  -7.4122, -12.7717,\n",
      "        -16.3703, -11.4417, -14.0653,  -8.9104, -10.7952,  -5.4286, -10.0548,\n",
      "         -4.8574,  -8.1558,  -3.9020, -24.5815, -10.2444, -18.0680, -13.8741,\n",
      "         -8.5378, -12.9360, -18.3247,  -9.0498, -13.0598, -16.9726, -15.9028,\n",
      "         -9.9569, -13.7275,  -4.4553, -11.2069, -24.1188, -13.2858, -11.5332,\n",
      "         -9.6111, -14.6178, -16.3961, -16.9104,  -6.5702, -12.2630,  -7.7997,\n",
      "        -12.6781, -15.0910, -20.1460,  -2.6473, -10.6507,  -9.1196,  -7.7265],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probabilities.size())\n",
    "print(probabilities[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "[42, 24, 39, 17, 22, 19, 3, 20, 32, 16, 0, 35, 29, 9, 30, 33, 2, 12, 23, 26, 21, 25, 28, 41, 4, 14, 27, 8, 5, 11, 36, 37, 13, 7, 6, 1, 34, 40, 15, 43, 38, 18, 31, 10]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "input_texts = dataset.english_phrases\n",
    "target_texts = dataset.japanese_phrases\n",
    "\n",
    "batch_shuffle = [i for i in range(min(len(input_texts), len(target_texts)))]\n",
    "# The kanji and the lack of space in japanese sentences makes them prone to lower lengths\n",
    "shuffle(batch_shuffle)\n",
    "\n",
    "print(len(batch_shuffle))\n",
    "print(batch_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "#BATCH_SIZE = 4\n",
    "TARGET_SIZE = dataset.japanese_maximum_length\n",
    "\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the Transformer uses the Target sentences as input\n",
    "makes it fall into exposure bias.\n",
    "\n",
    "It's easy to answer a question correctly when you are given the correct answer.\n",
    "\n",
    "This makes the evaluation mode rubbish and the Beam Search/Greedy Search more costly\n",
    "\n",
    "\n",
    "There are basically two ways of overcoming this problem:\n",
    "\n",
    "-> Schedule Sampling: https://aclanthology.org/P19-2049/#:~:text=Scheduled%20sampling%20is%20a%20technique,previous%20step%20in%20training%20time.\n",
    "\n",
    "-> Reinforcement Learning: https://arxiv.org/abs/2203.02155; https://arxiv.org/pdf/2303.08774.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15956924946123402\n",
      "1.7552926369521265\n",
      "3.3510587900891546\n",
      "4.9468485606022226\n",
      "6.542642799493576\n",
      "8.138422357420778\n",
      "9.734168085079688\n",
      "11.329860833587446\n",
      "12.925481454865453\n",
      "14.521010802022314\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean = 50\n",
    "std = 5000\n",
    "\n",
    "for i in range(1, 100, 10):\n",
    "\n",
    "    A = ((i - mean)**2) / (2*(std**2))\n",
    "\n",
    "    num = math.e ** (-A)\n",
    "    den = std * math.sqrt(math.pi*2)\n",
    "\n",
    "    y = (num/den) * (i/mean) * 1e5\n",
    "\n",
    "    print(y)\n",
    "    plt.scatter(i, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_function(current_epoch, scale=1e5):\n",
    "\n",
    "    MEAN = 50\n",
    "    STD = 5000\n",
    "\n",
    "    A = ((current_epoch - MEAN)**2) / (2*(STD**2))\n",
    "\n",
    "    num = math.e ** (-A)\n",
    "    den = STD * math.sqrt(math.pi*2)\n",
    "\n",
    "    y = (num/den) * (current_epoch/MEAN) * scale\n",
    "\n",
    "    return min(y, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_sampler(original_target, generated_target, sampler_chance=0.1):\n",
    "\n",
    "    new_target = []\n",
    "\n",
    "    for batch in range(max(len(original_target), len(generated_target))):\n",
    "\n",
    "        target = []\n",
    "\n",
    "        try:\n",
    "\n",
    "            candidateA = original_target[batch]\n",
    "        \n",
    "        except IndexError:\n",
    "\n",
    "            new_target.append(generated_target[batch])\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            candidateB = generated_target[batch]\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            new_target.append(original_target[batch])\n",
    "            continue\n",
    "\n",
    "        for item in range(max(len(candidateA), len(candidateB))):\n",
    "\n",
    "            if torch.rand((1,)) < sampler_chance:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    target.append(candidateB[item])\n",
    "\n",
    "                except IndexError:\n",
    "\n",
    "                    target.append(candidateA[item])\n",
    "\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    target.append(candidateA[item])\n",
    "\n",
    "                except IndexError:\n",
    "\n",
    "                    target.append(candidateB[item])\n",
    "        \n",
    "        new_target.append(target)\n",
    "\n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "\n",
    "    shuffle(batch_shuffle)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in range(0, len(batch_shuffle)):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        input_text = input_texts[batch]\n",
    "        target_text = target_texts[batch]\n",
    "        target_text = \"<SOS>\" + target_text # Adding Start-Of-Sentence token\n",
    "\n",
    "        target, target_length = model.preprocess_dialogue(target_text)\n",
    "        target_length = target_length - 1 # Removing SOS token space\n",
    "\n",
    "        input, real_input_length = model.preprocess_dialogue(input_text)\n",
    "\n",
    "        encoder_vectors = model.embedding(input) + math.sqrt(model.d_model)\n",
    "        decoder_vectors = model.embedding(target) + math.sqrt(model.d_model)\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors, target_length)\n",
    "\n",
    "        target_vector = torch.zeros_like(output).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "        cost1 = loss(output[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "\n",
    "        generated_target, _ = model.talk2me(output[:, :TARGET_SIZE].argmax(-1))\n",
    "        sampler_chance = sampler_function(epoch, scale=1e4)\n",
    "        #sampler_chance = 0.5\n",
    "        new_target = schedule_sampler(target_text, generated_target, sampler_chance)\n",
    "        new_target, target_length = model.preprocess_dialogue(new_target[0])\n",
    "\n",
    "        decoder_vectors = model.embedding(new_target) + math.sqrt(model.d_model)\n",
    "\n",
    "        #print(encoder_vectors.size(), decoder_vectors.size())\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors, target_length)\n",
    "\n",
    "        #print(output.size(), target_vector.size())\n",
    "\n",
    "        cost2 = loss(output[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "\n",
    "        total_cost = cost1 + (cost2 * (epoch/100))\n",
    "\n",
    "        total_cost.backward()\n",
    "\n",
    "        total_loss += total_cost\n",
    "\n",
    "        grads = torch.mean(model.encoder[0].attention_heads[0].create_queries.weight.grad)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "\n",
    "            print(f\"Current Iteration: {iters}\\tSampler chance: {sampler_chance}\")\n",
    "            print(f\"Last Loss: {total_cost.item()}\\tBatch Loss: {total_loss/iters}\")\n",
    "            print(f\"Gradients Average: {grads}\")\n",
    "            print(f\"Generating Text...\")\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            generated_text, possibilities = model.generate_sentences(input_text)\n",
    "\n",
    "            target_vector = torch.zeros_like(possibilities).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "            cost = loss(possibilities[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "            cost.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            generated_text, _ = model.talk2me(generated_text)\n",
    "\n",
    "            print(f\"Evaluation Loss: {cost.item()}\")\n",
    "            print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration: 600\tSampler chance: 0.07978522471325092\n",
      "Last Loss: 77.35613250732422\tBatch Loss: 0.12892688810825348\n",
      "Gradients Average: -0.0581555999815464\n",
      "Generating Text...\n",
      "Evaluation Loss: 68.41954040527344\n",
      "['とtheyreflowerstaking憎closeちtrust寄っorleaningorderわto諦たleaningたbecausecant思yourた憎cant番君often憎じと憎because思quiteleaningたたた残anyonesourgaveたareourevenた憎夢oftencold間あiasevendreamsevenたたdreami憎anyones寄憎と間憎iたandbecauseったとcoldleaning憎leaningleaningourbecause持judgedた憎寄花 憎often残theyre', 'とtheyreflowerstaking憎closeちtrust寄っorleaningorderわto諦たleaningたbecausecant思yourた憎cant番君often憎じと憎because思quiteleaningたたた残anyonesourgaveたareourevenた憎夢oftencold間あiasevendreamsevenたたdreami憎anyones寄憎と間憎iたandbecauseったとcoldleaning憎leaningleaningourbecause持judgedた憎寄花 憎often残theyre', 'とtheyreflowerstaking憎closeちtrust寄っorleaningorderわto諦たleaningたbecausecant思yourた憎cant番君often憎じと憎because思quiteleaningたたた残anyonesourgaveたareourevenた憎夢oftencold間あiasevendreamsevenたたdreami憎anyones寄憎と間憎iたandbecauseったとcoldleaning憎leaningleaningourbecause持judgedた憎寄花 憎often残theyre', 'とtheyreflowerstaking憎closeちtrust寄っorleaningorderわto諦たleaningたbecausecant思yourた憎cant番君often憎じと憎because思quiteleaningたたた残anyonesourgaveたareourevenた憎夢oftencold間あiasevendreamsevenたたdreami憎anyones寄憎と間憎iたandbecauseったとcoldleaning憎leaningleaningourbecause持judgedた憎寄花 憎often残theyre']\n",
      "Current Iteration: 700\tSampler chance: 0.223401886372861\n",
      "Last Loss: 78.6220932006836\tBatch Loss: 0.2311704307794571\n",
      "Gradients Average: 1.5005488395690918\n",
      "Generating Text...\n",
      "Evaluation Loss: 72.19296264648438\n",
      "['寄judgedる繰take総orderるtake返何take総painfulsettle総takeるcomejudgedorder憎isる集stumbletakejudgedontosettletakeorderorderintoるるるwritten僕easilytrusttake僕is許僕るcarecome願見away総た寒るるdarkorder葉葉quiteる苦quite葉るcare子葉judgedるjudged総careめ る何takesettle総takecomecant葉youら憎skiesょ僕葉るtake葉', '寄judgedる繰take総orderるtake返何take総painfulsettle総takeるcomejudgedorder憎isる集stumbletakejudgedontosettletakeorderorderintoるるるwritten僕easilytrusttake僕is許僕るcarecome願見away総た寒るるdarkorder葉葉quiteる苦quite葉るcare子葉judgedるjudged総careめ る何takesettle総takecomecant葉youら憎skiesょ僕葉るtake葉', '寄judgedる繰take総orderるtake返何take総painfulsettle総takeるcomejudgedorder憎isる集stumbletakejudgedontosettletakeorderorderintoるるるwritten僕easilytrusttake僕is許僕るcarecome願見away総た寒るるdarkorder葉葉quiteる苦quite葉るcare子葉judgedるjudged総careめ る何takesettle総takecomecant葉youら憎skiesょ僕葉るtake葉', '寄judgedる繰take総orderるtake返何take総painfulsettle総takeるcomejudgedorder憎isる集stumbletakejudgedontosettletakeorderorderintoるるるwritten僕easilytrusttake僕is許僕るcarecome願見away総た寒るるdarkorder葉葉quiteる苦quite葉るcare子葉judgedるjudged総careめ る何takesettle総takecomecant葉youら憎skiesょ僕葉るtake葉']\n",
      "Current Iteration: 800\tSampler chance: 0.3670215467561561\n",
      "Last Loss: 159.219970703125\tBatch Loss: 0.578673779964447\n",
      "Gradients Average: -2.28312611579895\n",
      "Generating Text...\n",
      "Evaluation Loss: 129.2141571044922\n",
      "['もmistake微のmistakeやや供toasよ飲くforcameでforや空繰to込close躓forme愛供smile暗forforgivenwritten繰思intopainfuliれ追供foroften誰rain間供憎繰追fortoつう繰smile追hurt伝供追なonto追追近writtenち追even簡takingthrewonto繰飲て切smile繰伝飲て願convey繰日追runningよeven繰繰追葉葉追', 'もmistake微のmistakeやや供toasよ飲くforcameでforや空繰to込close躓forme愛供smile暗forforgivenwritten繰思intopainfuliれ追供foroften誰rain間供憎繰追fortoつう繰smile追hurt伝供追なonto追追近writtenち追even簡takingthrewonto繰飲て切smile繰伝飲て願convey繰日追runningよeven繰繰追葉葉追', 'もmistake微のmistakeやや供toasよ飲くforcameでforや空繰to込close躓forme愛供smile暗forforgivenwritten繰思intopainfuliれ追供foroften誰rain間供憎繰追fortoつう繰smile追hurt伝供追なonto追追近writtenち追even簡takingthrewonto繰飲て切smile繰伝飲て願convey繰日追runningよeven繰繰追葉葉追', 'もmistake微のmistakeやや供toasよ飲くforcameでforや空繰to込close躓forme愛供smile暗forforgivenwritten繰思intopainfuliれ追供foroften誰rain間供憎繰追fortoつう繰smile追hurt伝供追なonto追追近writtenち追even簡takingthrewonto繰飲て切smile繰伝飲て願convey繰日追runningよeven繰繰追葉葉追']\n"
     ]
    }
   ],
   "source": [
    "'''for epoch in range(30):\n",
    "\n",
    "    shuffle(batch_shuffle)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in range(0, len(batch_shuffle), BATCH_SIZE):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        current_batch = batch_shuffle[batch:min(len(batch_shuffle)-1, batch+BATCH_SIZE)]\n",
    "\n",
    "        input_text, target_text = [], []\n",
    "\n",
    "        for i in range(len(current_batch)):\n",
    "\n",
    "            input_text.append(input_texts[current_batch[i]])\n",
    "            target_text.append(target_texts[current_batch[i]])\n",
    "\n",
    "        #input, real_input_length = model.preprocess_dialogue(input_text, dataset.english_dictionary)\n",
    "        #target, target_length = model.preprocess_target(target_text, dataset.japanese_dictionary)\n",
    "        target, target_length = model.preprocess_target(target_text, dataset.dictionary)\n",
    "\n",
    "        input, real_input_length = model.preprocess_dialogue(input_text)\n",
    "\n",
    "        #print(real_input_length)\n",
    "        #print(target_length)\n",
    "\n",
    "        encoder_vectors = model.embedding(input) + math.sqrt(model.d_model)\n",
    "        decoder_vectors = model.embedding(target) + math.sqrt(model.d_model)\n",
    "\n",
    "        #print(encoder_vectors.size(), decoder_vectors.size())\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors, target_length)\n",
    "\n",
    "        target_vector = torch.zeros_like(output).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "        #print(output.size(), target_vector.size())\n",
    "\n",
    "        cost1 = loss(output[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "\n",
    "        generated_target, _ = model.talk2me(output[:, :TARGET_SIZE].argmax(-1))\n",
    "        sampler_chance = sampler_function(epoch, scale=1e4)\n",
    "        #sampler_chance = 0.5\n",
    "        new_target = schedule_sampler(target_text, generated_target, sampler_chance)\n",
    "        #new_target, target_length = model.preprocess_target(new_target, dataset.japanese_dictionary)\n",
    "        new_target, target_length = model.preprocess_target(new_target, dataset.dictionary)\n",
    "\n",
    "        decoder_vectors = model.embedding(new_target) + math.sqrt(model.d_model)\n",
    "\n",
    "        #print(encoder_vectors.size(), decoder_vectors.size())\n",
    "\n",
    "        output = model.forward(encoder_vectors, real_input_length, decoder_vectors, target_length)\n",
    "\n",
    "        #print(output.size(), target_vector.size())\n",
    "\n",
    "        cost2 = loss(output[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "\n",
    "        total_cost = cost1 + (cost2 * (epoch/100))\n",
    "\n",
    "        total_cost.backward()\n",
    "\n",
    "        total_loss += total_cost\n",
    "\n",
    "        grads = torch.mean(model.encoder[0].attention_heads[0].create_queries.weight.grad)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "\n",
    "            print(f\"Current Iteration: {iters}\\tSampler chance: {sampler_chance}\")\n",
    "            print(f\"Last Loss: {total_cost.item()}\\tBatch Loss: {total_loss/iters}\")\n",
    "            print(f\"Gradients Average: {grads}\")\n",
    "            print(f\"Generating Text...\")\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            generated_text, possibilities = model.generate_sentences(input_text)\n",
    "\n",
    "            target_vector = torch.zeros_like(possibilities).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "            cost = loss(possibilities[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "            cost.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            generated_text, _ = model.talk2me(generated_text)\n",
    "\n",
    "            print(f\"Evaluation Loss: {cost.item()}\")\n",
    "            print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning --> Deep Q-Learning\n",
    "\n",
    "For each token in the input sentence, the Transformer tries to select the most likely token for the target sentence.\n",
    "\n",
    "In this case, we can consider that,\n",
    "for each observation (input token), the Transformer must select the action (target token) that will provide the highest value for the next observation\n",
    "\n",
    "\n",
    "## ATTENTION: Reinforcement Learning can be too unstable. If you fail to make the model learn appropriately through this method, it may be wise to train it initially using Schedule Sampler, and then apply Reinforcement Learning to fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Reward Function ---> BLEU Score (The higher, the better)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class BleuScore:\n",
    "    \"\"\"\n",
    "    Adapted from https://stackoverflow.com/questions/56968434/bleu-score-in-python-from-scratch\n",
    "\n",
    "    The input tensors are converted to strings so the n-grams can be extracted and the Bleu Score measured.\n",
    "\n",
    "    Args:\n",
    "        candidate_sentence: Tensor of shape (sentence_length, ) containing the values which corresponds to each token.\n",
    "        target_sentence: Tensor of shape (length, ) which will be used as reference.\n",
    "        max_n: The maximum n-gram we'll use. E.g. if max_n=3, we'll use unigrams, bigrams and trigrams.\n",
    "            For compatibility reasons, the max_n must be equal to or smaller than sentence_length\n",
    "            Default = 4\n",
    "        weights: a list of weights to be used for each n_gram category (uniform by default).\n",
    "        dictionary: a dictionary to be used for translating the tensors into strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_n=4, weights=[0.25]*4, dictionary=dataset.dictionary):\n",
    "\n",
    "        self.max_n = max_n\n",
    "        self.weights = weights\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "\n",
    "    def _n_gram_generator(self, sentence, n=2,remove_repeating=False):\n",
    "\n",
    "        sentence = sentence.lower() # converting to lower case\n",
    "        sent_arr = np.array(sentence.split()) # split to string arrays\n",
    "        length = len(sent_arr)\n",
    "\n",
    "        word_list = []\n",
    "        for i in range(length+1):\n",
    "            if i < n:\n",
    "                continue\n",
    "            word_range = list(range(i-n,i))\n",
    "            s_list = sent_arr[word_range]\n",
    "            string = ' '.join(s_list) # converting list to strings\n",
    "            word_list.append(string) # append to word_list\n",
    "            if remove_repeating:\n",
    "                word_list = list(set(word_list))\n",
    "        return word_list\n",
    "\n",
    "    \n",
    "    def bleu_score(self, original,machine_translated):\n",
    "        '''\n",
    "        Bleu score function given an original and a machine translated sentences\n",
    "        '''\n",
    "\n",
    "        original, _ = dataset.detokenize(original, self.dictionary)\n",
    "\n",
    "        machine_translated, _ = dataset.detokenize(machine_translated, self.dictionary)\n",
    "\n",
    "\n",
    "        mt_length = len(machine_translated.split())\n",
    "        o_length = len(original.split())\n",
    "\n",
    "        # Brevity Penalty \n",
    "        if mt_length>o_length:\n",
    "            BP=1\n",
    "        else:\n",
    "            penality=1-(mt_length/o_length)\n",
    "            BP=np.exp(penality)\n",
    "\n",
    "        # Clipped precision\n",
    "        clipped_precision_score = []\n",
    "\n",
    "        for i in range(1, self.max_n+1):\n",
    "            original_n_gram = Counter(self._n_gram_generator(original,i))\n",
    "            machine_n_gram = Counter(self._n_gram_generator(machine_translated,i))\n",
    "\n",
    "            counter = len(machine_n_gram.values())\n",
    "\n",
    "            if counter == 0:\n",
    "                counter += 1\n",
    "            \n",
    "            for j in machine_n_gram:\n",
    "                if j in original_n_gram:\n",
    "                    if machine_n_gram[j] > original_n_gram[j]:\n",
    "                        machine_n_gram[j] = original_n_gram[j]\n",
    "                else:\n",
    "                    machine_n_gram[j] = 0\n",
    "\n",
    "            clipped_precision = sum(machine_n_gram.values())/counter\n",
    "            clipped_precision_score.append(clipped_precision)\n",
    "\n",
    "        weights = self.weights\n",
    "\n",
    "        scores = []\n",
    "        dscores = []\n",
    "\n",
    "        for weight, precision in zip(weights, clipped_precision_score):\n",
    "            w_i = weight\n",
    "            p_i = precision\n",
    "\n",
    "            if p_i == 0.0:\n",
    "                score = (w_i * math.log(0.0001)) # Log of 0 tends to -infinite. But let's just stick to 0.0001 so a single wrong word won't break the score.\n",
    "                                                # Since log of 0.0001 = -9.21, which is also a quite big negative number(?)\n",
    "                dscore = (w_i/math.log(0.0001)) # Again, a division by -infinite would result in 0, which would also break the derivative.\n",
    "\n",
    "            else:\n",
    "                score = (w_i * math.log(p_i))\n",
    "\n",
    "                if p_i != 1.0:\n",
    "                    dscore = (w_i/math.log(p_i))\n",
    "                else:\n",
    "                    dscore = 0\n",
    "        \n",
    "            scores.append(score)\n",
    "            dscores.append(dscore)\n",
    "\n",
    "        score = BP * math.exp(math.fsum(scores))\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REINFORCEMENT LEARNING ---> DEEP Q-LEARNING ###\n",
    "\n",
    "bleu = BleuScore(dictionary=dataset.dictionary)\n",
    "\n",
    "target_network = deepcopy(model).eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    shuffle(batch_shuffle)\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in range(0, len(batch_shuffle)):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        observation = input_texts[batch]\n",
    "\n",
    "        next_observation, Q_s_a = model.generate_sentences(observation)\n",
    "\n",
    "        reward = bleu.bleu_score(observation, next_observation)\n",
    "\n",
    "        surprise, Q_s_t = target_network.generate_sentences(next_observation)\n",
    "\n",
    "        y = reward + 0.995 * Q_s_t\n",
    "\n",
    "        loss = criterion(possibilities, Q_s_t)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        grads = torch.mean(model.encoder[0].attention_heads[0].create_queries.weight.grad)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 100 == 0:\n",
    "\n",
    "            print(f\"Current Iteration: {iters}\\tSampler chance: {sampler_chance}\")\n",
    "            print(f\"Last Loss: {total_cost.item()}\\tBatch Loss: {total_loss/iters}\")\n",
    "            print(f\"Gradients Average: {grads}\")\n",
    "            print(f\"Generating Text...\")\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            target_network.load_state_dict(model.state_dict())\n",
    "\n",
    "            generated_text, possibilities = model.generate_sentences(input_text)\n",
    "\n",
    "            target_vector = torch.zeros_like(possibilities).scatter(dim=-1, index=target.unsqueeze(-1).long(), value=1.0).to(device)\n",
    "\n",
    "            score = bleu.bleu_score(observation, next_observation)\n",
    "\n",
    "            cost = loss(possibilities[:, :TARGET_SIZE], target_vector[:, :TARGET_SIZE])\n",
    "            cost.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            generated_text, _ = model.talk2me(generated_text)\n",
    "\n",
    "            print(f\"Evaluation Loss: {score}\")\n",
    "            print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 3, 8]\n",
      "['but you take it all in', 'theyre judged equallyright', 'what do i do']\n",
      "['ああ 時に 時に 躓いたって', '時々雨 総計に 頼り切りだ どうしよう', '捨ててきた夢をあつめて']\n",
      "[['あ', 'あ', 'r', '時', 'o', ' ', '時', 'c', 'o', '躓', 'い', 'た', 'っ', 'て', 'd', 'め', 'b', 'r', 'i', 'n', 'g', 'i', 'n', 'g', 'c', 'o', 'l', 'd', 'f', 'a', 'r', 's', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', 'c', 'o', 'l', 'd', 'c', 'o', 'l', 'd'], ['ま', 'b', '雨', 'i', 'n', 'g', 'i', 'n', 'g', 'り', '切', 'n', 't', ' ', 'ど', 'う', 'し', 'よ', 'う', 'e', 'c', 't', 'e', 'd', '怖', 'c', 'o', 'l', 'd', 'b', 'r', 'i', 'n', 'g', 'i', 'n', 'g', 'h', 'e', 'r', 'e', '裁', 'b', 'r', 'i', 'n', 'g', 'i', 'n', 'g', 'f', 'a', 'r'], ['r', 'e', 'て', 'き', 'た', '夢', 'e', 'あ', 'つ', 'め', 'd', 'd', 'e', 'p', 'r', 'e', 's', 's', 'i', 'o', 'n', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 's', 'o', 'm', 'e', 't', 'i', 'm', 'e', 's', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 'r', 'e', 'c', 'o', 'l', 'l', 'e', 'c', 't', 'e', 'd', 'd', 'e', 'p', 'r', 'e', 's', 's', 'i', 'o', 'n', 'y', 'o', 'u']]\n",
      "[48, 53, 109]\n",
      "torch.Size([3, 96, 128]) torch.Size([3, 96, 128])\n",
      "[12, 6, 8]\n",
      "[14, 19, 11]\n"
     ]
    }
   ],
   "source": [
    "print(current_batch)\n",
    "print(input_text)\n",
    "print(target_text)\n",
    "print(new_target)\n",
    "teste = [len(x) for x in new_target]\n",
    "print(teste)\n",
    "print(encoder_vectors.size(), decoder_vectors.size())\n",
    "print(real_input_length)\n",
    "print(target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Generative Adversarial Networks\n",
    "\n",
    "## We can obtain an AI capable of detecting AI-generated texts, while also making an AI capable of generating realistic texts\n",
    "\n",
    "\n",
    "**TEXT ----> WERNICKE AREA: PROCESS INFORMATION (What does it mean?)**\n",
    "\n",
    "**WERNICKE ------> ASSOCIATIVE CORTEX(TEMPORAL + PARIETAL?) -----> BROCA AREA: GENERATES NEW(?) INFORMATION ---> MOTOR CORTEX (PRECENTRAL GYRUS)**\n",
    "\n",
    "\n",
    "*Question: Can she detect metaphores, implicit messages, sense of humour, poetry? Such things aren't as simple as \"Word ---> Meaning\", afterall.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
