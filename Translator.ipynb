{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。始めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.english_maximum_length = self._get_maximum_length(self.english_phrases)\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "\n",
    "        self.english_dictionary = self._create_dictionary(self.english_words)\n",
    "        self._normalize(self.english_dictionary)\n",
    "\n",
    "        self.japanese_dictionary = self._create_dictionary(self.japanese_characters)\n",
    "        self._normalize(self.japanese_dictionary)\n",
    "\n",
    "        self.english_tokens = self._tokenize_english()\n",
    "        self.japanese_tokens = self._tokenize_japanese()\n",
    "\n",
    "        self.data_english = None\n",
    "        self.data_japanese = None\n",
    "        \n",
    "        \n",
    "    def create_data(self):\n",
    "        data_english = torch.from_numpy(self.english_tokens)\n",
    "        data_japanese = torch.from_numpy(self.japanese_tokens)\n",
    "\n",
    "        data_english = data_english.unsqueeze(-1)\n",
    "        data_japanese = data_japanese.unsqueeze(-1)\n",
    "\n",
    "        #self.data_english = torch.unsequeeze(self.data_english, -1)\n",
    "        #self.data_japanese = torch.unsequeeze(self.data_japanese, -1)\n",
    "\n",
    "        self.data_english = data_english\n",
    "        self.data_japanese = data_japanese\n",
    "\n",
    "        print(f\"English Data Size: {self.data_english.size()}\\t Japanese Data Size: {self.data_japanese.size()}\")\n",
    "\n",
    "    def detokenize(self, data, reference_dict):\n",
    "        data = data.cpu().numpy()\n",
    "        values = list(reference_dict.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        _, index = knn.kneighbors(data)\n",
    "\n",
    "        keys = list(reference_dict.keys())\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                words.append(keys[i])\n",
    "        \n",
    "        phrase = ' '.join(words)\n",
    "\n",
    "        return phrase, words\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_english)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        english_sentence = self.data_english[idx]\n",
    "        japanese_sentence = self.data_japanese[idx]\n",
    "\n",
    "        return english_sentence, japanese_sentence\n",
    "\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "        \n",
    "            sentence_length = len(word_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in japanese_phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_dictionary(self, words):\n",
    "        idx2word = []\n",
    "        word2idx = {}\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                idx2word.append(word)\n",
    "                word2idx[word] = len(idx2word) - 1\n",
    "\n",
    "        return word2idx\n",
    "\n",
    "    def _normalize(self, dictionary):\n",
    "        maximum = max(dictionary.values())\n",
    "\n",
    "        for word, value in dictionary.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            dictionary[word] = scaled_value\n",
    "    \n",
    "    def _tokenize_english(self):\n",
    "        \n",
    "        phrases = [x.split() for x in self.english_phrases]\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            tokenized_sentence = []\n",
    "            for word in sentence:\n",
    "                value = self.english_dictionary.get(word)\n",
    "\n",
    "                tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.english_maximum_length:\n",
    "                pad_size = self.english_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize_japanese(self):\n",
    "\n",
    "        phrases = [x.split() for x in self.japanese_phrases]\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for sublist in phrases:\n",
    "            for sentence in sublist:\n",
    "                tokenized_sentence = []\n",
    "                for character in sentence:\n",
    "                    value = self.japanese_dictionary.get(character)\n",
    "\n",
    "                    tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.japanese_maximum_length:\n",
    "                pad_size = self.japanese_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 74)\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = WordDataset(english_phrases, japanese_phrases)\n",
    "print(dataset_creator.japanese_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data Size: torch.Size([6, 28, 1])\t Japanese Data Size: torch.Size([6, 74, 1])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataset_creator.create_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000],\n",
      "        [-0.9697],\n",
      "        [-0.9394],\n",
      "        [-0.9091],\n",
      "        [-0.8788],\n",
      "        [-0.8485],\n",
      "        [-0.8182],\n",
      "        [-0.7879],\n",
      "        [-0.7576],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_creator.data_english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my dog doesnt like bones it prefers ground beef bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch', ['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch'])\n"
     ]
    }
   ],
   "source": [
    "teste = dataset_creator.detokenize(dataset_creator.data_english[0], dataset_creator.english_dictionary)\n",
    "\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our data ready. (N_samples, n_features) and normalized\n",
    "# Remember: LSTM = (N_samples, Sequence_Length, N_features). Each sentence = 1 sequence of tokens ---> (N_samples, N_features, 1)\n",
    "# Input = (N_samples, N_features, 1)\n",
    "# However, output will be (N_samples, N_features), so it needs a Repeating Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.lstm1 = torch.nn.LSTM(1, 28, 10, batch_first=True, bias=False)\n",
    "        self.repeatvector = 74\n",
    "        #Add repeating vector\n",
    "        self.lstm2 = torch.nn.LSTM(28*28, 10, 10, batch_first=True, bias=False)\n",
    "        self.neuron = torch.nn.Linear(10, 1, bias=False)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x, hidden = self.lstm1(input)\n",
    "\n",
    "        #print(x.size()) # (batch, 28, 28)\n",
    "        #print(hidden[0].size()) # (10,28)\n",
    "\n",
    "        x = x.repeat(1, self.repeatvector, 1)\n",
    "        x = x.view(x.size(0), self.repeatvector, -1)\n",
    "\n",
    "        #print(x.size()) # (batch, 74, 28*28)\n",
    "\n",
    "        x, hidden = self.lstm2(x)\n",
    "\n",
    "        #print(x.size()) # (1, 74, 10)\n",
    "        #print(hidden[0].size()) # (10, 1, 10)\n",
    "\n",
    "        x = self.neuron(x)\n",
    "\n",
    "        #output = self.tanh(x)\n",
    "        output = x\n",
    "\n",
    "        return output # (1, 74, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator().double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\t Current Loss: 0.1799799536706417\n",
      "100/100\t Current Loss: 0.18088331886787473\n",
      "200/100\t Current Loss: 0.17974051069632735\n",
      "300/100\t Current Loss: 0.16689358242531543\n",
      "400/100\t Current Loss: 0.17899582512616644\n",
      "500/100\t Current Loss: 0.1663501226817732\n",
      "600/100\t Current Loss: 0.15841088332826622\n",
      "700/100\t Current Loss: 0.14670436829030095\n",
      "800/100\t Current Loss: 0.1438153839202247\n",
      "900/100\t Current Loss: 0.14183392457175673\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_creator, batch_size=6, shuffle=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for i, (english, japanese) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        input_data = english.cuda()\n",
    "        labels = japanese.cuda()\n",
    "\n",
    "        output = model(input_data)\n",
    "\n",
    "        #print(output.size())\n",
    "\n",
    "        cost = loss(output, labels)\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch}/100\\t Current Loss: {cost.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5920e-07,  5.0225e-11, -1.8695e-04,  ..., -1.5026e-04,\n",
      "         -1.3216e-07, -1.2024e-12],\n",
      "        [-5.8854e-11,  4.4368e-11,  1.5414e-05,  ..., -1.0821e-05,\n",
      "         -2.8990e-09, -2.1488e-14],\n",
      "        [ 1.7191e-10,  6.1581e-08,  2.5159e-04,  ..., -1.7052e-03,\n",
      "         -5.1897e-07, -2.2401e-10],\n",
      "        ...,\n",
      "        [ 9.3724e-07, -2.1324e-11,  4.2228e-04,  ...,  8.9386e-04,\n",
      "         -1.6415e-04,  3.0094e-14],\n",
      "        [ 7.6479e-10, -2.7818e-09,  2.7644e-07,  ...,  6.3740e-08,\n",
      "          1.6437e-08, -3.7159e-13],\n",
      "        [ 5.6168e-10,  1.9884e-08, -7.5409e-07,  ...,  4.8892e-04,\n",
      "          1.6741e-05,  2.5479e-12]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.weight_ih_l9.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Parameter containing:\n",
      "tensor([[-3.1537],\n",
      "        [ 1.0168],\n",
      "        [ 2.4149],\n",
      "        [-2.3363],\n",
      "        [-2.7019],\n",
      "        [ 4.7196],\n",
      "        [-3.4235],\n",
      "        [ 4.6948],\n",
      "        [ 2.6935],\n",
      "        [ 4.0982],\n",
      "        [-5.0197],\n",
      "        [ 2.4118],\n",
      "        [-2.5449],\n",
      "        [ 2.6152],\n",
      "        [ 2.1790],\n",
      "        [-5.2558],\n",
      "        [-1.0405],\n",
      "        [ 0.2663],\n",
      "        [-3.6508],\n",
      "        [ 2.4692],\n",
      "        [-2.2581],\n",
      "        [-3.5061],\n",
      "        [ 2.4309],\n",
      "        [ 3.4463],\n",
      "        [-7.7491],\n",
      "        [-2.2516],\n",
      "        [-2.5307],\n",
      "        [-0.1579],\n",
      "        [-5.7188],\n",
      "        [-0.6784],\n",
      "        [ 0.3450],\n",
      "        [-5.3955],\n",
      "        [-2.8618],\n",
      "        [ 4.1940],\n",
      "        [-2.4424],\n",
      "        [-0.7449],\n",
      "        [ 1.1524],\n",
      "        [-0.3171],\n",
      "        [-4.1593],\n",
      "        [ 3.4510],\n",
      "        [-4.0769],\n",
      "        [-1.4131],\n",
      "        [ 1.9032],\n",
      "        [-3.6074],\n",
      "        [ 0.7404],\n",
      "        [ 3.1978],\n",
      "        [-2.1903],\n",
      "        [-5.5314],\n",
      "        [-0.3694],\n",
      "        [-0.2462],\n",
      "        [-1.9679],\n",
      "        [-3.8915],\n",
      "        [-0.1126],\n",
      "        [-4.3696],\n",
      "        [ 4.9272],\n",
      "        [-6.4125],\n",
      "        [ 2.9868],\n",
      "        [-0.2353],\n",
      "        [ 2.1299],\n",
      "        [-2.0152],\n",
      "        [-2.4173],\n",
      "        [-2.8752],\n",
      "        [ 2.2615],\n",
      "        [ 2.3920],\n",
      "        [ 3.3012],\n",
      "        [-0.2091],\n",
      "        [ 2.6628],\n",
      "        [-2.6485],\n",
      "        [ 1.7084],\n",
      "        [-1.3249],\n",
      "        [-2.5361],\n",
      "        [-7.0184],\n",
      "        [-4.8209],\n",
      "        [-2.0433],\n",
      "        [-4.2149],\n",
      "        [ 2.5665],\n",
      "        [ 2.4716],\n",
      "        [-3.7169],\n",
      "        [-1.5457],\n",
      "        [-2.4315],\n",
      "        [ 3.2603],\n",
      "        [ 3.8020],\n",
      "        [-4.1993],\n",
      "        [-2.5180],\n",
      "        [-3.0253],\n",
      "        [ 1.5643],\n",
      "        [-7.1724],\n",
      "        [-2.6341],\n",
      "        [-2.9755],\n",
      "        [ 4.9011],\n",
      "        [-1.4320],\n",
      "        [ 1.9785],\n",
      "        [ 3.8015],\n",
      "        [ 4.1391],\n",
      "        [-2.6654],\n",
      "        [ 2.5267],\n",
      "        [-5.2343],\n",
      "        [ 0.0682],\n",
      "        [-0.9449],\n",
      "        [-6.1420],\n",
      "        [-0.0773],\n",
      "        [ 3.5648],\n",
      "        [-5.9529],\n",
      "        [-3.3044],\n",
      "        [-3.7980],\n",
      "        [-2.9011],\n",
      "        [ 1.7181],\n",
      "        [-5.1503],\n",
      "        [-7.2757],\n",
      "        [-2.4299],\n",
      "        [-6.6248],\n",
      "        [-0.7062]], device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-5.8842, -3.7468,  3.5711,  ...,  3.4020,  2.1429,  3.5545],\n",
      "        [ 4.2777, -4.0695, -1.2673,  ...,  5.6920, -3.7314,  1.0259],\n",
      "        [-5.1765,  2.0601,  4.4740,  ..., -1.7554,  0.3265,  1.7212],\n",
      "        ...,\n",
      "        [-0.7761, -1.1592,  2.7524,  ..., -2.2160,  0.7951,  5.0654],\n",
      "        [-6.4115, -3.7703, -2.7429,  ...,  0.0158, -1.1473,  3.7359],\n",
      "        [-4.4820, -4.0176,  1.6598,  ...,  4.1056, -3.8050,  1.3452]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 2.6203, -1.0072,  1.3157,  ..., -0.8171, -3.9461, -3.7003],\n",
      "        [-2.0624, -3.0276, -1.5773,  ..., -3.4946, -2.7733,  2.0279],\n",
      "        [-1.5812,  4.0396, -4.4537,  ..., -4.6083,  5.7186,  2.7251],\n",
      "        ...,\n",
      "        [-0.0537, -2.7435,  4.4656,  ...,  4.7591, -6.6312,  0.6191],\n",
      "        [ 5.0483, -1.2000,  5.7758,  ..., -3.1785,  1.5468,  1.3189],\n",
      "        [-0.3635, -5.1566,  4.0636,  ...,  6.3196, -5.5113, -0.2844]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 6.5145, -8.1499, -2.3883,  ..., -3.1540, -2.6194, -0.9730],\n",
      "        [-0.3422,  4.8416,  0.3558,  ...,  2.3250, -2.5723,  2.3114],\n",
      "        [-1.7890,  7.2851, -3.2859,  ...,  4.6992,  1.0658,  2.7253],\n",
      "        ...,\n",
      "        [ 2.9282, -1.4724, -0.3063,  ..., -2.2537, -3.6965, -2.8215],\n",
      "        [-1.8596,  1.7419, -5.5659,  ...,  4.1991, -5.0785, -1.0925],\n",
      "        [ 1.4029, -1.7084, -1.2481,  ..., -1.4111,  2.0751, -4.7925]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.2461,  1.2104,  2.6703,  ..., -3.0080, -2.4465, -0.6961],\n",
      "        [-5.2541, -8.4538,  2.7712,  ..., -2.3723,  2.6908, -2.8831],\n",
      "        [ 0.8584, -0.1045, -4.0871,  ..., -0.7743,  1.5793,  0.7563],\n",
      "        ...,\n",
      "        [-1.0155, -1.9297,  7.5945,  ...,  2.7694,  1.3733,  3.5836],\n",
      "        [-3.4895,  3.6755,  0.5799,  ..., -1.8899,  3.1045,  0.0910],\n",
      "        [ 1.8317, -3.7402,  4.5625,  ...,  0.7655,  4.4816,  2.1179]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 2.2815, -2.2626,  5.1520,  ..., -2.9553,  3.7157,  5.9313],\n",
      "        [-1.1173,  0.9414,  0.1650,  ...,  0.5000,  1.7153, -0.3867],\n",
      "        [ 0.5522, -0.2198,  0.7559,  ..., -1.8733,  1.3451, -0.3482],\n",
      "        ...,\n",
      "        [-3.3074,  3.5151, -3.3557,  ..., -2.1433, -1.0878, -2.6164],\n",
      "        [-5.5635,  6.7187,  5.0021,  ...,  5.5492,  0.8576,  3.6452],\n",
      "        [ 3.1100, -1.7621,  1.9226,  ..., -4.4487,  3.2056, -1.3856]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[  4.0430,   6.5488,  -2.3130,  ...,  -6.9981,  -4.7994,   3.7930],\n",
      "        [  0.4268,   7.3531,  -1.8580,  ..., -10.7676,  -6.3243,   1.2112],\n",
      "        [  2.3327,  15.2918,  -5.7121,  ...,   9.1135,  -5.3948,  -1.4143],\n",
      "        ...,\n",
      "        [  3.8877,   6.2346,   0.4597,  ...,  -3.9870,  -1.9860,   0.6842],\n",
      "        [ -2.0160,  -5.0071,  -2.9093,  ...,   1.2327,   6.6225,   2.7566],\n",
      "        [  0.7247,   0.2868,  -7.5985,  ...,  -0.3564,  -6.3583,   1.1037]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.7130, -2.3107, -3.7447,  ..., -2.9519, -5.2426,  2.1186],\n",
      "        [ 0.6180, -2.6787, -2.7454,  ...,  3.4388, -4.5496, -4.6145],\n",
      "        [ 0.4611,  1.6742, -1.0124,  ..., -7.9329,  2.2000, -1.0155],\n",
      "        ...,\n",
      "        [-3.6883, -2.5827, -2.0859,  ...,  3.1873, -5.4533,  2.5858],\n",
      "        [ 7.6528,  3.9499,  5.3480,  ...,  6.4369,  0.7850, -1.0250],\n",
      "        [ 1.6241,  1.6075,  2.9734,  ...,  5.0037, -3.7536,  0.4522]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-2.7729e-01,  4.4914e+00,  6.3070e+00,  ..., -8.5803e-01,\n",
      "          5.1559e+00,  5.7699e+00],\n",
      "        [-3.8461e+00,  3.9568e-01, -8.8957e+00,  ...,  3.3362e+00,\n",
      "         -5.8190e+00,  7.2400e+00],\n",
      "        [-4.1305e+00, -2.9297e+00, -6.9911e+00,  ..., -2.6539e+00,\n",
      "          1.1988e+00,  4.2242e-01],\n",
      "        ...,\n",
      "        [-3.6932e+00,  1.4153e+00,  2.0372e+00,  ...,  2.6858e-03,\n",
      "          1.8530e+00,  5.3776e+00],\n",
      "        [ 6.3342e+00,  3.2147e+00,  4.3889e+00,  ..., -2.3499e+00,\n",
      "         -2.6191e+00, -5.0439e+00],\n",
      "        [ 2.7266e+00, -2.0933e+00,  4.0216e+00,  ...,  8.0376e+00,\n",
      "         -8.4089e-01,  3.8997e+00]], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ -4.4314,   3.9314,   1.2465,  ...,  -1.2313,  -4.2317,   5.6255],\n",
      "        [ -1.9794,   2.5206,   1.5784,  ...,  -0.7015,  -0.8773,  -2.2305],\n",
      "        [ -7.8834, -10.4872,  -3.8925,  ...,   7.8371,   2.7496,   6.4128],\n",
      "        ...,\n",
      "        [  1.4541,  -2.3686,   3.9391,  ...,  -0.3157,   4.3635,  -0.9394],\n",
      "        [ -1.7599,   2.0726,  -0.4192,  ...,  -0.7653,  -3.0745,  -5.2155],\n",
      "        [ -3.0862,  -8.1473,  -5.9994,  ...,   4.6069,   5.1940,  -3.5343]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.3034,  2.2298, -9.3284,  ...,  3.9795,  2.0190,  2.8322],\n",
      "        [ 3.7965,  5.4595, -5.1935,  ..., -5.4856,  2.1621, -2.6557],\n",
      "        [-1.1416, -5.0688,  1.6711,  ..., -2.0366,  2.0522,  9.1020],\n",
      "        ...,\n",
      "        [-7.7030, -5.5411, -9.4746,  ..., -0.7866, -0.6678, -0.5034],\n",
      "        [-5.3637,  1.2446, -3.2384,  ..., -1.0326,  0.5463, -3.7567],\n",
      "        [ 0.5043,  2.7847, -0.3642,  ..., -2.6919, -0.2876,  1.0608]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 7.5401,  7.0808,  1.2030,  ..., -6.0489, -3.4029,  9.9934],\n",
      "        [-6.5999,  0.4792, -8.4070,  ..., -1.2749,  0.1519, -7.0731],\n",
      "        [-1.8950,  2.1550, -6.5975,  ...,  1.3551, -7.9145, -1.7282],\n",
      "        ...,\n",
      "        [ 1.1065, -5.6001, -5.0992,  ..., -4.5278, -1.9967, -4.8098],\n",
      "        [ 1.3325,  3.4938, -5.0499,  ..., -0.0754,  4.3208, -5.1598],\n",
      "        [-2.2885,  1.3214,  2.7789,  ..., -3.0578, -0.0381, -0.8784]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 8.3086, -3.4099,  3.9226,  ..., -4.1956, -2.1987,  9.5970],\n",
      "        [-3.0827, -6.0173, -3.2758,  ..., -7.4683, -3.6275,  1.5048],\n",
      "        [-1.1229, -3.2271,  0.8501,  ..., -8.4439, -6.8393, -7.0766],\n",
      "        ...,\n",
      "        [ 3.5245, -1.1072, -7.0972,  ...,  3.3979, -5.8414, -4.4271],\n",
      "        [15.1780, -1.5289, -0.4370,  ..., -7.4480, -0.1509,  1.9053],\n",
      "        [ 3.3788,  0.5751,  2.6235,  ...,  7.4683,  1.1261,  5.3670]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ -2.4984,  -1.2164,   1.9837,  ...,  -2.0578,  -2.0346,   3.9403],\n",
      "        [ -0.4966,  -2.7804,  12.2364,  ...,   4.0994,  -4.6167,   2.7234],\n",
      "        [  1.6116,   3.4264,  12.4496,  ..., -17.0154,  -5.3774,  -6.2224],\n",
      "        ...,\n",
      "        [  1.2430,   1.2314,  -0.5809,  ...,  -8.8194,  -8.7069,   4.5053],\n",
      "        [  1.6214,  -1.9009,  -0.8825,  ...,  -0.7352, -10.4338,  -0.1260],\n",
      "        [ -3.2430,  -4.2530,  -3.1473,  ...,   2.0638,   8.8212,   7.7301]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ -4.8821,   6.4704,  -2.6178,  ...,  -5.3429,  -3.8778,   1.4523],\n",
      "        [ -8.1424,  -7.0693,   4.7484,  ...,   0.1816,   4.3455,   2.0780],\n",
      "        [ -7.2583,   3.9988,   1.7974,  ...,  -5.6450,  -8.4459,  -5.7168],\n",
      "        ...,\n",
      "        [  5.5085,   9.3036,   1.4104,  ...,  -5.5904,  -2.5027, -11.7600],\n",
      "        [ -6.8733,   5.4091,   5.5372,  ...,   2.2278,  -3.7283,  -4.3206],\n",
      "        [  1.1740,  -2.2984,   5.4334,  ...,  10.4437,   4.3288,   0.7368]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.3812,  1.9238, -2.0301,  ..., -3.5577, -0.9156,  4.6599],\n",
      "        [ 6.8356,  8.1217,  0.9305,  ..., -0.6498,  1.8184,  6.0843],\n",
      "        [-7.3351, -4.3773,  6.0432,  ...,  7.5396, -2.4902,  1.4172],\n",
      "        ...,\n",
      "        [ 0.1664,  1.6240, -5.4281,  ..., -0.5765,  3.1275, -6.8313],\n",
      "        [ 5.8545,  6.4826,  4.6600,  ..., -0.6979, -6.2526,  0.1110],\n",
      "        [-5.2110, -3.2174,  0.7180,  ..., -7.4168,  3.4228, -3.9899]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-4.0351, -7.2158, -3.8615,  ..., -8.3922, -3.5224, -4.9980],\n",
      "        [-8.1543, -8.3194,  5.1258,  ...,  0.1953, 13.9887, -2.9257],\n",
      "        [ 2.5842,  0.9041, -6.2448,  ..., -4.4378, -3.4711, -2.4850],\n",
      "        ...,\n",
      "        [-5.1822,  4.8158,  4.1001,  ...,  6.7845,  3.6262, -7.1262],\n",
      "        [ 1.2622,  1.7417, -2.5147,  ..., -8.5917, -7.1231,  1.0910],\n",
      "        [ 4.7563,  4.9212, -2.6474,  ..., -6.3245, -5.2401,  7.2449]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.9099,  9.8963,  1.3115,  ...,  3.7007, -0.6323, -7.7898],\n",
      "        [ 2.1682,  4.5247,  2.0261,  ...,  3.7953,  5.3130, -0.0375],\n",
      "        [ 3.8071, -7.3427, -0.5397,  ..., -2.3551, -3.2542, -5.2960],\n",
      "        ...,\n",
      "        [ 2.9774,  1.7428, -4.6371,  ..., -7.9757, -2.3608, -5.8785],\n",
      "        [-6.5312,  1.4260,  1.3205,  ...,  1.0748, -6.3642,  2.7626],\n",
      "        [ 3.5597, -7.5011, -5.0162,  ...,  0.8049,  0.0682,  3.3506]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[  4.1502,   0.4758,  -2.2625,  ...,  -3.9867,  -3.0760,   2.7351],\n",
      "        [ -2.0055,  -8.4214,   0.0783,  ...,   2.9373,  -4.3154,   2.6613],\n",
      "        [ -1.0629,  -3.0499,  -4.7225,  ...,   1.4795,  -1.7609,   6.2047],\n",
      "        ...,\n",
      "        [  2.1947, -11.1233,  -3.5562,  ...,  -2.9867,  -0.0359,   4.2643],\n",
      "        [  1.7548,   1.2798,   2.0544,  ...,   0.0727,   0.3586,  -0.1012],\n",
      "        [ -1.7973,  -2.4332,  -0.4796,  ...,   6.3802,  -3.7676,   4.5508]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-2.6790, -1.9335,  0.1376,  ...,  0.5896,  0.2512,  0.7812],\n",
      "        [ 5.5068, -0.3954, -1.2404,  ..., -0.5505,  1.8197, -0.8804],\n",
      "        [-0.5099,  2.8567,  3.1194,  ..., -5.0949, -7.0437, -3.3206],\n",
      "        ...,\n",
      "        [-5.9105,  1.5535, -0.0453,  ...,  0.4306, -4.3219,  3.3560],\n",
      "        [-0.5949, -1.1699, -6.4206,  ..., -6.4923,  2.8675, -4.9900],\n",
      "        [-4.3540, -2.3813, -2.4754,  ..., -3.2794, -4.1914,  0.7888]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000],\n",
      "         [-0.7273],\n",
      "         [-0.6970],\n",
      "         [-0.6667],\n",
      "         [-0.6364],\n",
      "         [-0.6061],\n",
      "         [-0.5758],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [ 0.8788],\n",
      "         [ 0.9091],\n",
      "         [ 0.9394],\n",
      "         [ 0.9697],\n",
      "         [ 1.0000],\n",
      "         [ 0.9697],\n",
      "         [-0.4242],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.5758],\n",
      "         [-0.1818],\n",
      "         [-0.1515],\n",
      "         [-0.1212],\n",
      "         [-0.0909],\n",
      "         [-0.0606],\n",
      "         [-0.0303],\n",
      "         [-0.2727],\n",
      "         [ 0.0000],\n",
      "         [ 0.0303],\n",
      "         [ 0.0606],\n",
      "         [ 0.0909],\n",
      "         [ 0.1212],\n",
      "         [-0.5758],\n",
      "         [ 0.1515],\n",
      "         [ 0.1818],\n",
      "         [ 0.2121],\n",
      "         [ 0.2424],\n",
      "         [ 0.2727],\n",
      "         [ 0.3030],\n",
      "         [ 0.3333],\n",
      "         [ 0.3636],\n",
      "         [-0.1515],\n",
      "         [ 0.3939],\n",
      "         [-0.5758],\n",
      "         [ 0.4242],\n",
      "         [ 0.4545],\n",
      "         [ 0.4848]],\n",
      "\n",
      "        [[-0.5455],\n",
      "         [-0.5152],\n",
      "         [-0.4848],\n",
      "         [-0.5455],\n",
      "         [-0.4545],\n",
      "         [-0.6364],\n",
      "         [-0.4242],\n",
      "         [-0.3939],\n",
      "         [-0.3636],\n",
      "         [-0.5455],\n",
      "         [-0.3333],\n",
      "         [-0.3030],\n",
      "         [-0.2727],\n",
      "         [-0.2424],\n",
      "         [-0.2121],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[ 0.5152],\n",
      "         [ 0.5455],\n",
      "         [ 0.5758],\n",
      "         [ 0.6061],\n",
      "         [ 0.6364],\n",
      "         [ 0.6667],\n",
      "         [ 0.6970],\n",
      "         [ 0.7273],\n",
      "         [ 0.5758],\n",
      "         [ 0.7576],\n",
      "         [ 0.6667],\n",
      "         [ 0.7879],\n",
      "         [-0.0606],\n",
      "         [ 0.2727],\n",
      "         [ 0.8182],\n",
      "         [ 0.7879],\n",
      "         [-0.9091],\n",
      "         [ 0.8485],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [-0.9697],\n",
      "         [-0.9394],\n",
      "         [-0.9091],\n",
      "         [-0.8788],\n",
      "         [-0.8485],\n",
      "         [-0.8182],\n",
      "         [-0.7879],\n",
      "         [-0.7576],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset_creator.detokenize(labels[0], dataset_creator.japanese_dictionary)\n",
    "\n",
    "output = output.detach()\n",
    "predicted = dataset_creator.detokenize(output[0], dataset_creator.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('私 の 名 前 は ア リ ス で す 始 め ま し て に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に', ['私', 'の', '名', '前', 'は', 'ア', 'リ', 'ス', 'で', 'す', '始', 'め', 'ま', 'し', 'て', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に'])\n",
      "('ト 始 羨 め む い 前 た っ っ 始 ゲ ア い す な サ い 始 始 て な ゲ た 良 サ ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト ト', ['ト', '始', '羨', 'め', 'む', 'い', '前', 'た', 'っ', 'っ', '始', 'ゲ', 'ア', 'い', 'す', 'な', 'サ', 'い', '始', '始', 'て', 'な', 'ゲ', 'た', '良', 'サ', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト', 'ト'])\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
