{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。始めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.english_maximum_length = self._get_maximum_length(self.english_phrases)\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "\n",
    "        self.english_dictionary = self._create_dictionary(self.english_words)\n",
    "        self._normalize(self.english_dictionary)\n",
    "\n",
    "        self.japanese_dictionary = self._create_dictionary(self.japanese_characters)\n",
    "        self._normalize(self.japanese_dictionary)\n",
    "\n",
    "        self.english_tokens = self._tokenize_english()\n",
    "        self.japanese_tokens = self._tokenize_japanese()\n",
    "\n",
    "        self.data_english = None\n",
    "        self.data_japanese = None\n",
    "        \n",
    "        \n",
    "    def create_data(self):\n",
    "        data_english = torch.from_numpy(self.english_tokens)\n",
    "        data_japanese = torch.from_numpy(self.japanese_tokens)\n",
    "\n",
    "        data_english = data_english.unsqueeze(-1)\n",
    "        data_japanese = data_japanese.unsqueeze(-1)\n",
    "\n",
    "        #self.data_english = torch.unsequeeze(self.data_english, -1)\n",
    "        #self.data_japanese = torch.unsequeeze(self.data_japanese, -1)\n",
    "\n",
    "        self.data_english = data_english\n",
    "        self.data_japanese = data_japanese\n",
    "\n",
    "        print(f\"English Data Size: {self.data_english.size()}\\t Japanese Data Size: {self.data_japanese.size()}\")\n",
    "\n",
    "    def detokenize(self, data, reference_dict):\n",
    "        data = data.cpu().numpy()\n",
    "        values = list(reference_dict.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        _, index = knn.kneighbors(data)\n",
    "\n",
    "        keys = list(reference_dict.keys())\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                words.append(keys[i])\n",
    "        \n",
    "        phrase = ' '.join(words)\n",
    "\n",
    "        return phrase, words\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_english)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        english_sentence = self.data_english[idx]\n",
    "        japanese_sentence = self.data_japanese[idx]\n",
    "\n",
    "        return english_sentence, japanese_sentence\n",
    "\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "        \n",
    "            sentence_length = len(word_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in japanese_phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_dictionary(self, words):\n",
    "        idx2word = []\n",
    "        word2idx = {}\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                idx2word.append(word)\n",
    "                word2idx[word] = len(idx2word) - 1\n",
    "\n",
    "        word2idx['<EOS>'] = len(idx2word) # Adding an End of Sentence tag to improve model's accuracy\n",
    "\n",
    "        return word2idx\n",
    "\n",
    "    def _normalize(self, dictionary):\n",
    "        maximum = max(dictionary.values())\n",
    "\n",
    "        for word, value in dictionary.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            dictionary[word] = scaled_value\n",
    "    \n",
    "    def _tokenize_english(self):\n",
    "        \n",
    "        phrases = [x.split() for x in self.english_phrases]\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            tokenized_sentence = []\n",
    "            for word in sentence:\n",
    "                value = self.english_dictionary.get(word)\n",
    "\n",
    "                tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.english_maximum_length:\n",
    "                pad_size = self.english_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)], constant_values=1.0) # Remember: <'EOS'> token is 1.0\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize_japanese(self):\n",
    "\n",
    "        phrases = [x.split() for x in self.japanese_phrases]\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for sublist in phrases:\n",
    "            for sentence in sublist:\n",
    "                tokenized_sentence = []\n",
    "                for character in sentence:\n",
    "                    value = self.japanese_dictionary.get(character)\n",
    "\n",
    "                    tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.japanese_maximum_length:\n",
    "                pad_size = self.japanese_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 74)\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = WordDataset(english_phrases, japanese_phrases)\n",
    "print(dataset_creator.japanese_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data Size: torch.Size([6, 28, 1])\t Japanese Data Size: torch.Size([6, 74, 1])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataset_creator.create_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_creator.data_english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my dog doesnt like bones it prefers ground beef <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>', ['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>'])\n"
     ]
    }
   ],
   "source": [
    "teste = dataset_creator.detokenize(dataset_creator.data_english[0], dataset_creator.english_dictionary)\n",
    "\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our data ready. (N_samples, n_features) and normalized\n",
    "# Remember: LSTM = (N_samples, Sequence_Length, N_features). Each sentence = 1 sequence of tokens ---> (N_samples, N_features, 1)\n",
    "# Input = (N_samples, N_features, 1)\n",
    "# However, output will be (N_samples, N_features), so it needs a Repeating Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.lstm1 = torch.nn.LSTM(1, 28, 10, batch_first=True, bias=False)\n",
    "        self.repeatvector = 74\n",
    "        #Add repeating vector\n",
    "        self.lstm2 = torch.nn.LSTM(28*28, 10, 10, batch_first=True, bias=False)\n",
    "        self.neuron = torch.nn.Linear(10, 1, bias=False)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x, hidden = self.lstm1(input)\n",
    "\n",
    "        #print(x.size()) # (batch, 28, 28)\n",
    "        #print(hidden[0].size()) # (10,28)\n",
    "\n",
    "        x = x.repeat(1, self.repeatvector, 1)\n",
    "        x = x.view(x.size(0), self.repeatvector, -1)\n",
    "\n",
    "        #print(x.size()) # (batch, 74, 28*28)\n",
    "\n",
    "        x, hidden = self.lstm2(x)\n",
    "\n",
    "        #print(x.size()) # (1, 74, 10)\n",
    "        #print(hidden[0].size()) # (10, 1, 10)\n",
    "\n",
    "        x = self.neuron(x)\n",
    "\n",
    "        #output = self.tanh(x)\n",
    "        output = x\n",
    "\n",
    "        return output # (1, 74, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator().double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\t Current Loss: 0.178194922609209\n",
      "100/100\t Current Loss: 0.17819492260859224\n",
      "200/100\t Current Loss: 0.17819492260804384\n",
      "300/100\t Current Loss: 0.17819492260714997\n",
      "400/100\t Current Loss: 0.1781949226045399\n",
      "500/100\t Current Loss: 0.17819492257832478\n",
      "600/100\t Current Loss: 0.17151640692661163\n",
      "700/100\t Current Loss: 0.1386561436979506\n",
      "800/100\t Current Loss: 0.13861357669354163\n",
      "900/100\t Current Loss: 0.13860382300360374\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_creator, batch_size=6, shuffle=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for i, (english, japanese) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        input_data = english.cuda()\n",
    "        labels = japanese.cuda()\n",
    "\n",
    "        output = model(input_data)\n",
    "\n",
    "        #print(output.size())\n",
    "\n",
    "        cost = loss(output, labels)\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch}/1000\\t Current Loss: {cost.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.weight_ih_l9.grad) # Checking if we didn't get vanishing gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a possibility..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Parameter containing:\n",
      "tensor([[ 0.5118],\n",
      "        [ 3.1641],\n",
      "        [ 1.0648],\n",
      "        [ 2.6764],\n",
      "        [-3.0317],\n",
      "        [ 1.2592],\n",
      "        [ 3.4163],\n",
      "        [ 1.4599],\n",
      "        [ 2.9996],\n",
      "        [ 2.9929],\n",
      "        [ 0.9836],\n",
      "        [-2.4657],\n",
      "        [ 3.4233],\n",
      "        [ 2.4852],\n",
      "        [ 0.4302],\n",
      "        [ 2.7075],\n",
      "        [-2.5656],\n",
      "        [ 1.1821],\n",
      "        [-0.7142],\n",
      "        [ 2.5953],\n",
      "        [-2.5981],\n",
      "        [ 3.1464],\n",
      "        [ 3.6141],\n",
      "        [-0.0594],\n",
      "        [ 3.2762],\n",
      "        [ 3.0874],\n",
      "        [-0.1664],\n",
      "        [ 3.2585],\n",
      "        [-2.7380],\n",
      "        [-0.4738],\n",
      "        [-2.0779],\n",
      "        [-2.0547],\n",
      "        [-2.1306],\n",
      "        [-0.0068],\n",
      "        [-1.6980],\n",
      "        [-1.5971],\n",
      "        [-0.9406],\n",
      "        [-0.6860],\n",
      "        [ 0.6219],\n",
      "        [-1.4594],\n",
      "        [-0.9583],\n",
      "        [-2.2461],\n",
      "        [-1.9820],\n",
      "        [-1.7840],\n",
      "        [-1.5291],\n",
      "        [-1.9633],\n",
      "        [-1.5685],\n",
      "        [-0.2747],\n",
      "        [-1.6127],\n",
      "        [-2.5084],\n",
      "        [-0.1898],\n",
      "        [-2.2897],\n",
      "        [-2.4546],\n",
      "        [-1.3422],\n",
      "        [-2.1036],\n",
      "        [-1.9509],\n",
      "        [-2.1599],\n",
      "        [ 0.5495],\n",
      "        [ 1.7212],\n",
      "        [-1.8463],\n",
      "        [ 3.5263],\n",
      "        [ 1.4838],\n",
      "        [ 0.9725],\n",
      "        [ 2.2852],\n",
      "        [-1.6201],\n",
      "        [ 0.3460],\n",
      "        [-0.2949],\n",
      "        [ 2.2080],\n",
      "        [ 0.6088],\n",
      "        [ 3.0511],\n",
      "        [ 2.7026],\n",
      "        [ 1.2690],\n",
      "        [ 3.0743],\n",
      "        [ 2.9477],\n",
      "        [ 3.0184],\n",
      "        [-1.2457],\n",
      "        [ 2.9978],\n",
      "        [ 1.5066],\n",
      "        [ 0.2578],\n",
      "        [ 2.5439],\n",
      "        [-1.5775],\n",
      "        [-0.5859],\n",
      "        [ 0.2098],\n",
      "        [ 1.3466],\n",
      "        [-0.9810],\n",
      "        [ 3.1077],\n",
      "        [ 3.0074],\n",
      "        [ 2.9468],\n",
      "        [-1.9640],\n",
      "        [ 1.1061],\n",
      "        [ 3.4581],\n",
      "        [ 0.3467],\n",
      "        [ 3.2279],\n",
      "        [ 3.1029],\n",
      "        [ 1.0586],\n",
      "        [-2.5696],\n",
      "        [ 3.3351],\n",
      "        [ 1.1810],\n",
      "        [-1.0045],\n",
      "        [ 3.5302],\n",
      "        [-1.6049],\n",
      "        [-0.7158],\n",
      "        [-0.5491],\n",
      "        [ 3.1672],\n",
      "        [-1.5413],\n",
      "        [ 3.0835],\n",
      "        [ 3.1407],\n",
      "        [-1.0999],\n",
      "        [ 3.0468],\n",
      "        [ 3.5872],\n",
      "        [-2.4894],\n",
      "        [ 3.3214]], device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-1.0332, -1.7446, -1.0741,  ...,  1.5792, -0.9817, -1.4504],\n",
      "        [-1.4298, -0.0432,  1.8102,  ..., -0.8038, -2.2478,  0.4134],\n",
      "        [ 1.2168,  1.1927,  0.9335,  ..., -1.6156,  1.2538,  1.2746],\n",
      "        ...,\n",
      "        [-0.5103, -2.0265,  0.0306,  ..., -1.2482,  2.2844, -1.2177],\n",
      "        [ 1.4978, -0.4991, -0.1056,  ..., -0.8258,  1.6735, -0.4440],\n",
      "        [-0.7914,  0.7893,  2.0345,  ..., -1.9720,  0.9168,  1.7233]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-2.4149, -2.2143, -0.9304,  ...,  1.4307, -1.2827, -1.5136],\n",
      "        [-2.7782, -1.8466, -0.6309,  ...,  1.1467, -2.0925, -1.0248],\n",
      "        [ 1.5636, -1.0717, -1.5262,  ...,  2.9935,  0.3589, -1.4597],\n",
      "        ...,\n",
      "        [-1.1329,  2.6243,  2.4739,  ..., -2.2966,  1.6221,  2.3189],\n",
      "        [ 0.6642,  1.0183,  1.5869,  ..., -2.0721,  2.1118,  1.1973],\n",
      "        [ 1.1596,  1.1389,  1.4228,  ..., -2.2805,  2.5521,  1.4856]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.2407,  0.1312, -1.5208,  ...,  0.1916,  0.9630, -1.9658],\n",
      "        [ 0.8605,  1.8528, -2.4256,  ...,  2.4111,  2.3248, -2.6261],\n",
      "        [-1.6174,  1.4650, -1.1160,  ..., -2.3231, -2.5906,  2.7506],\n",
      "        ...,\n",
      "        [ 2.3221, -1.7419, -2.3417,  ...,  2.4675,  2.3181, -2.3073],\n",
      "        [ 2.8874, -2.8065, -0.7677,  ..., -1.1605,  1.7933, -1.9757],\n",
      "        [ 2.4781, -2.3830,  1.1408,  ..., -1.3105,  1.7889, -1.6291]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 2.1966, -1.9233, -0.2029,  ...,  1.6598,  0.6916, -0.9402],\n",
      "        [ 2.6609, -2.3087,  2.3072,  ...,  1.8785,  1.7731, -1.9076],\n",
      "        [-0.2712,  0.5075, -0.6924,  ...,  1.9816,  1.4035, -1.2565],\n",
      "        ...,\n",
      "        [ 2.4724, -2.5994, -0.4869,  ...,  1.6413,  1.5178, -1.4217],\n",
      "        [-1.9796,  2.0450, -1.9007,  ...,  1.7364,  1.9535, -1.9384],\n",
      "        [-0.6567,  1.0538, -0.9382,  ...,  2.0667,  1.2874, -1.5386]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-1.9503,  1.5593, -2.0256,  ..., -2.2543, -1.4908, -2.1940],\n",
      "        [-2.5370, -2.0843, -1.7798,  ..., -2.0074, -2.3640, -2.3493],\n",
      "        [-0.0972, -1.9704, -0.7076,  ..., -0.3618, -1.3281, -0.5721],\n",
      "        ...,\n",
      "        [ 0.0932, -2.4905,  0.8182,  ...,  1.7200, -2.7652,  2.3046],\n",
      "        [-2.5669, -2.6597, -1.8242,  ..., -1.8712, -3.1787, -2.2248],\n",
      "        [ 1.2625, -2.3703,  0.0363,  ...,  0.6927, -2.4290,  0.2925]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 2.3595,  0.1730,  0.7651,  ...,  1.8346,  2.4815,  1.9385],\n",
      "        [-1.7226,  1.3050, -1.5352,  ..., -1.9338, -0.0128, -1.4975],\n",
      "        [ 0.5517, -0.6878, -1.4362,  ...,  0.9922, -0.9996,  1.0749],\n",
      "        ...,\n",
      "        [-3.1148,  0.3129, -0.9678,  ..., -0.3225, -2.3120,  1.1137],\n",
      "        [-2.3582,  2.4103, -2.2163,  ..., -2.4728, -0.6059, -2.3588],\n",
      "        [ 0.9749,  2.3435, -1.8078,  ...,  1.4182,  0.9223,  1.6506]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.9362,  1.1086, -0.4742,  ...,  1.4049, -0.3682, -0.7959],\n",
      "        [ 0.9708,  2.4059, -2.3013,  ...,  1.6828, -1.8916, -1.7051],\n",
      "        [-1.0222, -0.2626,  1.5118,  ..., -0.1464,  1.5771,  0.5205],\n",
      "        ...,\n",
      "        [-1.4490, -1.0402,  1.7150,  ...,  0.0029,  2.1739, -0.3290],\n",
      "        [-0.5366,  0.5025, -1.3945,  ...,  0.8378, -0.7505, -1.9296],\n",
      "        [-1.0376,  1.9121, -0.9603,  ...,  2.7196, -0.5896, -2.2328]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 2.7194,  2.1821, -2.6732,  ...,  2.5099, -2.4961, -2.5018],\n",
      "        [-1.4219, -2.6679,  2.5879,  ..., -2.7750,  2.4521,  2.4826],\n",
      "        [-0.9562, -2.0282,  2.1242,  ..., -2.1670,  1.9971,  2.2576],\n",
      "        ...,\n",
      "        [ 2.6422, -0.0678, -1.0558,  ..., -0.0066, -0.5946, -0.8945],\n",
      "        [-1.3762, -2.7787,  2.5339,  ..., -2.7788,  2.5147,  2.4443],\n",
      "        [ 1.4079,  1.9864, -1.5925,  ...,  1.6906, -1.7571, -1.4962]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-2.3970, -1.8334, -0.7192,  ...,  0.9660,  1.6597, -2.2918],\n",
      "        [-1.5964, -2.0000, -1.8405,  ...,  1.9562,  1.9475, -2.7776],\n",
      "        [-1.8944, -1.6313, -1.5911,  ...,  1.8898,  1.1151, -2.0325],\n",
      "        ...,\n",
      "        [-2.4543, -2.1405, -1.5369,  ...,  2.4968, -0.8069, -1.9534],\n",
      "        [ 0.8006,  0.4001, -0.2098,  ..., -1.4799,  2.5159,  1.3756],\n",
      "        [ 1.3648,  2.7277,  2.1560,  ..., -0.1197, -0.0725,  1.3928]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-2.0285,  2.4223,  2.2707,  ...,  1.9577, -2.9063, -0.1064],\n",
      "        [-2.5922, -0.8921, -0.3240,  ...,  1.8213,  1.4645, -2.3200],\n",
      "        [ 2.6831,  2.9667,  2.7325,  ...,  1.1806, -2.7858,  2.8304],\n",
      "        ...,\n",
      "        [-1.3725, -0.6622, -0.5198,  ...,  1.2415, -0.0276, -3.3537],\n",
      "        [-1.3973, -0.5935,  0.6333,  ...,  2.5309,  1.0778, -2.1674],\n",
      "        [ 1.0704,  2.3627,  2.3936,  ..., -0.4237, -0.4463,  1.8796]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.9963e+00, -2.4418e+00,  1.6525e+00,  ..., -9.8028e-01,\n",
      "         -1.4519e+00,  2.7758e+00],\n",
      "        [-2.2966e+00, -4.1619e-01, -2.1693e+00,  ...,  2.6305e+00,\n",
      "          2.8142e+00,  3.1796e-03],\n",
      "        [ 1.7709e+00, -2.0959e+00,  2.3493e+00,  ..., -1.4508e+00,\n",
      "         -1.6849e+00,  1.3366e-01],\n",
      "        ...,\n",
      "        [-1.8092e+00, -2.3873e+00,  3.3076e+00,  ...,  2.0842e-02,\n",
      "         -7.7322e-01,  2.1139e+00],\n",
      "        [ 6.1572e-01, -9.3522e-01,  1.8477e+00,  ..., -2.1839e+00,\n",
      "         -2.2045e+00,  1.4244e+00],\n",
      "        [-5.1167e-01, -2.1602e+00,  1.3870e+00,  ...,  1.2203e+00,\n",
      "          2.4342e+00, -1.0446e+00]], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 0.3196, -0.6640,  2.6195,  ..., -2.3724, -2.7619,  2.7999],\n",
      "        [ 2.0798,  1.2762,  2.7091,  ..., -2.2272, -2.6122, -1.7583],\n",
      "        [-0.9067,  0.3391,  2.3952,  ..., -1.5817, -2.6457,  2.2194],\n",
      "        ...,\n",
      "        [ 2.1679,  1.1024, -2.9465,  ...,  1.6992,  2.8285, -2.1000],\n",
      "        [ 0.7894,  0.5433,  2.5191,  ..., -2.3610, -2.2841, -0.9813],\n",
      "        [ 1.5940, -0.1504, -1.5738,  ...,  2.1864,  2.2136, -2.2026]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 1.8080, -0.5341, -1.9806,  ..., -2.9186, -1.5003, -2.1301],\n",
      "        [-1.8663,  2.4918,  1.6328,  ..., -2.6595,  2.2292, -2.0880],\n",
      "        [ 1.7426, -0.0807, -2.6916,  ..., -3.5362, -1.3479, -2.4008],\n",
      "        ...,\n",
      "        [ 0.0584, -2.6727,  1.1202,  ...,  3.0228,  1.2042,  1.0482],\n",
      "        [ 0.9561, -0.4955, -1.7851,  ..., -2.2448,  0.0129, -1.5963],\n",
      "        [-0.5226, -1.7926,  0.5131,  ...,  0.9594,  1.9256,  0.8634]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 3.1027, -3.1026, -3.0438,  ...,  1.1433, -2.9921,  2.2557],\n",
      "        [-1.8382,  2.7726,  1.6755,  ..., -2.6492,  1.3031, -2.2025],\n",
      "        [-2.1849,  2.3045,  1.9446,  ..., -2.0190,  1.9578,  0.1211],\n",
      "        ...,\n",
      "        [-0.9032,  2.6229,  0.5092,  ..., -0.4994,  0.7556,  2.5450],\n",
      "        [-1.5170,  2.1974,  1.7748,  ...,  3.1237,  1.3494,  2.3530],\n",
      "        [-1.0644,  1.5897,  1.0404,  ...,  1.8723,  1.2309,  3.2036]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-2.3282,  3.3182,  0.8662,  ..., -1.8018,  0.1121,  1.2645],\n",
      "        [-0.1489, -1.1255, -3.2108,  ...,  2.4048,  1.1788,  1.7721],\n",
      "        [ 1.7343, -3.0109, -2.8203,  ..., -2.6959, -2.0137,  2.4304],\n",
      "        ...,\n",
      "        [-1.9784, -1.1509, -2.2951,  ..., -3.1357,  1.3754,  1.9324],\n",
      "        [ 1.8854, -2.1329, -2.6846,  ..., -0.4567,  2.1547,  3.1146],\n",
      "        [-1.4652, -2.9201, -2.4124,  ..., -2.5244,  2.7446,  2.2912]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 3.0163, -1.0984, -1.1158,  ...,  0.9934, -1.7508, -1.5575],\n",
      "        [ 2.7501, -2.6068,  0.6494,  ...,  2.6235, -2.7850, -3.2421],\n",
      "        [ 2.2585, -2.0905, -1.3781,  ..., -1.5588,  0.9611,  1.2305],\n",
      "        ...,\n",
      "        [-0.5955,  1.1897,  2.9738,  ...,  2.9398,  1.4762, -2.8796],\n",
      "        [-1.8067, -1.4202,  0.5814,  ..., -0.2245, -0.5212, -1.6854],\n",
      "        [ 2.5011, -2.9009, -2.7637,  ...,  1.2295, -0.2640,  0.0719]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 2.4793, -2.8421,  2.8139,  ..., -3.6925,  2.4190, -2.0070],\n",
      "        [ 1.5329, -1.4502,  1.6507,  ...,  0.3428,  2.0335, -1.4917],\n",
      "        [ 1.4907, -1.5663, -1.8335,  ...,  2.8008, -2.3726, -1.5313],\n",
      "        ...,\n",
      "        [-3.3131,  3.5984, -3.5136,  ...,  3.1850, -2.7448,  2.4950],\n",
      "        [ 0.6056, -0.8190,  1.1326,  ..., -2.4347,  2.5838, -2.5211],\n",
      "        [ 2.5188, -2.6859,  2.6581,  ..., -2.6348,  2.8463, -2.3780]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-1.7282,  0.8266, -1.8636,  ..., -0.1272, -1.3053, -0.2462],\n",
      "        [ 1.8483, -1.9431,  1.9013,  ..., -0.3921,  1.6269, -1.6145],\n",
      "        [-2.6878,  3.2200, -2.7292,  ...,  2.6634, -2.7590,  2.1091],\n",
      "        ...,\n",
      "        [-1.8765,  1.8729, -2.3068,  ...,  2.7785, -2.5540,  1.7654],\n",
      "        [-3.4553,  3.5815, -1.7431,  ...,  2.3833, -3.7353,  1.6682],\n",
      "        [-1.3275,  1.6087, -1.5307,  ...,  1.8797, -1.6668,  1.5408]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-2.8755,  1.0507,  2.2712,  ...,  2.7494,  1.9276, -1.3674],\n",
      "        [ 2.2525, -0.0109, -2.1958,  ..., -1.9964, -2.9383,  2.2209],\n",
      "        [-1.9813,  0.5962,  1.2995,  ...,  0.5642,  2.1934, -1.0922],\n",
      "        ...,\n",
      "        [ 2.1013,  1.2848, -2.0587,  ..., -2.2346, -0.6439,  2.1810],\n",
      "        [ 2.1724, -0.1394, -2.3045,  ..., -2.2553, -0.0533,  1.7399],\n",
      "        [ 2.4756, -2.6129, -2.2340,  ..., -2.4171, -2.0149,  2.2520]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0000],\n",
      "         [-0.7273],\n",
      "         [-0.6970],\n",
      "         [-0.6667],\n",
      "         [-0.6364],\n",
      "         [-0.6061],\n",
      "         [-0.5758],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [ 0.8788],\n",
      "         [ 0.9091],\n",
      "         [ 0.9394],\n",
      "         [ 0.9697],\n",
      "         [ 1.0000],\n",
      "         [ 0.9697],\n",
      "         [-0.4242],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.5758],\n",
      "         [-0.1818],\n",
      "         [-0.1515],\n",
      "         [-0.1212],\n",
      "         [-0.0909],\n",
      "         [-0.0606],\n",
      "         [-0.0303],\n",
      "         [-0.2727],\n",
      "         [ 0.0000],\n",
      "         [ 0.0303],\n",
      "         [ 0.0606],\n",
      "         [ 0.0909],\n",
      "         [ 0.1212],\n",
      "         [-0.5758],\n",
      "         [ 0.1515],\n",
      "         [ 0.1818],\n",
      "         [ 0.2121],\n",
      "         [ 0.2424],\n",
      "         [ 0.2727],\n",
      "         [ 0.3030],\n",
      "         [ 0.3333],\n",
      "         [ 0.3636],\n",
      "         [-0.1515],\n",
      "         [ 0.3939],\n",
      "         [-0.5758],\n",
      "         [ 0.4242],\n",
      "         [ 0.4545],\n",
      "         [ 0.4848]],\n",
      "\n",
      "        [[-0.5455],\n",
      "         [-0.5152],\n",
      "         [-0.4848],\n",
      "         [-0.5455],\n",
      "         [-0.4545],\n",
      "         [-0.6364],\n",
      "         [-0.4242],\n",
      "         [-0.3939],\n",
      "         [-0.3636],\n",
      "         [-0.5455],\n",
      "         [-0.3333],\n",
      "         [-0.3030],\n",
      "         [-0.2727],\n",
      "         [-0.2424],\n",
      "         [-0.2121],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[ 0.5152],\n",
      "         [ 0.5455],\n",
      "         [ 0.5758],\n",
      "         [ 0.6061],\n",
      "         [ 0.6364],\n",
      "         [ 0.6667],\n",
      "         [ 0.6970],\n",
      "         [ 0.7273],\n",
      "         [ 0.5758],\n",
      "         [ 0.7576],\n",
      "         [ 0.6667],\n",
      "         [ 0.7879],\n",
      "         [-0.0606],\n",
      "         [ 0.2727],\n",
      "         [ 0.8182],\n",
      "         [ 0.7879],\n",
      "         [-0.9091],\n",
      "         [ 0.8485],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-1.0000],\n",
      "         [-0.9697],\n",
      "         [-0.9394],\n",
      "         [-0.9091],\n",
      "         [-0.8788],\n",
      "         [-0.8485],\n",
      "         [-0.8182],\n",
      "         [-0.7879],\n",
      "         [-0.7576],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset_creator.detokenize(labels[0], dataset_creator.japanese_dictionary)\n",
    "\n",
    "output = output.detach()\n",
    "predicted = dataset_creator.detokenize(output[0], dataset_creator.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('私 達 は a i は た だ の 数 学 の 集 ま り だ と あ な た は 言 い ま す が で も 人 間 の 脳 が ど の よ う に 機 能 す る か を 正 確 に 知 っ た ら そ れ は あ な た の 生 活 を 小 物 で す か に に に に に に に に', ['私', '達', 'は', 'a', 'i', 'は', 'た', 'だ', 'の', '数', '学', 'の', '集', 'ま', 'り', 'だ', 'と', 'あ', 'な', 'た', 'は', '言', 'い', 'ま', 'す', 'が', 'で', 'も', '人', '間', 'の', '脳', 'が', 'ど', 'の', 'よ', 'う', 'に', '機', '能', 'す', 'る', 'か', 'を', '正', '確', 'に', '知', 'っ', 'た', 'ら', 'そ', 'れ', 'は', 'あ', 'な', 'た', 'の', '生', '活', 'を', '小', '物', 'で', 'す', 'か', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に'])\n",
      "('ん ス め し て て さ さ さ 羨 羨 い い い ゲ ゲ ム ム も も や や た た か か っ っ 良 良 サ サ ポ ポ ー ー ト ト に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に', ['ん', 'ス', 'め', 'し', 'て', 'て', 'さ', 'さ', 'さ', '羨', '羨', 'い', 'い', 'い', 'ゲ', 'ゲ', 'ム', 'ム', 'も', 'も', 'や', 'や', 'た', 'た', 'か', 'か', 'っ', 'っ', '良', '良', 'サ', 'サ', 'ポ', 'ポ', 'ー', 'ー', 'ト', 'ト', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に'])\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
