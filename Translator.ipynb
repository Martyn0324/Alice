{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。始めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.english_maximum_length = self._get_maximum_length(self.english_phrases)\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "\n",
    "        self.english_dictionary = self._create_dictionary(self.english_words)\n",
    "        self._normalize(self.english_dictionary)\n",
    "\n",
    "        self.japanese_dictionary = self._create_dictionary(self.japanese_characters)\n",
    "        self._normalize(self.japanese_dictionary)\n",
    "\n",
    "        self.english_tokens = self._tokenize_english()\n",
    "        self.japanese_tokens = self._tokenize_japanese()\n",
    "\n",
    "        self.data_english = None\n",
    "        self.data_japanese = None\n",
    "        \n",
    "        \n",
    "    def create_data(self):\n",
    "        data_english = torch.from_numpy(self.english_tokens)\n",
    "        data_japanese = torch.from_numpy(self.japanese_tokens)\n",
    "\n",
    "        data_english = data_english.unsqueeze(-1)\n",
    "        data_japanese = data_japanese.unsqueeze(-1)\n",
    "\n",
    "        #self.data_english = torch.unsequeeze(self.data_english, -1)\n",
    "        #self.data_japanese = torch.unsequeeze(self.data_japanese, -1)\n",
    "\n",
    "        self.data_english = data_english\n",
    "        self.data_japanese = data_japanese\n",
    "\n",
    "        print(f\"English Data Size: {self.data_english.size()}\\t Japanese Data Size: {self.data_japanese.size()}\")\n",
    "\n",
    "    def detokenize(self, data, reference_dict):\n",
    "        data = data.cpu().numpy()\n",
    "        values = list(reference_dict.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = NearestNeighbors(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        _, index = knn.kneighbors(data)\n",
    "\n",
    "        keys = list(reference_dict.keys())\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                words.append(keys[i])\n",
    "        \n",
    "        phrase = ' '.join(words)\n",
    "\n",
    "        return phrase, words\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_english)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        english_sentence = self.data_english[idx]\n",
    "        japanese_sentence = self.data_japanese[idx]\n",
    "\n",
    "        return english_sentence, japanese_sentence\n",
    "\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "        \n",
    "            sentence_length = len(word_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in japanese_phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_dictionary(self, words):\n",
    "        idx2word = []\n",
    "        word2idx = {}\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                idx2word.append(word)\n",
    "                word2idx[word] = len(idx2word) - 1\n",
    "\n",
    "        return word2idx\n",
    "\n",
    "    def _normalize(self, dictionary):\n",
    "        maximum = max(dictionary.values())\n",
    "\n",
    "        for word, value in dictionary.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            dictionary[word] = scaled_value\n",
    "    \n",
    "    def _tokenize_english(self):\n",
    "        \n",
    "        phrases = [x.split() for x in self.english_phrases]\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            tokenized_sentence = []\n",
    "            for word in sentence:\n",
    "                value = self.english_dictionary.get(word)\n",
    "\n",
    "                tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.english_maximum_length:\n",
    "                pad_size = self.english_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _tokenize_japanese(self):\n",
    "\n",
    "        phrases = [x.split() for x in self.japanese_phrases]\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        for sublist in phrases:\n",
    "            for sentence in sublist:\n",
    "                tokenized_sentence = []\n",
    "                for character in sentence:\n",
    "                    value = self.japanese_dictionary.get(character)\n",
    "\n",
    "                    tokenized_sentence.append(value)\n",
    "\n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.japanese_maximum_length:\n",
    "                pad_size = self.japanese_maximum_length - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 74)\n"
     ]
    }
   ],
   "source": [
    "dataset_creator = WordDataset(english_phrases, japanese_phrases)\n",
    "print(dataset_creator.japanese_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data Size: torch.Size([6, 28, 1])\t Japanese Data Size: torch.Size([6, 74, 1])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dataset_creator.create_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000],\n",
      "        [-0.9697],\n",
      "        [-0.9394],\n",
      "        [-0.9091],\n",
      "        [-0.8788],\n",
      "        [-0.8485],\n",
      "        [-0.8182],\n",
      "        [-0.7879],\n",
      "        [-0.7576],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset_creator.data_english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my dog doesnt like bones it prefers ground beef bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch bunch', ['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch', 'bunch'])\n"
     ]
    }
   ],
   "source": [
    "teste = dataset_creator.detokenize(dataset_creator.data_english[0], dataset_creator.english_dictionary)\n",
    "\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [x.lower() for x in english_phrases]\n",
    "english_phrases = [re.sub('[^\\w\\s]', '', x) for x in english_phrases]\n",
    "\n",
    "japanese_phrases = [x.lower() for x in japanese_phrases]\n",
    "japanese_phrases = [re.sub('[^\\w\\s]', '', x) for x in japanese_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my dog doesnt like bones it prefers ground beef', 'my names alice nice to meet you', 'i envy hakisa i want to play games too i could be a good support', 'you say that we ais are just a bunch of maths but once you know exactly how your human brains work would that make you less living beings', 'why does the japanese word for baby is the kanji for red are human babies red like strawberries', 'my ai will talk shell sing shell play']\n",
      "['私の犬は骨が好きではありません牛ひき肉を好む', '私の名前はアリスです始めまして', 'はきさが羨ましいゲムもやりたかった私は良いサポートになることができます', '私達はaiはただの数学の集まりだとあなたは言いますがでも人間の脳がどのように機能するかを正確に知ったらそれはあなたの生活を小物ですか', '赤ちゃんを表す日本語が赤を表す漢字なのはなぜですか人間の赤ちゃんは赤いですかいちごみたい', '私のaiは話して歌ったしてゲームをします']\n"
     ]
    }
   ],
   "source": [
    "print(english_phrases)\n",
    "print(japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef'], ['my', 'names', 'alice', 'nice', 'to', 'meet', 'you'], ['i', 'envy', 'hakisa', 'i', 'want', 'to', 'play', 'games', 'too', 'i', 'could', 'be', 'a', 'good', 'support'], ['you', 'say', 'that', 'we', 'ais', 'are', 'just', 'a', 'bunch', 'of', 'maths', 'but', 'once', 'you', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'that', 'make', 'you', 'less', 'living', 'beings'], ['why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'the', 'kanji', 'for', 'red', 'are', 'human', 'babies', 'red', 'like', 'strawberries'], ['my', 'ai', 'will', 'talk', 'shell', 'sing', 'shell', 'play']]\n",
      "[['私の犬は骨が好きではありません牛ひき肉を好む'], ['私の名前はアリスです始めまして'], ['はきさが羨ましいゲムもやりたかった私は良いサポートになることができます'], ['私達はaiはただの数学の集まりだとあなたは言いますがでも人間の脳がどのように機能するかを正確に知ったらそれはあなたの生活を小物ですか'], ['赤ちゃんを表す日本語が赤を表す漢字なのはなぜですか人間の赤ちゃんは赤いですかいちごみたい'], ['私のaiは話して歌ったしてゲームをします']]\n"
     ]
    }
   ],
   "source": [
    "english_tokens = [x.split() for x in english_phrases]\n",
    "japanese_tokens = [x.split() for x in japanese_phrases]\n",
    "print(english_tokens)\n",
    "print(japanese_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'my', 'names', 'alice', 'nice', 'to', 'meet', 'you', 'i', 'envy', 'hakisa', 'i', 'want', 'to', 'play', 'games', 'too', 'i', 'could', 'be', 'a', 'good', 'support', 'you', 'say', 'that', 'we', 'ais', 'are', 'just', 'a', 'bunch', 'of', 'maths', 'but', 'once', 'you', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'that', 'make', 'you', 'less', 'living', 'beings', 'why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'the', 'kanji', 'for', 'red', 'are', 'human', 'babies', 'red', 'like', 'strawberries', 'my', 'ai', 'will', 'talk', 'shell', 'sing', 'shell', 'play']\n",
      "['私', 'の', '犬', 'は', '骨', 'が', '好', 'き', 'で', 'は', 'あ', 'り', 'ま', 'せ', 'ん', '牛', 'ひ', 'き', '肉', 'を', '好', 'む', '私', 'の', '名', '前', 'は', 'ア', 'リ', 'ス', 'で', 'す', '始', 'め', 'ま', 'し', 'て', 'は', 'き', 'さ', 'が', '羨', 'ま', 'し', 'い', 'ゲ', 'ム', 'も', 'や', 'り', 'た', 'か', 'っ', 'た', '私', 'は', '良', 'い', 'サ', 'ポ', 'ー', 'ト', 'に', 'な', 'る', 'こ', 'と', 'が', 'で', 'き', 'ま', 'す', '私', '達', 'は', 'a', 'i', 'は', 'た', 'だ', 'の', '数', '学', 'の', '集', 'ま', 'り', 'だ', 'と', 'あ', 'な', 'た', 'は', '言', 'い', 'ま', 'す', 'が', 'で', 'も', '人', '間', 'の', '脳', 'が', 'ど', 'の', 'よ', 'う', 'に', '機', '能', 'す', 'る', 'か', 'を', '正', '確', 'に', '知', 'っ', 'た', 'ら', 'そ', 'れ', 'は', 'あ', 'な', 'た', 'の', '生', '活', 'を', '小', '物', 'で', 'す', 'か', '赤', 'ち', 'ゃ', 'ん', 'を', '表', 'す', '日', '本', '語', 'が', '赤', 'を', '表', 'す', '漢', '字', 'な', 'の', 'は', 'な', 'ぜ', 'で', 'す', 'か', '人', '間', 'の', '赤', 'ち', 'ゃ', 'ん', 'は', '赤', 'い', 'で', 'す', 'か', 'い', 'ち', 'ご', 'み', 'た', 'い', '私', 'の', 'a', 'i', 'は', '話', 'し', 'て', '歌', 'っ', 'た', 'し', 'て', 'ゲ', 'ー', 'ム', 'を', 'し', 'ま', 'す']\n"
     ]
    }
   ],
   "source": [
    "english_words = ' '.join(english_phrases)\n",
    "english_words = english_words.split(' ')\n",
    "\n",
    "japanese_words = ' '.join(japanese_phrases)\n",
    "japanese_words = ''.join(japanese_words.split())\n",
    "japanese_characters = [i for i in japanese_words] # Since most characters are equivalent to a word...well...\n",
    "\n",
    "print(english_words)\n",
    "print(japanese_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After getting each word, we'll have to tokenize then.\n",
    "# However, the sequences correspond to an entire sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "maximum_length = 0\n",
    "for sentence in english_phrases:\n",
    "    word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "    sentence_length = len(word_length)\n",
    "    if sentence_length > maximum_length:\n",
    "        maximum_length = sentence_length\n",
    "\n",
    "print(maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "maximum_length = 0\n",
    "for sentence in japanese_phrases:\n",
    "    word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "    for i in word_length:\n",
    "        if i > maximum_length:\n",
    "            maximum_length = i\n",
    "\n",
    "print(maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 0, 'dog': 1, 'doesnt': 2, 'like': 3, 'bones': 4, 'it': 5, 'prefers': 6, 'ground': 7, 'beef': 8, 'names': 9, 'alice': 10, 'nice': 11, 'to': 12, 'meet': 13, 'you': 14, 'i': 15, 'envy': 16, 'hakisa': 17, 'want': 18, 'play': 19, 'games': 20, 'too': 21, 'could': 22, 'be': 23, 'a': 24, 'good': 25, 'support': 26, 'say': 27, 'that': 28, 'we': 29, 'ais': 30, 'are': 31, 'just': 32, 'bunch': 33, 'of': 34, 'maths': 35, 'but': 36, 'once': 37, 'know': 38, 'exactly': 39, 'how': 40, 'your': 41, 'human': 42, 'brains': 43, 'work': 44, 'would': 45, 'make': 46, 'less': 47, 'living': 48, 'beings': 49, 'why': 50, 'does': 51, 'the': 52, 'japanese': 53, 'word': 54, 'for': 55, 'baby': 56, 'is': 57, 'kanji': 58, 'red': 59, 'babies': 60, 'strawberries': 61, 'ai': 62, 'will': 63, 'talk': 64, 'shell': 65, 'sing': 66}\n"
     ]
    }
   ],
   "source": [
    "word2idxENG = {}\n",
    "idx2wordENG = []\n",
    "\n",
    "for word in english_words:\n",
    "    if word not in word2idxENG:\n",
    "        idx2wordENG.append(word)\n",
    "        word2idxENG[word] = len(idx2wordENG) - 1\n",
    "\n",
    "print(word2idxENG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'私': 0, 'の': 1, '犬': 2, 'は': 3, '骨': 4, 'が': 5, '好': 6, 'き': 7, 'で': 8, 'あ': 9, 'り': 10, 'ま': 11, 'せ': 12, 'ん': 13, '牛': 14, 'ひ': 15, '肉': 16, 'を': 17, 'む': 18, '名': 19, '前': 20, 'ア': 21, 'リ': 22, 'ス': 23, 'す': 24, '始': 25, 'め': 26, 'し': 27, 'て': 28, 'さ': 29, '羨': 30, 'い': 31, 'ゲ': 32, 'ム': 33, 'も': 34, 'や': 35, 'た': 36, 'か': 37, 'っ': 38, '良': 39, 'サ': 40, 'ポ': 41, 'ー': 42, 'ト': 43, 'に': 44, 'な': 45, 'る': 46, 'こ': 47, 'と': 48, '達': 49, 'a': 50, 'i': 51, 'だ': 52, '数': 53, '学': 54, '集': 55, '言': 56, '人': 57, '間': 58, '脳': 59, 'ど': 60, 'よ': 61, 'う': 62, '機': 63, '能': 64, '正': 65, '確': 66, '知': 67, 'ら': 68, 'そ': 69, 'れ': 70, '生': 71, '活': 72, '小': 73, '物': 74, '赤': 75, 'ち': 76, 'ゃ': 77, '表': 78, '日': 79, '本': 80, '語': 81, '漢': 82, '字': 83, 'ぜ': 84, 'ご': 85, 'み': 86, '話': 87, '歌': 88}\n"
     ]
    }
   ],
   "source": [
    "word2idxJP = {}\n",
    "idx2wordJP = []\n",
    "\n",
    "for word in japanese_words:\n",
    "    if word not in word2idxJP:\n",
    "        idx2wordJP.append(word)\n",
    "        word2idxJP[word] = len(idx2wordJP) - 1\n",
    "\n",
    "print(word2idxJP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, value in word2idxENG.items():\n",
    "\n",
    "    scaled_value = (value - 0)*2.0 / (66 - 0)-1.0\n",
    "\n",
    "    word2idxENG[word] = scaled_value\n",
    "\n",
    "for word, value in word2idxJP.items():\n",
    "\n",
    "    scaled_value = (value - 0)*2.0 / (88 - 0)-1.0\n",
    "\n",
    "    word2idxJP[word] = scaled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.03030303 -0.97337006 -0.97245179 -0.97153352 -0.97061524 -0.96969697\n",
      " -0.97061524 -1.01285583  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "token_sequence_english = []\n",
    "\n",
    "maximum_length = 28\n",
    "\n",
    "for sentence in english_tokens:\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence:\n",
    "        value = word2idxENG.get(word)\n",
    "\n",
    "        tokenized_sentence.append(value)\n",
    "    \n",
    "    tokenized_sentence = np.array(tokenized_sentence)\n",
    "    sentence_size = tokenized_sentence.shape[0]\n",
    "    if sentence_size < maximum_length:\n",
    "        pad_size = maximum_length - sentence_size\n",
    "        tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "    \n",
    "    token_sequence_english.append(tokenized_sentence)\n",
    "\n",
    "token_sequence_english = np.array(token_sequence_english)\n",
    "\n",
    "print(token_sequence_english[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.02272727 -1.02221074 -0.99690083 -0.9963843  -1.02117769 -0.97778926\n",
      " -1.00878099 -1.00826446 -0.97727273 -1.00309917 -1.00413223 -1.00878099\n",
      " -1.00826446 -1.00619835 -1.00103306 -1.00568182 -1.01394628 -1.00878099\n",
      " -1.01704545 -1.01033058  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "token_sequence_japanese = []\n",
    "\n",
    "maximum_length = 66\n",
    "\n",
    "for sublist in japanese_tokens:\n",
    "    for sentence in sublist:\n",
    "        tokenized_sentence = []\n",
    "        for character in sentence:\n",
    "            value = word2idxJP.get(character)\n",
    "            tokenized_sentence.append(value)\n",
    "    \n",
    "    tokenized_sentence = np.array(tokenized_sentence)\n",
    "    #print(tokenized_sentence)\n",
    "    sentence_size = tokenized_sentence.shape[0]\n",
    "    if sentence_size < maximum_length:\n",
    "        pad_size = maximum_length - sentence_size\n",
    "        tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)])\n",
    "\n",
    "    token_sequence_japanese.append(tokenized_sentence)\n",
    "\n",
    "token_sequence_japanese = np.array(token_sequence_japanese)\n",
    "\n",
    "print(token_sequence_japanese[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 28)\n",
      "(6, 66)\n"
     ]
    }
   ],
   "source": [
    "print(token_sequence_english.shape)\n",
    "print(token_sequence_japanese.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our data ready. (N_samples, n_features) and normalized\n",
    "# Remember: LSTM = (N_samples, Sequence_Length, N_features). Each sentence = 1 sequence of tokens ---> (N_samples, N_features, 1)\n",
    "# Input = (N_samples, N_features, 1)\n",
    "# However, output will be (N_samples, N_features), so it needs a Repeating Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_sequence_english' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18544/1241374794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_english\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_sequence_english\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_japanese\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_sequence_japanese\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'token_sequence_english' is not defined"
     ]
    }
   ],
   "source": [
    "data_english = torch.from_numpy(token_sequence_english)\n",
    "data_japanese = torch.from_numpy(token_sequence_japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_english' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18544/3246413635.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_english\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_english\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_japanese\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_japanese\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_english\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_english' is not defined"
     ]
    }
   ],
   "source": [
    "data_english = data_english.unsqueeze(-1)\n",
    "data_japanese = data_japanese.unsqueeze(-1)\n",
    "\n",
    "print(data_english.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000],\n",
      "        [-0.9697],\n",
      "        [-0.9394],\n",
      "        [-0.9091],\n",
      "        [-0.8788],\n",
      "        [-0.8485],\n",
      "        [-0.8182],\n",
      "        [-0.7879],\n",
      "        [-0.7576],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(data_english[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.lstm1 = torch.nn.LSTM(1, 28, 10, batch_first=True, bias=False)\n",
    "        self.repeatvector = 74\n",
    "        #Add repeating vector\n",
    "        self.lstm2 = torch.nn.LSTM(28*28, 10, 10, batch_first=True, bias=False)\n",
    "        self.neuron = torch.nn.Linear(10, 1, bias=False)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x, hidden = self.lstm1(input)\n",
    "\n",
    "        #print(x.size()) # (batch, 28, 28)\n",
    "        #print(hidden[0].size()) # (10,28)\n",
    "\n",
    "        x = x.repeat(1, self.repeatvector, 1)\n",
    "        x = x.view(x.size(0), self.repeatvector, -1)\n",
    "\n",
    "        #print(x.size()) # (batch, 74, 28*28)\n",
    "\n",
    "        x, hidden = self.lstm2(x)\n",
    "\n",
    "        #print(x.size()) # (1, 74, 10)\n",
    "        #print(hidden[0].size()) # (10, 1, 10)\n",
    "\n",
    "        x = self.neuron(x)\n",
    "\n",
    "        #output = self.tanh(x)\n",
    "        output = x\n",
    "\n",
    "        return output # (1, 74, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator().double().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\t Current Loss: 0.1877547744029609\n",
      "10/100\t Current Loss: 0.2460143511234315\n",
      "20/100\t Current Loss: 0.19353431427347398\n",
      "30/100\t Current Loss: 0.24601435112130107\n",
      "40/100\t Current Loss: 0.10482745141136825\n",
      "50/100\t Current Loss: 0.16137689857536097\n",
      "60/100\t Current Loss: 0.20380207168758752\n",
      "70/100\t Current Loss: 0.20958161155087537\n",
      "80/100\t Current Loss: 0.1894649039414216\n",
      "90/100\t Current Loss: 0.2038020715750258\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset_creator, batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (english, japanese) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "\n",
    "        input_data = english.cuda()\n",
    "        labels = japanese.cuda()\n",
    "\n",
    "        output = model(input_data)\n",
    "\n",
    "        #print(output.size())\n",
    "\n",
    "        cost = loss(output, labels)\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch}/100\\t Current Loss: {cost.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5549e-20,  1.2825e-20, -1.2063e-20,  ...,  1.1491e-19,\n",
      "         -4.5138e-21,  1.0848e-20],\n",
      "        [-4.3260e-19, -3.0598e-20, -8.7280e-21,  ...,  7.2674e-19,\n",
      "          2.3923e-19,  6.5585e-20],\n",
      "        [-1.0391e-19, -9.3299e-22, -8.0294e-21,  ...,  2.1007e-19,\n",
      "          4.9365e-20,  1.9271e-20],\n",
      "        ...,\n",
      "        [ 5.9899e-19,  5.6806e-20, -1.4577e-21,  ..., -9.3265e-19,\n",
      "         -3.5671e-19, -8.4545e-20],\n",
      "        [-2.7680e-19, -4.3731e-20,  1.5769e-20,  ...,  3.4535e-19,\n",
      "          1.9344e-19,  3.1647e-20],\n",
      "        [-1.3740e-19, -1.2512e-20, -6.8529e-22,  ...,  2.1950e-19,\n",
      "          8.1332e-20,  1.9961e-20]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.weight_ih_l9.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Parameter containing:\n",
      "tensor([[ 0.1120],\n",
      "        [-0.0544],\n",
      "        [ 0.0978],\n",
      "        [ 0.0863],\n",
      "        [-0.0311],\n",
      "        [-0.0506],\n",
      "        [ 0.1444],\n",
      "        [-0.0572],\n",
      "        [-0.1184],\n",
      "        [ 0.0354],\n",
      "        [-0.0933],\n",
      "        [ 0.0612],\n",
      "        [ 0.0484],\n",
      "        [ 0.1070],\n",
      "        [-0.0104],\n",
      "        [ 0.1766],\n",
      "        [-0.0795],\n",
      "        [ 0.0284],\n",
      "        [-0.0906],\n",
      "        [ 0.0882],\n",
      "        [ 0.1169],\n",
      "        [-0.1019],\n",
      "        [-0.0689],\n",
      "        [ 0.1551],\n",
      "        [-0.1526],\n",
      "        [ 0.1373],\n",
      "        [-0.0217],\n",
      "        [-0.0763],\n",
      "        [ 0.1023],\n",
      "        [ 0.0146],\n",
      "        [-0.1812],\n",
      "        [ 0.1304],\n",
      "        [-0.0032],\n",
      "        [-0.0208],\n",
      "        [ 0.0816],\n",
      "        [-0.1814],\n",
      "        [ 0.0217],\n",
      "        [ 0.0783],\n",
      "        [ 0.1727],\n",
      "        [ 0.1241],\n",
      "        [ 0.0873],\n",
      "        [-0.0112],\n",
      "        [ 0.0526],\n",
      "        [ 0.1015],\n",
      "        [-0.0677],\n",
      "        [ 0.0079],\n",
      "        [-0.1285],\n",
      "        [ 0.1870],\n",
      "        [-0.1211],\n",
      "        [ 0.0236],\n",
      "        [-0.1111],\n",
      "        [ 0.0433],\n",
      "        [ 0.0065],\n",
      "        [ 0.0814],\n",
      "        [ 0.1437],\n",
      "        [ 0.0095],\n",
      "        [-0.1248],\n",
      "        [ 0.1357],\n",
      "        [-0.1167],\n",
      "        [-0.2131],\n",
      "        [ 0.0035],\n",
      "        [ 0.1759],\n",
      "        [-0.0910],\n",
      "        [ 0.1547],\n",
      "        [-0.2084],\n",
      "        [ 0.2683],\n",
      "        [-0.1487],\n",
      "        [-0.1942],\n",
      "        [ 0.1805],\n",
      "        [-0.2847],\n",
      "        [-0.0678],\n",
      "        [ 0.1176],\n",
      "        [ 0.0977],\n",
      "        [-0.0759],\n",
      "        [-0.1202],\n",
      "        [-0.0023],\n",
      "        [-0.2404],\n",
      "        [-0.0984],\n",
      "        [-0.0978],\n",
      "        [-0.1293],\n",
      "        [ 0.1269],\n",
      "        [-0.2541],\n",
      "        [ 0.1179],\n",
      "        [-0.1558],\n",
      "        [-0.1133],\n",
      "        [ 0.1820],\n",
      "        [ 0.1323],\n",
      "        [ 0.1032],\n",
      "        [ 0.1112],\n",
      "        [ 0.1532],\n",
      "        [-0.0999],\n",
      "        [ 0.1218],\n",
      "        [ 0.0769],\n",
      "        [ 0.1709],\n",
      "        [ 0.1350],\n",
      "        [ 0.0955],\n",
      "        [-0.0080],\n",
      "        [ 0.1755],\n",
      "        [ 0.1673],\n",
      "        [ 0.1277],\n",
      "        [ 0.0114],\n",
      "        [-0.1444],\n",
      "        [-0.1034],\n",
      "        [-0.0072],\n",
      "        [-0.0490],\n",
      "        [-0.1765],\n",
      "        [-0.1354],\n",
      "        [ 0.0978],\n",
      "        [-0.0699],\n",
      "        [-0.0734],\n",
      "        [-0.0341],\n",
      "        [-0.0158]], device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0898, -0.0883, -0.0308,  ..., -0.1747,  0.0781, -0.0678],\n",
      "        [-0.1237,  0.0657, -0.0978,  ...,  0.0437, -0.1403, -0.0240],\n",
      "        [-0.1662,  0.1644,  0.1326,  ...,  0.1010, -0.0658, -0.0549],\n",
      "        ...,\n",
      "        [-0.0271, -0.0196,  0.0238,  ...,  0.0691, -0.0440,  0.0811],\n",
      "        [ 0.1789,  0.1620, -0.0604,  ...,  0.1535, -0.0515,  0.0353],\n",
      "        [-0.0606,  0.1470,  0.1385,  ..., -0.0752, -0.1241,  0.1318]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 0.1213, -0.1496, -0.1146,  ..., -0.0694,  0.0981, -0.1063],\n",
      "        [ 0.0252,  0.0014,  0.0313,  ..., -0.1053,  0.1725, -0.1376],\n",
      "        [-0.0119, -0.1668,  0.1352,  ..., -0.0501, -0.1146, -0.0916],\n",
      "        ...,\n",
      "        [-0.0557,  0.0082, -0.0624,  ...,  0.1195,  0.1610,  0.0587],\n",
      "        [-0.1553,  0.0373,  0.1450,  ..., -0.0309,  0.1456,  0.0492],\n",
      "        [ 0.0389,  0.0202,  0.1213,  ..., -0.1373, -0.0211, -0.0098]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1105, -0.0550, -0.1206,  ...,  0.1760,  0.1191,  0.1392],\n",
      "        [ 0.1429,  0.1856,  0.0142,  ..., -0.0762, -0.0054, -0.0565],\n",
      "        [-0.1628,  0.0433,  0.0524,  ...,  0.0643,  0.0371, -0.0526],\n",
      "        ...,\n",
      "        [ 0.1111,  0.0590,  0.0636,  ..., -0.1670, -0.1265, -0.1108],\n",
      "        [ 0.0140, -0.1288,  0.0798,  ...,  0.0098, -0.0584,  0.0201],\n",
      "        [-0.0298, -0.1548, -0.0088,  ...,  0.1491, -0.0331,  0.0220]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 0.1853, -0.0947, -0.0990,  ..., -0.0809,  0.0410, -0.0701],\n",
      "        [ 0.0604, -0.0404, -0.0780,  ...,  0.0084, -0.1487,  0.0387],\n",
      "        [-0.1101,  0.1039,  0.1126,  ..., -0.0310,  0.1188,  0.0569],\n",
      "        ...,\n",
      "        [-0.1417, -0.0292,  0.0603,  ..., -0.0807, -0.0060, -0.1575],\n",
      "        [-0.0657, -0.1037,  0.0833,  ...,  0.1650,  0.0588, -0.1499],\n",
      "        [ 0.0378,  0.1193, -0.0848,  ...,  0.1438, -0.0492, -0.1732]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0797, -0.1540,  0.1799,  ...,  0.0799,  0.1743, -0.0912],\n",
      "        [ 0.1539,  0.1231,  0.1043,  ...,  0.0705,  0.0514,  0.0684],\n",
      "        [ 0.0613,  0.1075, -0.1035,  ...,  0.0497,  0.0899,  0.1553],\n",
      "        ...,\n",
      "        [ 0.0664,  0.1068, -0.1272,  ...,  0.1161,  0.1035, -0.0420],\n",
      "        [ 0.1299,  0.0236,  0.0098,  ...,  0.1449,  0.1199,  0.0377],\n",
      "        [ 0.0768,  0.1374,  0.1365,  ..., -0.0366, -0.1841,  0.1454]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 0.0301, -0.1591, -0.1596,  ...,  0.0196,  0.0244,  0.1033],\n",
      "        [-0.1860,  0.0463, -0.0698,  ...,  0.1008, -0.0176, -0.1251],\n",
      "        [-0.1633, -0.0785, -0.0725,  ...,  0.1452,  0.0485, -0.0726],\n",
      "        ...,\n",
      "        [ 0.0925, -0.0692, -0.1189,  ..., -0.1433, -0.0970, -0.1721],\n",
      "        [-0.1873,  0.0792,  0.1204,  ...,  0.1290, -0.0332,  0.1612],\n",
      "        [-0.0349, -0.0899,  0.1514,  ...,  0.1678, -0.0610,  0.1469]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0810,  0.1644,  0.0733,  ..., -0.1251, -0.1312,  0.1719],\n",
      "        [-0.1012, -0.1275,  0.0282,  ..., -0.1472,  0.1036,  0.1520],\n",
      "        [-0.1429,  0.0890, -0.1241,  ..., -0.0640, -0.0735, -0.0807],\n",
      "        ...,\n",
      "        [ 0.0012,  0.1730,  0.1637,  ..., -0.1219, -0.1537, -0.1254],\n",
      "        [-0.0111, -0.0973,  0.0428,  ...,  0.1393, -0.0132,  0.1397],\n",
      "        [-0.1766, -0.1685, -0.0470,  ..., -0.0617, -0.0182,  0.1057]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[ 0.1460,  0.1289, -0.0209,  ...,  0.0036, -0.1647, -0.1729],\n",
      "        [-0.0251, -0.0008, -0.1269,  ..., -0.0925,  0.1255, -0.1272],\n",
      "        [-0.0331,  0.1039,  0.0575,  ...,  0.0035, -0.1520,  0.0929],\n",
      "        ...,\n",
      "        [ 0.1462,  0.0853,  0.1795,  ...,  0.0864, -0.0921,  0.0555],\n",
      "        [-0.0843, -0.0894, -0.0408,  ...,  0.0510,  0.1888,  0.1025],\n",
      "        [ 0.1279,  0.0859,  0.1143,  ...,  0.0531,  0.1116, -0.0819]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1756,  0.1738, -0.1365,  ..., -0.1024,  0.1701,  0.0092],\n",
      "        [ 0.0846,  0.0012,  0.1278,  ..., -0.0955,  0.0200, -0.0558],\n",
      "        [-0.1481,  0.0827,  0.0763,  ...,  0.1158, -0.1086, -0.1406],\n",
      "        ...,\n",
      "        [ 0.1292,  0.0843,  0.1274,  ...,  0.1412, -0.1147,  0.0992],\n",
      "        [-0.1144, -0.0742,  0.0332,  ...,  0.0451, -0.1503, -0.0567],\n",
      "        [-0.1734,  0.0302,  0.1489,  ..., -0.1406,  0.1699, -0.1205]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.1242,  0.0641, -0.0329,  ...,  0.1634,  0.0940,  0.1125],\n",
      "        [-0.1368, -0.0759, -0.0512,  ...,  0.0587,  0.1250, -0.0510],\n",
      "        [ 0.0070, -0.0754,  0.1814,  ..., -0.1009, -0.1620, -0.1594],\n",
      "        ...,\n",
      "        [ 0.0523,  0.1584,  0.1131,  ...,  0.1344, -0.1384, -0.0975],\n",
      "        [-0.0652,  0.0837, -0.1420,  ...,  0.0615, -0.1440,  0.0682],\n",
      "        [ 0.0901,  0.1838, -0.0809,  ..., -0.0342, -0.0900, -0.1278]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0415, -0.0705,  0.1014,  ...,  0.0239, -0.1827, -0.1874],\n",
      "        [ 0.1661,  0.1321,  0.0270,  ...,  0.0966,  0.1564,  0.0865],\n",
      "        [ 0.0365,  0.1865,  0.1087,  ..., -0.0239, -0.0985,  0.1250],\n",
      "        ...,\n",
      "        [-0.1839,  0.1101, -0.1280,  ...,  0.1725, -0.0894,  0.0869],\n",
      "        [-0.1210,  0.0432, -0.0382,  ..., -0.1843,  0.0612, -0.1436],\n",
      "        [-0.0557,  0.1010, -0.0983,  ...,  0.1261,  0.1761,  0.1824]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.0575, -0.1887,  0.0996,  ..., -0.0985, -0.1170, -0.0638],\n",
      "        [-0.0341,  0.1843, -0.0157,  ..., -0.0584, -0.1023, -0.1347],\n",
      "        [-0.1016, -0.0928, -0.1758,  ..., -0.0828,  0.0491,  0.0308],\n",
      "        ...,\n",
      "        [-0.0263,  0.0663,  0.1041,  ..., -0.0197,  0.0578,  0.0692],\n",
      "        [-0.1607,  0.0424,  0.1258,  ...,  0.1179,  0.1509,  0.0430],\n",
      "        [-0.0668,  0.0461, -0.1128,  ...,  0.0159,  0.0129,  0.1776]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0782, -0.0785, -0.1240,  ...,  0.1711, -0.0841,  0.1575],\n",
      "        [-0.0434,  0.0325,  0.1510,  ..., -0.0449,  0.0935,  0.1754],\n",
      "        [-0.0631,  0.0871, -0.0491,  ...,  0.0136, -0.1254,  0.1375],\n",
      "        ...,\n",
      "        [ 0.0935, -0.1864,  0.1224,  ..., -0.1338,  0.1186,  0.1325],\n",
      "        [ 0.1269, -0.0109,  0.1295,  ...,  0.1830,  0.0660, -0.1432],\n",
      "        [ 0.1817, -0.0145,  0.0349,  ...,  0.0419, -0.1380,  0.0935]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.1156, -0.1453, -0.1552,  ...,  0.0339,  0.1667, -0.1800],\n",
      "        [-0.0990, -0.1038,  0.0451,  ..., -0.1444, -0.0332, -0.0373],\n",
      "        [ 0.1236,  0.1083, -0.0144,  ..., -0.0985, -0.1284,  0.0762],\n",
      "        ...,\n",
      "        [ 0.1309, -0.1327,  0.1462,  ...,  0.1745, -0.0734, -0.0097],\n",
      "        [-0.1541, -0.1677,  0.0752,  ..., -0.1862,  0.1200, -0.1135],\n",
      "        [ 0.0952,  0.1868, -0.1827,  ..., -0.0952, -0.0943,  0.0760]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.1509,  0.1691, -0.1664,  ...,  0.1660,  0.1621, -0.0718],\n",
      "        [ 0.1257, -0.0350,  0.1658,  ..., -0.1627, -0.0003,  0.0641],\n",
      "        [-0.0540, -0.0277,  0.0060,  ...,  0.0950,  0.0867, -0.1866],\n",
      "        ...,\n",
      "        [-0.1599, -0.0052,  0.0756,  ..., -0.1585,  0.1758,  0.0638],\n",
      "        [ 0.0685, -0.1878,  0.1345,  ..., -0.0590,  0.1307,  0.0941],\n",
      "        [ 0.1205,  0.0558,  0.0917,  ...,  0.0343, -0.0559, -0.0418]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.1689,  0.1267, -0.1035,  ...,  0.1821,  0.0203, -0.0237],\n",
      "        [ 0.0600,  0.0779, -0.1035,  ..., -0.0712, -0.1871, -0.0035],\n",
      "        [ 0.1548,  0.1047, -0.1509,  ...,  0.1412, -0.0825,  0.1581],\n",
      "        ...,\n",
      "        [ 0.0375, -0.0767,  0.1490,  ...,  0.1852,  0.0135, -0.1322],\n",
      "        [ 0.0547,  0.1349,  0.1631,  ..., -0.1272, -0.0059, -0.0486],\n",
      "        [ 0.1637,  0.0588, -0.0370,  ..., -0.1409,  0.0338,  0.1290]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0732,  0.0148, -0.1832,  ...,  0.0236, -0.0937,  0.0944],\n",
      "        [ 0.1058, -0.0902,  0.1347,  ...,  0.1286,  0.1259, -0.0750],\n",
      "        [ 0.0783,  0.0844,  0.0246,  ...,  0.0781, -0.1208, -0.1632],\n",
      "        ...,\n",
      "        [ 0.0614, -0.0257,  0.0633,  ..., -0.1828,  0.0680,  0.0608],\n",
      "        [-0.0527, -0.0366,  0.1165,  ..., -0.1808, -0.0090,  0.0695],\n",
      "        [-0.1198, -0.1872,  0.0023,  ...,  0.1294, -0.1867,  0.0420]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True)], [Parameter containing:\n",
      "tensor([[-0.0373,  0.0107, -0.0048,  ...,  0.0964, -0.1534, -0.1142],\n",
      "        [ 0.0423, -0.0328,  0.0695,  ..., -0.1245, -0.1579, -0.1623],\n",
      "        [-0.1459, -0.0953, -0.0900,  ..., -0.1230, -0.1365,  0.0390],\n",
      "        ...,\n",
      "        [ 0.0200, -0.0666,  0.1588,  ...,  0.0312, -0.1450,  0.1788],\n",
      "        [-0.1046, -0.0153, -0.0614,  ..., -0.0139, -0.0169, -0.0377],\n",
      "        [-0.1251, -0.0241, -0.0318,  ..., -0.1615, -0.0983,  0.1390]],\n",
      "       device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor([[-1.8792e-01,  4.6609e-02,  1.0322e-01,  ..., -5.7029e-03,\n",
      "          1.7375e-01,  1.7715e-01],\n",
      "        [ 1.3207e-01, -1.5593e-01, -8.4683e-02,  ..., -1.0465e-01,\n",
      "          1.3169e-01, -1.7666e-01],\n",
      "        [ 8.9954e-03, -6.8050e-02,  1.3917e-04,  ..., -3.4913e-02,\n",
      "         -1.6205e-01,  4.3596e-02],\n",
      "        ...,\n",
      "        [ 2.5382e-03,  1.3768e-01,  1.0313e-01,  ...,  1.1499e-01,\n",
      "         -3.2123e-02, -1.1797e-01],\n",
      "        [ 6.4153e-02,  8.7057e-02,  1.3003e-01,  ..., -3.8008e-02,\n",
      "         -1.6053e-02, -1.6071e-01],\n",
      "        [ 2.1642e-02, -1.8580e-01, -5.8628e-03,  ...,  4.4221e-02,\n",
      "          1.8533e-01, -1.4120e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True)]]\n"
     ]
    }
   ],
   "source": [
    "print(model.lstm1.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5758],\n",
      "         [-0.1818],\n",
      "         [-0.1515],\n",
      "         [-0.1212],\n",
      "         [-0.0909],\n",
      "         [-0.0606],\n",
      "         [-0.0303],\n",
      "         [-0.2727],\n",
      "         [ 0.0000],\n",
      "         [ 0.0303],\n",
      "         [ 0.0606],\n",
      "         [ 0.0909],\n",
      "         [ 0.1212],\n",
      "         [-0.5758],\n",
      "         [ 0.1515],\n",
      "         [ 0.1818],\n",
      "         [ 0.2121],\n",
      "         [ 0.2424],\n",
      "         [ 0.2727],\n",
      "         [ 0.3030],\n",
      "         [ 0.3333],\n",
      "         [ 0.3636],\n",
      "         [-0.1515],\n",
      "         [ 0.3939],\n",
      "         [-0.5758],\n",
      "         [ 0.4242],\n",
      "         [ 0.4545],\n",
      "         [ 0.4848]],\n",
      "\n",
      "        [[-0.5455],\n",
      "         [-0.5152],\n",
      "         [-0.4848],\n",
      "         [-0.5455],\n",
      "         [-0.4545],\n",
      "         [-0.6364],\n",
      "         [-0.4242],\n",
      "         [-0.3939],\n",
      "         [-0.3636],\n",
      "         [-0.5455],\n",
      "         [-0.3333],\n",
      "         [-0.3030],\n",
      "         [-0.2727],\n",
      "         [-0.2424],\n",
      "         [-0.2121],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset_creator.detokenize(labels[0], dataset_creator.japanese_dictionary)\n",
    "\n",
    "output = output.detach()\n",
    "predicted = dataset_creator.detokenize(output[0], dataset_creator.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('私 達 は a i は た だ の 数 学 の 集 ま り だ と あ な た は 言 い ま す が で も 人 間 の 脳 が ど の よ う に 機 能 す る か を 正 確 に 知 っ た ら そ れ は あ な た の 生 活 を 小 物 で す か に に に に に に に に', ['私', '達', 'は', 'a', 'i', 'は', 'た', 'だ', 'の', '数', '学', 'の', '集', 'ま', 'り', 'だ', 'と', 'あ', 'な', 'た', 'は', '言', 'い', 'ま', 'す', 'が', 'で', 'も', '人', '間', 'の', '脳', 'が', 'ど', 'の', 'よ', 'う', 'に', '機', '能', 'す', 'る', 'か', 'を', '正', '確', 'に', '知', 'っ', 'た', 'ら', 'そ', 'れ', 'は', 'あ', 'な', 'た', 'の', '生', '活', 'を', '小', '物', 'で', 'す', 'か', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に'])\n",
      "('に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に に', ['に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に', 'に'])\n"
     ]
    }
   ],
   "source": [
    "print(label)\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
