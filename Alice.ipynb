{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from librosa import display\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2Speech(object):\n",
    "    def __init__(self, audio_path, text_path, n_mels=80, n_features=1024, hop_length=256):\n",
    "\n",
    "        self.audio_path = audio_path\n",
    "        self.text_path = text_path\n",
    "        self.n_features = n_features\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        self.phrases = self._get_phrases()\n",
    "\n",
    "        self.text_length = self._get_text_length()\n",
    "\n",
    "        self.words = self._get_words(self.phrases)\n",
    "        self.letters = self._get_letters(self.words)\n",
    "\n",
    "        self.dictionary = self._create_dictionary(self.letters)\n",
    "        self._normalize(self.dictionary)\n",
    "\n",
    "        self.tokens = self._encode_letters()\n",
    "\n",
    "        self.text_data = None\n",
    "        self.sample_rate = None\n",
    "        self.audio_length = None\n",
    "        self.audio_data = self._create_audio_data()\n",
    "        self.spectrograms = self._create_spectrograms()\n",
    "        \n",
    "        \n",
    "    def create_data(self):\n",
    "        text_data = torch.from_numpy(self.tokens)\n",
    "\n",
    "        text_data = text_data.unsqueeze(-1)\n",
    "\n",
    "        self.text_data = text_data\n",
    "\n",
    "        print(f\"Text Data Size: {self.text_data.size()}\")\n",
    "\n",
    "    def detokenize(self, data, reference_dict):\n",
    "        data = data.cpu().numpy()\n",
    "        values = list(reference_dict.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = KNN(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        _, index = knn.kneighbors(data)\n",
    "\n",
    "        keys = list(reference_dict.keys())\n",
    "\n",
    "        words = []\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                words.append(keys[i])\n",
    "\n",
    "        words = [item for item in words if item != \"<EOS>\"]\n",
    "        \n",
    "        phrase = ''.join(words)\n",
    "\n",
    "        return phrase, words\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        audio = self.audio_data[idx]\n",
    "        spectrograms = self.spectrograms[idx]\n",
    "        text = self.text_data[idx]\n",
    "\n",
    "        return audio, spectrograms, text\n",
    "\n",
    "\n",
    "    def _create_audio_data(self):\n",
    "        audio_files = []\n",
    "\n",
    "        for directory, _, files in os.walk(self.audio_path):\n",
    "            for file in files:\n",
    "                audio_files.append(directory+'/'+file)\n",
    "\n",
    "        audio_files = [i for i in audio_files if '.wav' in i]\n",
    "\n",
    "        audio_data_preprocessed = []\n",
    "\n",
    "        for audio in audio_files:\n",
    "            data, sr = torchaudio.load(audio)\n",
    "            audio_data_preprocessed.append(data.numpy())\n",
    "\n",
    "        length = [x.shape[1] for x in audio_data_preprocessed]\n",
    "        self.audio_length = max(length)\n",
    "        self.sample_rate = sr\n",
    "\n",
    "        audio_data = []\n",
    "        \n",
    "        for i in range(len(audio_data_preprocessed)):\n",
    "            audio = audio_data_preprocessed[i]\n",
    "            audio_size = audio.shape[1]\n",
    "            \n",
    "            if audio_size < self.audio_length:\n",
    "                pad_size = self.audio_length - audio_size\n",
    "                audio = np.pad(audio, [(0, 0), (0, pad_size)])\n",
    "\n",
    "            audio_data.append(audio)\n",
    "\n",
    "        audio_data = np.array(audio_data)\n",
    "        audio_data = np.stack(audio_data, 0)\n",
    "        audio_data = torch.from_numpy(audio_data)\n",
    "\n",
    "        print(f\"Audio dataset size: {audio_data.size()}\")\n",
    "\n",
    "        return audio_data\n",
    "\n",
    "    def _create_spectrograms(self):\n",
    "\n",
    "        transformer = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_mels=self.n_mels,\n",
    "            n_fft=self.n_features,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.n_features\n",
    "        )\n",
    "\n",
    "        audio_dataset = []\n",
    "\n",
    "        for i in range(len(self.audio_data)):\n",
    "            data = transformer(self.audio_data[i])\n",
    "            audio_dataset.append(data.numpy())\n",
    "\n",
    "        audio_dataset = np.array(audio_dataset) # Using Numpy because Pytorch's stack is rubbish\n",
    "        audio_dataset = np.stack(audio_dataset, 0)\n",
    "\n",
    "        audio_dataset = torch.from_numpy(audio_dataset)\n",
    "\n",
    "        # The LSTM expects to receive input with shapes (Batch, Timesteps, Features), so we have to transpose our features and remove the Channels size\n",
    "\n",
    "        # HOWEVER....we should consider using MultiAttention blocks\n",
    "\n",
    "        audio_dataset = audio_dataset.view(audio_dataset.size(0), audio_dataset.size(3), audio_dataset.size(2))\n",
    "\n",
    "        return audio_dataset\n",
    "\n",
    "    def _get_phrases(self):\n",
    "        script_files = []\n",
    "\n",
    "        for directory, _, files in os.walk(self.text_path):\n",
    "            for file in files:\n",
    "                script_files.append(directory+'/'+file)\n",
    "\n",
    "        script_files = [i for i in script_files if '.txt' in i]\n",
    "\n",
    "        text_data_preprocessed = []\n",
    "\n",
    "        for item in script_files:\n",
    "            file = open(item, 'r')\n",
    "            for i in file:\n",
    "                text_data_preprocessed.append(str(i))\n",
    "    \n",
    "            file.close()\n",
    "\n",
    "        text_data_preprocessed = [x.lower() for x in text_data_preprocessed]\n",
    "        phrases = [re.sub('[^a-z0-9\\s\\.\\!\\,\\?]', '', x) for x in text_data_preprocessed]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_letters(self, words):\n",
    "        letters = ' '.join(words)\n",
    "        letters = ''.join(letters.split())\n",
    "        letters = [i for i in letters]\n",
    "\n",
    "        return letters\n",
    "\n",
    "    def _create_dictionary(self, words):\n",
    "        idx2word = []\n",
    "        word2idx = {}\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                idx2word.append(word)\n",
    "                word2idx[word] = len(idx2word) - 1\n",
    "        \n",
    "        word2idx[' '] = len(idx2word)\n",
    "        word2idx['.'] = len(idx2word) + 1\n",
    "        word2idx[' ! '] = len(idx2word) + 2\n",
    "        word2idx[', '] = len(idx2word) + 3\n",
    "        word2idx['?'] = len(idx2word) + 4\n",
    "\n",
    "        word2idx['<EOS>'] = len(idx2word) + 5\n",
    "\n",
    "        return word2idx\n",
    "\n",
    "    def _normalize(self, dictionary):\n",
    "        maximum = max(dictionary.values())\n",
    "\n",
    "        for word, value in dictionary.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            dictionary[word] = scaled_value\n",
    "\n",
    "    def _get_text_length(self):\n",
    "\n",
    "        maximum_length = 0\n",
    "        \n",
    "        for sentence in self.phrases:\n",
    "            word_length = 0\n",
    "\n",
    "            for character in sentence:\n",
    "                word_length += len(character)\n",
    "\n",
    "            sentence_length = word_length\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "    \n",
    "    def _encode_letters(self):\n",
    "        \n",
    "        phrases = [x.split() for x in self.phrases]\n",
    "        \n",
    "\n",
    "        tokens = []\n",
    "\n",
    "\n",
    "        for sentence in phrases:\n",
    "            tokenized_sentence = []\n",
    "\n",
    "            for word in sentence:\n",
    "                tokenized_word = []\n",
    "\n",
    "                for character in word:\n",
    "\n",
    "                    value = self.dictionary.get(character)\n",
    "\n",
    "                    tokenized_sentence.append(value)\n",
    "                \n",
    "                value = self.dictionary.get(' ')\n",
    "                tokenized_sentence.append(value) # After each word, add a space.\n",
    "            \n",
    "            tokenized_sentence = np.array(tokenized_sentence)\n",
    "\n",
    "            sentence_size = tokenized_sentence.shape[0]\n",
    "\n",
    "            if sentence_size < self.text_length+1:\n",
    "                pad_size = self.text_length+1 - sentence_size\n",
    "                tokenized_sentence = np.pad(tokenized_sentence, [(0, pad_size)], constant_values=1.0)\n",
    "\n",
    "            tokens.append(tokenized_sentence)\n",
    "        \n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"D:/Python/Audio/English/\"\n",
    "TEXT_PATH = \"D:/Python/Audio/Script/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio dataset size: torch.Size([149, 1, 341087])\n"
     ]
    }
   ],
   "source": [
    "dataset = Text2Speech(AUDIO_PATH, TEXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -3.0518e-05,  6.1035e-05,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.audio_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 1333, 80])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.spectrograms.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341087\n"
     ]
    }
   ],
   "source": [
    "print(dataset.audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "print(dataset.text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data Size: torch.Size([149, 77, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset.create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': -1.0, 'o': -0.9411764705882353, ',': -0.8823529411764706, 't': -0.8235294117647058, 'h': -0.7647058823529411, 'i': -0.7058823529411764, 'c': -0.6470588235294117, 'a': -0.5882352941176471, 'm': -0.5294117647058824, 'e': -0.47058823529411764, 'f': -0.4117647058823529, 'r': -0.3529411764705882, 'v': -0.2941176470588235, 'n': -0.23529411764705888, '.': 0.7647058823529411, 'l': -0.11764705882352944, 'w': -0.05882352941176472, 'd': 0.0, 'p': 0.05882352941176472, '?': 0.9411764705882353, 'k': 0.17647058823529416, '!': 0.23529411764705888, 'j': 0.2941176470588236, 'y': 0.3529411764705883, 'u': 0.41176470588235303, 'g': 0.47058823529411775, 'b': 0.5294117647058822, 'z': 0.588235294117647, 'q': 0.6470588235294117, ' ': 0.7058823529411764, ' ! ': 0.8235294117647058, ', ': 0.8823529411764706, '<EOS>': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.94117647 -0.88235294  0.70588235 -0.82352941 -0.76470588\n",
      " -0.70588235 -1.          0.70588235 -0.64705882 -0.58823529 -0.52941176\n",
      " -0.47058824  0.70588235 -0.41176471 -0.35294118 -0.94117647 -0.52941176\n",
      "  0.70588235 -0.82352941 -0.76470588 -0.47058824  0.70588235 -0.76470588\n",
      " -0.47058824 -0.58823529 -0.29411765 -0.47058824 -0.23529412 -1.\n",
      "  0.76470588  0.76470588  0.76470588  0.70588235  1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.text_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so, this came from the heavens... \n"
     ]
    }
   ],
   "source": [
    "teste, teste2 = dataset.detokenize(dataset.text_data[0], dataset.dictionary)\n",
    "print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to create the model\n",
    "# Remember: The idea is not using LSTMs. We can use them for starters, but that shouldn't be our final model\n",
    "# Our input should be text. In order to replace LSTMs, we could use MultiAttentionHeads from Transformer\n",
    "# Our output should be audio. WaveGlow inserts Spectrograms directly into the final layer in order to condition the output\n",
    "# The output have shape (1, data), so it can be converted to a waveform\n",
    "\n",
    "\n",
    "# In order to generate words spontaneously, we'll probably need a Text GAN\n",
    "# For that, we could use LSTMs, or, Attention Layers\n",
    "# We could use a transformer(or even BERT itself) to generate words and we could also use a discriminator.\n",
    "# This discriminator will distinguish between fake and true texts.\n",
    "# How can this discriminator distinguish between nonsense and (with)sense texts? ----> Bleu score\n",
    "# https://aclanthology.org/P02-1040.pdf\n",
    "# The Bleu score takes an input text and a reference text. The input text will the the one generated by the GAN. The reference, our original data.\n",
    "# After generating the text, we pass it to the voice generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuScore:\n",
    "    \"\"\"\n",
    "    Using a separate class because Pytorch's autograd is rubbish\n",
    "\n",
    "    BleuScore Function adapted to be used in backpropagation. Adapted from https://stackoverflow.com/questions/56968434/bleu-score-in-python-from-scratch\n",
    "\n",
    "\n",
    "    The input tensors are converted to strings so the n-grams can be extracted and the Bleu Score measured.\n",
    "\n",
    "    Args:\n",
    "        candidate_sentence: Tensor of shape (sentence_length, ) containing the values which corresponds to each token.\n",
    "        target_sentence: Tensor of shape (length, ) which will be used as reference.\n",
    "        max_n: The maximum n-gram we'll use. E.g. if max_n=3, we'll use unigrams, bigrams and trigrams.\n",
    "            For compatibility reasons, the max_n must be equal to or smaller than sentence_length\n",
    "            Default = 4\n",
    "        weights: a list of weights to be used for each n_gram category (uniform by default).\n",
    "        dictionary: a dictionary to be used for translating the tensors into strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_n=4, weights=[0.25]*4, dictionary=dataset.dictionary):\n",
    "\n",
    "        self.max_n = max_n\n",
    "        self.weights = weights\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "\n",
    "    def _n_gram_generator(self, sentence, n=2,remove_repeating=False):\n",
    "\n",
    "        sentence = sentence.lower() # converting to lower case\n",
    "        sent_arr = np.array(sentence.split()) # split to string arrays\n",
    "        length = len(sent_arr)\n",
    "\n",
    "        word_list = []\n",
    "        for i in range(length+1):\n",
    "            if i < n:\n",
    "                continue\n",
    "            word_range = list(range(i-n,i))\n",
    "            s_list = sent_arr[word_range]\n",
    "            string = ' '.join(s_list) # converting list to strings\n",
    "            word_list.append(string) # append to word_list\n",
    "            if remove_repeating:\n",
    "                word_list = list(set(word_list))\n",
    "        return word_list\n",
    "\n",
    "    \n",
    "    def bleu_score(self, original,machine_translated):\n",
    "        '''\n",
    "        Bleu score function given an original and a machine translated sentences\n",
    "        '''\n",
    "\n",
    "        original, _ = dataset.detokenize(original, self.dictionary)\n",
    "\n",
    "        machine_translated, _ = dataset.detokenize(machine_translated, self.dictionary)\n",
    "\n",
    "\n",
    "        mt_length = len(machine_translated.split())\n",
    "        o_length = len(original.split())\n",
    "\n",
    "        # Brevity Penalty \n",
    "        if mt_length>o_length:\n",
    "            BP=1\n",
    "        else:\n",
    "            penality=1-(mt_length/o_length)\n",
    "            BP=np.exp(penality)\n",
    "\n",
    "        # Clipped precision\n",
    "        clipped_precision_score = []\n",
    "\n",
    "        for i in range(1, self.max_n+1):\n",
    "            original_n_gram = Counter(self._n_gram_generator(original,i))\n",
    "            machine_n_gram = Counter(self._n_gram_generator(machine_translated,i))\n",
    "\n",
    "            counter = len(machine_n_gram.values())\n",
    "\n",
    "            if counter == 0:\n",
    "                counter += 1\n",
    "            \n",
    "            for j in machine_n_gram:\n",
    "                if j in original_n_gram:\n",
    "                    if machine_n_gram[j] > original_n_gram[j]:\n",
    "                        machine_n_gram[j] = original_n_gram[j]\n",
    "                else:\n",
    "                    machine_n_gram[j] = 0\n",
    "\n",
    "            clipped_precision = sum(machine_n_gram.values())/counter\n",
    "            clipped_precision_score.append(clipped_precision)\n",
    "\n",
    "        weights = self.weights\n",
    "\n",
    "        scores = []\n",
    "        dscores = []\n",
    "\n",
    "        for weight, precision in zip(weights, clipped_precision_score):\n",
    "            w_i = weight\n",
    "            p_i = precision\n",
    "\n",
    "            if p_i == 0.0:\n",
    "                score = (w_i * math.log(0.0001)) # Log of 0 tends to -infinite. But let's just stick to 0.0001 so a single wrong word won't break the score.\n",
    "                                                # Since log of 0.0001 = -9.21, which is also a quite big negative number(?)\n",
    "                dscore = (w_i/math.log(0.0001)) # Again, a division by -infinite would result in 0, which would also break the derivative.\n",
    "\n",
    "            else:\n",
    "                score = (w_i * math.log(p_i))\n",
    "\n",
    "                if p_i != 1.0:\n",
    "                    dscore = (w_i/math.log(p_i))\n",
    "                else:\n",
    "                    dscore = 0\n",
    "        \n",
    "            scores.append(score)\n",
    "            dscores.append(dscore)\n",
    "\n",
    "        score = BP * math.exp(math.fsum(scores))\n",
    "\n",
    "        gradients = []\n",
    "\n",
    "        for i in dscores:\n",
    "            derivative = BP * math.exp(math.fsum(scores)) * len(scores) * i\n",
    "\n",
    "            # When the candidate sentence is too small, the BP causes the derivative to be too small. The result is, we can get\n",
    "            # a derivative that is bigger for small yet completely wrong sentences than the derivative for a bigger yet almost correct sentence\n",
    "            # i.e: reference: \"so, this is how the sea smells...\"\n",
    "            # candidate 1: \"so, this is how the sea shells...\" ---> derivative = -2.78\n",
    "            # candidate 2: \"panda\" ---> derivative = -9e-5\n",
    "            # So we're gonna use a correction factor to avoid this, which is simply applying a log 10\n",
    "\n",
    "            if derivative != 0.0:\n",
    "\n",
    "                correction_factor = math.log10(abs(derivative))\n",
    "\n",
    "                if correction_factor > 0:\n",
    "                    correction_factor = -correction_factor\n",
    "\n",
    "                derivative = correction_factor\n",
    "\n",
    "            gradients.append(derivative)\n",
    "\n",
    "        return score, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuScoreLoss(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    The BleuScore adapted to be a Loss Function in Pytorch.\n",
    "    Bear in mind that the gradients output in the backward method must have the same shape as the input in the forward method\n",
    "    (the function input).\n",
    "    For this motive, the number of n-grams is the same as the sentence_length in the input.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        input: The candidate sentence, being an tensor with size (sentence_length, 1)\n",
    "        target: The reference sentence, a tensor with size (sentence_length', 1). Can have a different length than the input.\n",
    "        weights: the weights to be applied for each n-gram.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target, weights=[0.25]):\n",
    "\n",
    "        max_n = input.size(0)\n",
    "        weights = weights * max_n\n",
    "\n",
    "        functional = BleuScore(max_n, weights)\n",
    "\n",
    "        score, derivative = functional.bleu_score(target, input)\n",
    "\n",
    "        score, derivative = torch.tensor(score, device=device), torch.tensor(derivative, device=device)\n",
    "\n",
    "        ctx.save_for_backward(derivative)\n",
    "\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        derivative, = ctx.saved_tensors\n",
    "        derivative = derivative.unsqueeze(-1)\n",
    "\n",
    "        output = grad_output*derivative\n",
    "        output = output.to(device) # Just to make sure. Gradients returned in a different device will throw an error.\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, dropout, in_decoder=False):\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_keys = d_values # size of key vectors, same as of the query vectors to allow dot-products for similarity\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.in_decoder = in_decoder\n",
    "\n",
    "        self.create_queries = nn.Linear(d_model, n_heads*d_queries, bias=False)\n",
    "        self.create_values = nn.Linear(d_model, n_heads*d_values, bias=False)\n",
    "        self.create_keys = nn.Linear(d_model, n_heads*d_values, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.cast_output = nn.Linear(n_heads*d_values, d_model, bias=False)\n",
    "\n",
    "    def forward(self, query_sequences, key_sequences, value_sequences):\n",
    "\n",
    "        batch_size = query_sequences.size(0)\n",
    "        query_sequences_length = query_sequences.size(1)\n",
    "\n",
    "        self_attention = torch.equal(key_sequences, query_sequences)\n",
    "\n",
    "        residual = query_sequences\n",
    "\n",
    "        query_sequences = self.layer_norm(query_sequences)\n",
    "\n",
    "        if self_attention:\n",
    "            key_sequences = self.layer_norm(key_sequences)\n",
    "            value_sequences = self.layer_norm(value_sequences)\n",
    "\n",
    "        queries = self.create_queries(query_sequences)\n",
    "        keys = self.create_keys(key_sequences)\n",
    "        values = self.create_values(value_sequences)\n",
    "\n",
    "        queries = queries.contiguous().view(batch_size, query_sequences_length, self.n_heads, self.d_queries)\n",
    "        queries = queries.permute(0, 2, 1, 3).contiguous().view(batch_size*self.n_heads, query_sequences_length, self.d_queries)\n",
    "\n",
    "        keys = keys.contiguous().view(batch_size, query_sequences_length, self.n_heads, self.d_keys)\n",
    "        keys = keys.permute(0, 2, 1, 3).contiguous().view(batch_size*self.n_heads, query_sequences_length, self.d_keys)\n",
    "\n",
    "        values = values.contiguous().view(batch_size, query_sequences_length, self.n_heads, self.d_values)\n",
    "        values = values.permute(0, 2, 1, 3).contiguous().view(batch_size*self.n_heads, query_sequences_length, self.d_values)\n",
    "\n",
    "\n",
    "        dotproduct = torch.bmm(queries, keys.permute(0, 2, 1))\n",
    "\n",
    "        dotproduct = dotproduct/(math.sqrt(self.d_keys))\n",
    "\n",
    "\n",
    "        if self.in_decoder and self_attention:\n",
    "            not_future_mask = torch.ones_like(dotproduct).tril().bool().to(device)\n",
    "\n",
    "            attention_weights = dotproduct.masked_fill(~not_future_mask, -float('inf'))\n",
    "\n",
    "        attention_weights = self.softmax(dotproduct)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        sequences = torch.bmm(attention_weights, values)\n",
    "\n",
    "        sequences = sequences.view(batch_size, query_sequences_length, -1)\n",
    "\n",
    "        sequences = self.cast_output(sequences)\n",
    "\n",
    "        sequences = self.dropout(sequences)\n",
    "\n",
    "        output = sequences + residual\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_inner, dropout):\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.neuron1 = nn.Linear(d_model, d_inner)\n",
    "\n",
    "        self.Relu = nn.ReLU()\n",
    "\n",
    "        self.neuron2 = nn.Linear(d_inner, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, sequences):\n",
    "\n",
    "        residual = sequences\n",
    "\n",
    "        sequences = self.layer_norm(sequences)\n",
    "\n",
    "        sequences = self.neuron1(sequences)\n",
    "        sequences = self.Relu(sequences)\n",
    "        sequences = self.dropout(sequences)\n",
    "\n",
    "        sequences = self.neuron2(sequences)\n",
    "        sequences = self.dropout(sequences)\n",
    "\n",
    "        output = sequences + residual\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: (1, 77, 1) = (Batch, n_timesteps, n_features[character]).\n",
    "    Our data here is composed of (Batch, n_characters, character), and each batch is equivalent to an entire sentence.\n",
    "\n",
    "    Encoder output: (1, 77, positional_encoding) ----> Encoding, just like one-hot encoding, label-encoding...\n",
    "    Perhaps this is why BERT, using just the Encoder, can be used as a classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positional_encoding, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.positional_encoding.requires_grad = False\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([self.make_encoder_layer() for i in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def make_encoder_layer(self):\n",
    "        encoder_layer = nn.ModuleList([MultiHeadAttention(d_model=self.d_model,\n",
    "                                                          n_heads=self.n_heads,\n",
    "                                                          d_queries=self.d_queries,\n",
    "                                                          d_values=self.d_values,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          in_decoder=False),\n",
    "                                       PositionWiseFeedForward(d_model=self.d_model,\n",
    "                                                             d_inner=self.d_inner,\n",
    "                                                             dropout=self.dropout)])\n",
    "\n",
    "        return encoder_layer\n",
    "\n",
    "    def forward(self, encoder_sequences):\n",
    "\n",
    "        encoder_sequences = encoder_sequences * math.sqrt(self.d_model) + self.positional_encoding.to(device)\n",
    "\n",
    "        encoder_sequences = self.dropout(encoder_sequences)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "\n",
    "            encoder_sequences = layer[0](query_sequences=encoder_sequences, key_sequences=encoder_sequences, value_sequences=encoder_sequences)\n",
    "            \n",
    "            encoder_sequences = layer[1](sequences=encoder_sequences)\n",
    "\n",
    "        encoder_sequences = self.layer_norm(encoder_sequences)\n",
    "\n",
    "        return encoder_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder input: Encoder output: (1, 77, positional_encoding)\n",
    "    Decoder output: (1, 77, 1) = (Batch, n_timesteps, n_features[character])\n",
    "\n",
    "    This is what will generate our sentence. Since I won't be using word embedding nor integer-encoding, this structure is essential, simulating the output\n",
    "    of a LSTM...but with less risk of vanishing gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positional_encoding, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.positional_encoding.requires_grad = False\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([self.make_decoder_layer() for i in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.neuron = nn.Linear(d_model, 1)\n",
    "\n",
    "    def make_decoder_layer(self):\n",
    "\n",
    "        decoder_layer = nn.ModuleList([MultiHeadAttention(d_model=self.d_model,\n",
    "                                                          n_heads=self.n_heads,\n",
    "                                                          d_queries=self.d_queries,\n",
    "                                                          d_values=self.d_values,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          in_decoder=True),\n",
    "                                       MultiHeadAttention(d_model=self.d_model,\n",
    "                                                          n_heads=self.n_heads,\n",
    "                                                          d_queries=self.d_queries,\n",
    "                                                          d_values=self.d_values,\n",
    "                                                          dropout=self.dropout,\n",
    "                                                          in_decoder=True),\n",
    "                                       PositionWiseFeedForward(d_model=self.d_model,\n",
    "                                                             d_inner=self.d_inner,\n",
    "                                                             dropout=self.dropout)])\n",
    "\n",
    "        return decoder_layer\n",
    "\n",
    "    def forward(self, decoder_sequences, encoder_sequences):\n",
    "\n",
    "        decoder_sequences = decoder_sequences * math.sqrt(self.d_model) + self.positional_encoding.to(device)\n",
    "\n",
    "        decoder_sequences = self.dropout(decoder_sequences)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "\n",
    "            decoder_sequences = layer[0](query_sequences=decoder_sequences, key_sequences=decoder_sequences, value_sequences=decoder_sequences)\n",
    "\n",
    "            decoder_sequences = layer[1](query_sequences=decoder_sequences, key_sequences=encoder_sequences, value_sequences=encoder_sequences)\n",
    "\n",
    "            decoder_sequences = layer[2](sequences=decoder_sequences)\n",
    "\n",
    "        decoder_sequences = self.layer_norm(decoder_sequences)\n",
    "\n",
    "        output = self.neuron(decoder_sequences)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, positional_encoding, d_model=512, n_heads=8, d_queries=64, d_values=64, d_inner=2056, n_layers=6, dropout=0.1):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = Encoder(positional_encoding=positional_encoding,\n",
    "                               d_model=d_model,\n",
    "                               n_heads=n_heads,\n",
    "                               d_queries=d_queries,\n",
    "                               d_values=d_values,\n",
    "                               d_inner=d_inner,\n",
    "                               n_layers=n_layers,\n",
    "                               dropout=self.dropout)\n",
    "\n",
    "        self.decoder = Decoder(positional_encoding=positional_encoding,\n",
    "                               d_model=d_model,\n",
    "                               n_heads=n_heads,\n",
    "                               d_queries=d_queries,\n",
    "                               d_values=d_values,\n",
    "                               d_inner=d_inner,\n",
    "                               n_layers=n_layers,\n",
    "                               dropout=self.dropout)\n",
    "                               \n",
    "    def forward(self, encoder_sequences, decoder_sequences):\n",
    "\n",
    "        encoder_sequences = self.encoder(encoder_sequences)\n",
    "\n",
    "        decoder_sequences = self.decoder(decoder_sequences, encoder_sequences)\n",
    "\n",
    "        return decoder_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model, max_length=100):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as defined in the paper.\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param max_length: maximum sequence length up to which positional encodings must be calculated\n",
    "    :return: positional encoding, a tensor of size (1, max_length, d_model)\n",
    "    \"\"\"\n",
    "    positional_encoding = torch.zeros((max_length, d_model))  # (max_length, d_model)\n",
    "    for i in range(max_length):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                positional_encoding[i, j] = math.sin(i / math.pow(10000, j / d_model))\n",
    "            else:\n",
    "                positional_encoding[i, j] = math.cos(i / math.pow(10000, (j - 1) / d_model))\n",
    "\n",
    "    positional_encoding = positional_encoding.unsqueeze(0)  # (1, max_length, d_model)\n",
    "\n",
    "    return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 77, 16])\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = get_positional_encoding(d_model=16, max_length=77)\n",
    "\n",
    "print(positional_encoding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Transformer input is (batch_size, n_timesteps, n_features), just like a LSTM. Remember that each batch will contain a time_step, which will contain a single\n",
    "# character. So a single batch is a sequence of time_steps, which will compose a sentence.\n",
    "\n",
    "# Would it be interesting to generate random, gaussian noise with that shape and directly pass it through the transformer?\n",
    "# Or would it be better if we generate a random noise, pass it through some convolution layers and reshape the output into (batch_size, n_timesteps, n_features)\n",
    "# only for, then, pass it into the transformer?\n",
    "\n",
    "# If we decide to use convolutions...would it be better if we use a Conv1D with noise inputs (batch_size, 500, 100) and output (batch, n_timesteps, 1), or would it\n",
    "# be better if we simply use neurons, with noise input (batch_size, 500*100)?\n",
    "\n",
    "# https://stackoverflow.com/questions/55576314/conv1d-with-kernel-size-1-vs-linear-layer\n",
    "# Conv1Ds involves more operations, thus is considerably slower and more computationally expensive. It's slightly more accurated, but it doesn't seem to be really worth it.\n",
    "# Sticking to Linear layers.\n",
    "\n",
    "# Remember: Our labels have shape (Batch, 77, 1). This must be our output in the end.\n",
    "\n",
    "# The generator can be optimized by using our BleuScore Loss. This could, perhaps, make the discriminator disposable. Or, we could combine the 2 things?\n",
    "\n",
    "# The discriminator. It'll receive a sentence as input. If each time_step correspond to a single character, then a data composed of complete sentences will have no time_step.\n",
    "# Thus, the discriminator input will have shape (batch_size, 1). Then, it'll proceed to classify the sentence between real or false. The classic thing.\n",
    "# BERT can use the Transformer's Encoder to classify sentences, too. However, this would require creating sentences sequences. Let's try the classic one first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broca(nn.Module):\n",
    "    \"\"\"\n",
    "    The generator, which will generate the words that she'll speak.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positional_encoding, d_transformer=512, n_heads=8, d_queries=64, d_values=64, d_inner=2056, n_layers=6, dropout=0.1):\n",
    "\n",
    "        super(Broca, self).__init__()\n",
    "\n",
    "        self.sequence_length = positional_encoding.size(1)\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.d_transformer = d_transformer\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        self.neurons1 = nn.Linear(500*100, self.sequence_length, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            positional_encoding=self.positional_encoding,\n",
    "            d_model=self.d_transformer,\n",
    "            n_heads=self.n_heads,\n",
    "            d_queries=self.d_queries,\n",
    "            d_values=self.d_values,\n",
    "            d_inner=self.d_inner,\n",
    "            n_layers=self.n_layers,\n",
    "            dropout=self.dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input, target):\n",
    "\n",
    "        input = input.view(input.size(0), -1)\n",
    "\n",
    "        x = self.neurons1(input)\n",
    "        x = self.tanh(x)\n",
    "\n",
    "        x = x.view(input.size(0), self.sequence_length, -1)\n",
    "\n",
    "        output = self.transformer(x, target)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wernicke(nn.Module):\n",
    "    \"\"\"\n",
    "    The discriminator, which will receive the words that she must learn, and also check if the words\n",
    "    she's speaking are close to what we speak or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Wernicke, self).__init__()\n",
    "\n",
    "        self.neuron1 = nn.Linear(77, 100, bias=False)\n",
    "        self.neuron2 = nn.Linear(100, 100, bias=False)\n",
    "        self.neuron3 = nn.Linear(100, 1, bias=False)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(100)\n",
    "\n",
    "        #self.dropout = nn.Dropout(0.25)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        input = input.view(input.size(0), -1)\n",
    "\n",
    "        x = self.neuron1(input)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.ReLU(x)\n",
    "\n",
    "        x = self.neuron2(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.ReLU(x)\n",
    "\n",
    "        output = self.neuron3(x)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "costsD = []\n",
    "costsG = []\n",
    "Glearning_rate = []\n",
    "Dlearning_rate = []\n",
    "Dgrads = [] # To check if there's vanishing/exploding gradients\n",
    "Ggrads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_TextGAN(\n",
    "    dataset=None,\n",
    "    batch_size=16,\n",
    "    epochs=1000,\n",
    "    checkpoint=100,\n",
    "    lr=1e-5,\n",
    "    beta=1e-3,\n",
    "    grad_clip=None,\n",
    "    save_path=None,\n",
    "    d_transformer=512,\n",
    "    n_heads=8,\n",
    "    d_queries=64,\n",
    "    d_values=64,\n",
    "    d_inner=2056,\n",
    "    n_layers=6,\n",
    "    dropout=0.1\n",
    "):\n",
    "\n",
    "    Speak = Broca(positional_encoding, d_transformer, n_heads, d_queries, d_values, d_inner, n_layers, dropout).to(device).double()\n",
    "\n",
    "    Recognize = Wernicke().to(device).double()\n",
    "\n",
    "    if save_path is None:\n",
    "\n",
    "        start_epoch = 0\n",
    "\n",
    "        for name, param in Speak.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param, 0, 0.01)\n",
    "            \n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "        for name, param in Speak.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param, 0, 0.01)\n",
    "            \n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        params = torch.load(f\"{save_path}/Alice_checkpoint.json\")\n",
    "\n",
    "        start_epoch = params['Epoch'] + 1\n",
    "        Speak.load_state_dict(params['Generator_Params'])\n",
    "        Recognize.load_state_dict(params['Discriminator_Params'])\n",
    "        lr = params['Discriminator_LR']\n",
    "\n",
    "        print(f\"\\nLoaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "    Goptim = torch.optim.AdamW(Speak.parameters(), lr=lr*5e5, betas=(0.9, 0.999), eps=1e-9, weight_decay=0)\n",
    "    Gscheduler = torch.optim.lr_scheduler.StepLR(Goptim, 1, gamma=0.1)\n",
    "\n",
    "    Doptim = torch.optim.AdamW(Recognize.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-9, weight_decay=0)\n",
    "    Dscheduler = torch.optim.lr_scheduler.StepLR(Doptim, 10, gamma=0.1)\n",
    "\n",
    "    # In SRGAN, content_loss + (beta * adversarial_loss) = perceptual_loss ---> the adversarial loss updates the discriminator, and the perceptual, the generator\n",
    "\n",
    "    adversarial_loss = nn.BCEWithLogitsLoss().double()\n",
    "    content_loss = BleuScoreLoss()\n",
    "    beta = beta\n",
    "\n",
    "\n",
    "    # Instead of assigning 1 for real label and 0 for fake, assigning numbers that are close to 1 and 0 ---> label-smoothing.\n",
    "\n",
    "    real_label = 0.9\n",
    "    fake_label = 0.\n",
    "\n",
    "    Speak.train()\n",
    "    Recognize.train()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, (_, _, text) in enumerate(dataloader):\n",
    "\n",
    "            text = text.to(device)\n",
    "\n",
    "            # text shape = (batch, 77, 1)\n",
    "\n",
    "            random_noise = torch.normal(0, 0.05, size=text.size(), device=device, dtype=torch.double)\n",
    "\n",
    "            text = text + random_noise # Adding random noise to both real and fake input in order to avoid collapse and facilitate convergence\n",
    "\n",
    "            Recognize.zero_grad()\n",
    "\n",
    "            labels = torch.full((text.size(0),), real_label, dtype=torch.double, device=device) # Shape = (batch)\n",
    "\n",
    "            Doutput = Recognize(text) # Doutput shape = (batch, 1)\n",
    "\n",
    "            Doutput = Doutput.view(-1) # Doutput shape = (batch)\n",
    "\n",
    "            errorD_real = adversarial_loss(Doutput, labels)\n",
    "\n",
    "            errorD_real.backward()\n",
    "\n",
    "            noise = torch.normal(0, 1000, size=(text.size(0), 500*100, 1), device=device, dtype=torch.double) # Maybe using a higher std might avoid Gen's vanishing gradients\n",
    "            \n",
    "            fake_phrases = Speak(noise, text) # Phrases shape = (batch, 77, 1)\n",
    "\n",
    "            labels = labels.fill_(fake_label)\n",
    "\n",
    "            fake_phrases = fake_phrases + random_noise\n",
    "\n",
    "            Doutput = Recognize(fake_phrases.detach())\n",
    "\n",
    "            Doutput = Doutput.view(-1)\n",
    "\n",
    "            errorD_fake = adversarial_loss(Doutput, labels)\n",
    "\n",
    "            errorD_fake.backward()\n",
    "\n",
    "            errorD = errorD_real + errorD_fake\n",
    "\n",
    "            costsD.append(errorD.item())\n",
    "\n",
    "            for p in Recognize.parameters():\n",
    "                g = p.clone()\n",
    "\n",
    "                Dgrads.append(torch.mean(g).item())\n",
    "\n",
    "                if grad_clip is not None:\n",
    "                    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "            Doptim.step()\n",
    "\n",
    "\n",
    "            Speak.zero_grad()\n",
    "\n",
    "            Dlabels = Recognize(fake_phrases)\n",
    "\n",
    "            Dlabels = Dlabels.view(-1)\n",
    "\n",
    "            labels.fill_(real_label)\n",
    "\n",
    "            adversarial_cost = adversarial_loss(Dlabels, labels)\n",
    "\n",
    "            # Our tokenizer's KNN can only accept input with 2 dimmensions. So we'll be passing each batch into the BleuLoss and use their mean.\n",
    "\n",
    "            content_cost = 0\n",
    "\n",
    "            for batch in range(len(fake_phrases)):\n",
    "\n",
    "                content_cost += content_loss.apply(fake_phrases[batch], text[batch])\n",
    "\n",
    "            content_cost = content_cost/len(fake_phrases)\n",
    "\n",
    "            perceptual_loss = content_cost + (adversarial_cost * beta)\n",
    "\n",
    "            perceptual_loss.backward()\n",
    "\n",
    "            costsG.append(perceptual_loss.item())\n",
    "\n",
    "            for p in Speak.parameters():\n",
    "                g = p.clone()\n",
    "\n",
    "                Ggrads.append(torch.mean(g).item())\n",
    "\n",
    "                if grad_clip is not None:\n",
    "                    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "            Goptim.step()\n",
    "\n",
    "\n",
    "            best_disc_loss = float('inf')\n",
    "            best_gen_loss = float('inf')\n",
    "\n",
    "            if errorD.item() < best_disc_loss:\n",
    "\n",
    "                best_disc_loss = errorD.item()\n",
    "                best_disc_params = Recognize.state_dict()\n",
    "\n",
    "            if perceptual_loss.item() < best_gen_loss:\n",
    "\n",
    "                best_gen_loss = perceptual_loss.item()\n",
    "                best_gen_params = Speak.state_dict()\n",
    "\n",
    "            \n",
    "            if i % checkpoint == 0:\n",
    "\n",
    "                print(f\"{epoch}/{epochs}\")\n",
    "                print(f\"Best Discriminator Loss: {best_disc_loss}\\tCurrent LR: {Dscheduler.get_last_lr()[0]}\\tGrads Average: {Dgrads[-1]}\")\n",
    "                print(f\"Best Generator Loss: {best_gen_loss}\\tCurrent LR: {Gscheduler.get_last_lr()[0]}\\tGrads Average: {Ggrads[-1]}\")\n",
    "\n",
    "                if save_path is None:\n",
    "                    try:\n",
    "                        os.mkdir(\"Alice\")\n",
    "                        save_path = \"Alice\"\n",
    "                    except:\n",
    "                        save_path = \"Alice\"\n",
    "                        \n",
    "                torch.save({\n",
    "                    'Epoch': epoch,\n",
    "                    'Generator_Params': best_gen_params,\n",
    "                    'Generator_LR': Gscheduler.get_last_lr()[0],\n",
    "                    'Discriminator_Params': best_disc_params,\n",
    "                    'Discriminator_LR': Dscheduler.get_last_lr()[0]\n",
    "                }, f\"{save_path}/Alice_checkpoint.json\")\n",
    "\n",
    "                print(\"Models saved!\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    sample_text = Speak(noise, text)\n",
    "\n",
    "                    with open(f\"{save_path}/sample_text_{epoch}.txt\", 'w+') as f:\n",
    "                        for batch in range(len(sample_text)):\n",
    "                            sentence, _ = dataset.detokenize(sample_text[batch], dataset.dictionary)\n",
    "\n",
    "                            f.write(sentence)\n",
    "                            f.write('\\n')\n",
    "\n",
    "                    #f.close()\n",
    "                \n",
    "                print(\"Samples saved!\")\n",
    "            \n",
    "            f.close()\n",
    "\n",
    "        \n",
    "        Gscheduler.step()\n",
    "        Dscheduler.step()\n",
    "\n",
    "        Glearning_rate.append(Gscheduler.get_last_lr()[0])\n",
    "        Dlearning_rate.append(Dscheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5\n",
      "Best Discriminator Loss: 1.590544018086319\tCurrent LR: 1e-05\tGrads Average: 0.0\n",
      "Best Generator Loss: 0.0006285470264459421\tCurrent LR: 5.0\tGrads Average: 0.0\n",
      "Models saved!\n",
      "Samples saved!\n",
      "1/5\n",
      "Best Discriminator Loss: 1.3094553423955126\tCurrent LR: 1e-05\tGrads Average: -2.274737345670894e-06\n",
      "Best Generator Loss: 0.0007421080758653822\tCurrent LR: 0.5\tGrads Average: 29.516257034547124\n",
      "Models saved!\n",
      "Samples saved!\n",
      "2/5\n",
      "Best Discriminator Loss: 1.3046392231380162\tCurrent LR: 1e-05\tGrads Average: 4.09221978192605e-06\n",
      "Best Generator Loss: 0.0008112323117914499\tCurrent LR: 0.05\tGrads Average: 29.603664904172756\n",
      "Models saved!\n",
      "Samples saved!\n",
      "3/5\n",
      "Best Discriminator Loss: 1.272027541633915\tCurrent LR: 1e-05\tGrads Average: 1.7739951751202434e-05\n",
      "Best Generator Loss: 0.0008751492644046873\tCurrent LR: 0.005000000000000001\tGrads Average: 29.607049909543697\n",
      "Models saved!\n",
      "Samples saved!\n",
      "4/5\n",
      "Best Discriminator Loss: 1.0827107356696406\tCurrent LR: 1e-05\tGrads Average: 4.2266294865712174e-05\n",
      "Best Generator Loss: 0.0009305425605896106\tCurrent LR: 0.0005000000000000001\tGrads Average: 29.607436271110885\n",
      "Models saved!\n",
      "Samples saved!\n"
     ]
    }
   ],
   "source": [
    "train_TextGAN(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    epochs=5,\n",
    "    checkpoint=100,\n",
    "    lr=1e-5,\n",
    "    beta=1e-3,\n",
    "    grad_clip=1.0,\n",
    "    save_path=None,\n",
    "    d_transformer=16,\n",
    "    n_heads=2,\n",
    "    d_queries=4,\n",
    "    d_values=4,\n",
    "    d_inner=32,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import tensorboard\n",
    "\n",
    "Speak = Broca(positional_encoding, 16, 2, 4, 4, 32, 2, 0.1).to(device).double()\n",
    "\n",
    "Recognize = Wernicke().to(device).double()\n",
    "\n",
    "writer = tensorboard.SummaryWriter(log_dir='Generator', comment=\"Testing Alice's Text GAN\")\n",
    "\n",
    "writer.add_graph(Speak, [torch.randn((1, 50000, 1), dtype=torch.double, device=device), dataset.text_data[0].to(device)])\n",
    "\n",
    "writer.close()\n",
    "\n",
    "writer = tensorboard.SummaryWriter(log_dir='Discriminator', comment=\"Testing Alice's Text GAN\")\n",
    "\n",
    "writer.add_graph(Recognize, torch.randn((1, 77, 1)).to(device).double())\n",
    "\n",
    "epochs=5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for a, b, c, d, e, f in zip(costsD, costsG, Glearning_rate, Dlearning_rate, Dgrads, Ggrads):\n",
    "\n",
    "        writer.add_scalar('Discriminator Loss', a, global_step=(epochs*140//1))\n",
    "        writer.add_scalar('Generator Loss', b, global_step=(epochs*140//1))\n",
    "        writer.add_scalar('Generator Learning Rate', c, global_step=(epochs*140//1))\n",
    "        writer.add_scalar('Discriminator Learning Rate', d, global_step=(epochs*140//1))\n",
    "        writer.add_scalar('Discriminator Gradients Average', e, global_step=(epochs*140//1))\n",
    "        writer.add_scalar('Generator Gradients Average', f, global_step=(epochs*140//1))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Alice/sample_text_4.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateGAN(\n",
    "    dataset=None,\n",
    "    batch_size=16,\n",
    "    d_transformer=512,\n",
    "    n_heads=8,\n",
    "    d_queries=64,\n",
    "    d_values=64,\n",
    "    d_inner=2056,\n",
    "    n_layers=6,\n",
    "    dropout=0.1,\n",
    "    model_path=None\n",
    "):\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    Speak = Broca(positional_encoding, d_transformer, n_heads, d_queries, d_values, d_inner, n_layers, dropout).to(device).double()\n",
    "\n",
    "    params = torch.load(f\"{model_path}/Alice_checkpoint.json\")\n",
    "\n",
    "    Speak.load_state_dict(params['Generator_Params'])\n",
    "\n",
    "    Speak.eval()\n",
    "\n",
    "    _, (_, _, text) = next(enumerate(dataloader))\n",
    "\n",
    "    text = text.to(device)\n",
    "\n",
    "    noise = torch.normal(0, 1000, size=(text.size(0), 500*100, 1), device=device, dtype=torch.double)\n",
    "    \n",
    "    sample_text = Speak(noise, text)\n",
    "\n",
    "    for i in range(len(sample_text)):\n",
    "\n",
    "        sentence, _ = dataset.detokenize(sample_text[i].detach(), dataset.dictionary)\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateGAN(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    model_path=\"Alice\",\n",
    "    d_transformer=16,\n",
    "    n_heads=2,\n",
    "    d_queries=4,\n",
    "    d_values=4,\n",
    "    d_inner=32,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GAN has been built. It needs some polishment(It only generates blank spaces), but this will do for now. Now, onward to the audio generator.\n",
    "# Tacotron 2 uses as input embedding matrices which are passed into conv2d layers, with kernels 5x1. The output is batchnormalized + ReLU\n",
    "# After that, it's passed into a bidirectional LSTM with 512 cells, and, then, into a Local Sensitive Attention. This word seems familiar...\n",
    "# This is the encoder, which will generate vectors (batch, ). After that, the output is passed into a decoder which will try to predict Mel Spectrograms\n",
    "# The decoder has 2 LSTM layers. EACH TIMESTEP of the LSTM output is then passed through 2 Linear layers, concatenated and again passed into the initial 2 LSTM layers\n",
    "# This output is already a spectrogram, and part of it will be sent into the attention layer again + sigmoid activation, while another part will become a residual block\n",
    "# Each timestep of that output is passed into a Conv2D layer with kernels 5x1 and this output is summed to the residual block, generating the Spectrogram\n",
    "\n",
    "# The Spectrogram is then passed into the WaveGlow. WaveGlow is actually a GAN which generates audio from random noise conditioned by a spectrogram.\n",
    "# In Tacotron training, however, instead of using random noise, it's used actually audio data squeezed to vectors, passed into a Conv1D layer with kernels 1x1\n",
    "# and which weights are initialized from an orthogonal distribution (thus, being called invertible)\n",
    "# Half of this layer output is passed through a WN layer which are a bunch of Conv1Ds. Some of those Conv1Ds will also include Mel Spectrograms that will condition the output.\n",
    "# This output is concatenated to part of that half before, and voilá\n",
    "\n",
    "# We could take the Text Generator output as input for our Audio Generator. Each timestep could be passed into a bunch of Transposed Conv2D layers, which will\n",
    "# generate our Spectrograms.\n",
    "# This generation could be conditioned by the addition of target spectrograms properly vectorized.\n",
    "# We might also consider using Average/MaxPool2D...and maybe upsampling2D to avoid vanishing gradients\n",
    "# After generating our spectrograms, we could convert them to audio by applying convs or vectorizing them.\n",
    "\n",
    "# Also, WaveGlow uses convolution layers at each timestep in the audio. We could do the same ---> For time_step in (batch, 1, time_steps, features)\n",
    "# Since we're using mono audio, we can make that 1 disappear. ---> (batch, time_steps, features)\n",
    "# We'll be doing that for each batch ----> for time_step in (time_steps, features) ----> (features,). This way, we can use a 1D Convolution (Like WaveGlow),\n",
    "# or linear layers, which are more effective\n",
    "\n",
    "# We'll probably also need a SuperResolution model, which is used for Uberduck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vagus(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives text and spectrograms as input and outputs a waveform.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Vagus, self).__init__()\n",
    "        \n",
    "        self.neuron1 = nn.Linear(77, 100, bias=False)\n",
    "        #self.neuron2 = nn.Linear(100, 10, bias=False)\n",
    "        #self.neuron3 = nn.Linear(10, 80, bias=False)\n",
    "        #self.neuron4 = nn.Linear(80, 10, bias=False)\n",
    "        #self.neuron5 = nn.Linear(10, 60, bias=False)\n",
    "        #self.neuron6 = nn.Linear(60, 10, bias=False)\n",
    "        #self.neuron7 = nn.Linear(10, 50, bias=False)\n",
    "        #self.neuron8 = nn.Linear(50, 10, bias=False)\n",
    "        #self.neuron9 = nn.Linear(10, 40, bias=False)\n",
    "        #self.neuron10 = nn.Linear(40, 100, bias=False)\n",
    "        #self.neuron11 = nn.Linear(100, 20, bias=False)\n",
    "        #self.neuron12 = nn.Linear(20, 100, bias=False)\n",
    "        self.neuron13 = nn.Linear(100, 100, bias=False)\n",
    "        self.neuron14 = nn.Linear(100, 80, bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU() # Spectrogram values goes from 0 and beyond, so a ReLU might be quite convenient\n",
    "\n",
    "        #self.layer_norm = nn.LayerNorm(10)\n",
    "        self.layer_norm2 = nn.LayerNorm(100)\n",
    "\n",
    "        # Linears might be a nice way to get more weights to optimize our model,\n",
    "        # but they also make our model too heavy and might cause vanishing gradients\n",
    "\n",
    "        #self.neuron15 = nn.Linear(1333*80, 1, bias=False)\n",
    "        #self.neuron16 = nn.Linear(1, 1333*256, bias=False)\n",
    "\n",
    "        self.upsampler = nn.Upsample(scale_factor=3.2, mode='nearest-exact')\n",
    "        # Besides... ProGrow showed us that, perhaps, it might be good to start with few weights, and then use more as the model learns.\n",
    "\n",
    "\n",
    "    def forward(self, text_input, target_spectrogram):\n",
    "\n",
    "        batch_size = text_input.size(0)\n",
    "\n",
    "        spectrogram_length = target_spectrogram.size(1)\n",
    "\n",
    "        # Text ----> Extract character ----> Generate MelSpec time_step from character\n",
    "        # (Batch, time_steps, 1) ----> (Batch, 1) ----> (Batch, 1, features)\n",
    "\n",
    "        # Concatenate each MelSpec time_step generated in order to form a complete MelSpec ----> (Batch, time_steps, features) = Target Spectrogram\n",
    "        # Use formed spectrogram to generate waveform: (Batch, time_steps, features) ----------> (Batch, audio_data)\n",
    "\n",
    "        # Flatten? ----> (Batch, 1333*80) = (Batch, 106640) ----> Linear Neurons -----> (Batch, 341087) ---> Unsqueeze ---> (Batch, 1, 341087)\n",
    "\n",
    "        mel_spec = torch.zeros_like(target_spectrogram, device=device)\n",
    "\n",
    "        for time_step in range(spectrogram_length):\n",
    "\n",
    "            text_feature = text_input.view(batch_size, -1) # (Batch, text_length * text_features) ---> (Batch, 76)\n",
    "\n",
    "            text_feature = self.neuron1(text_feature) # 76*1 ----> 76*1000 -----> 76*10 ----> 76*800 ----> 76*10 ---> 76*600 ---> 76*10 ---> [...] ---> 80\n",
    "\n",
    "            text_feature = self.relu(text_feature)\n",
    "\n",
    "            text_feature = text_feature\n",
    "\n",
    "            #text_feature = self.neuron2(text_feature)\n",
    "\n",
    "            #r1 = text_feature\n",
    "\n",
    "            #text_feature = self.layer_norm(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron3(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron4(text_feature)\n",
    "\n",
    "            #r2 = text_feature\n",
    "\n",
    "            #text_feature = self.layer_norm(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron5(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron6(text_feature)\n",
    "\n",
    "            #text_feature = text_feature + r2\n",
    "\n",
    "            #text_feature = self.layer_norm(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron7(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron8(text_feature)\n",
    "\n",
    "            #text_feature = text_feature + r1\n",
    "\n",
    "            #text_feature = self.layer_norm(text_feature)\n",
    "            #text_feature = self.neuron9(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron10(text_feature)\n",
    "\n",
    "            #r3 = text_feature\n",
    "\n",
    "            #text_feature = self.layer_norm2(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron11(text_feature)\n",
    "\n",
    "            #text_feature = self.neuron12(text_feature)\n",
    "\n",
    "            #text_feature = text_feature + r3\n",
    "\n",
    "            #text_feature = self.layer_norm2(text_feature)\n",
    "\n",
    "            text_feature = self.neuron13(text_feature)\n",
    "\n",
    "            text_feature = self.layer_norm2(text_feature)\n",
    "\n",
    "            text_feature = self.relu(text_feature)\n",
    "\n",
    "            text_feature = self.neuron14(text_feature)\n",
    "\n",
    "            text_feature = self.relu(text_feature)\n",
    "\n",
    "            mel_spec[:, time_step] += text_feature\n",
    "\n",
    "            #del r1, r2, r3, text_feature # Freeing some memory\n",
    "\n",
    "        mel_spec = mel_spec + target_spectrogram\n",
    "\n",
    "        # The ideia, originally, was to pass the features from each spec time_step into a linear layer in order\n",
    "        # to get 256 features. However, I failed to think on how can I concatenate it all into a single tensor with size (batch, 1, sequence*features)\n",
    "\n",
    "        spec_features = mel_spec.view(batch_size, -1) # (Batch, 1333*80)\n",
    "\n",
    "        #spec_features = self.neuron15(spec_features)\n",
    "        #spec_features = self.neuron16(spec_features)\n",
    "\n",
    "        audio = spec_features.view(batch_size, 1, -1) # (Batch, 1, 1333*80)\n",
    "\n",
    "        audio = self.upsampler(audio) # (Batch, 1, 1333*256)\n",
    "\n",
    "        # 1333*256 = 341248, and we need 341087\n",
    "\n",
    "        output = audio[:, :, 0:341087]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "costAudio = []\n",
    "VLR = []\n",
    "VGrads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Vagus(\n",
    "    dataset=None,\n",
    "    batch_size=16,\n",
    "    epochs=1000,\n",
    "    checkpoint=100,\n",
    "    lr=1,\n",
    "    beta=1e-3,\n",
    "    grad_clip=None,\n",
    "    save_path=None\n",
    "):\n",
    "\n",
    "    Talker = Vagus().to(device).double()\n",
    "\n",
    "    if save_path is None:\n",
    "\n",
    "        start_epoch = 0\n",
    "\n",
    "        for name, param in Talker.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param, 0, 0.01)\n",
    "            \n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        params = torch.load(f\"{save_path}/Alice_Voice_checkpoint.json\")\n",
    "\n",
    "        start_epoch = params['Epoch'] + 1\n",
    "        Talker.load_state_dict(params['Vagus_Params'])\n",
    "        lr = params['Vagus_LR']\n",
    "\n",
    "        print(f\"\\nLoaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "    Voptim = torch.optim.AdamW(Talker.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-9, weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(Voptim, 1, gamma=0.1)\n",
    "\n",
    "    # I feel that we should maybe use a log-based loss function, since Amplitudes in decibels require logs, but...meh... Maybe she'll figure it out something...\n",
    "\n",
    "    audio_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    Talker.train()\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, (audio, spectrogram, text) in enumerate(dataloader):\n",
    "\n",
    "            Talker.zero_grad()\n",
    "\n",
    "            audio = audio.to(device).double()\n",
    "            spectrogram = spectrogram.to(device).double()\n",
    "            text = text.to(device).double()\n",
    "\n",
    "            generated_audio = Talker(text, spectrogram)\n",
    "\n",
    "            cost = audio_loss(generated_audio, audio)\n",
    "\n",
    "            cost.backward()\n",
    "\n",
    "            costAudio.append(cost.item())\n",
    "\n",
    "            for p in Talker.parameters():\n",
    "                g = p.clone()\n",
    "\n",
    "                VGrads.append(torch.mean(g).item())\n",
    "\n",
    "                if grad_clip is not None:\n",
    "                    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "            Voptim.step()\n",
    "\n",
    "            best_loss = float('inf')\n",
    "\n",
    "            if cost.item() < best_loss:\n",
    "\n",
    "                best_loss = cost.item()\n",
    "                best_params = Talker.state_dict()\n",
    "\n",
    "            \n",
    "            if i % checkpoint == 0:\n",
    "\n",
    "                print(f\"{epoch}/{epochs}\\nCurrent Loss: {cost.item()}\\tBest Loss: {best_loss}\\nCurrent LR: {scheduler.get_last_lr()[0]}\")\n",
    "                print(f\"Grads Average: {VGrads[-1]}\")\n",
    "\n",
    "                if save_path is None:\n",
    "                    try:\n",
    "                        os.mkdir(\"Alice\")\n",
    "                        save_path = \"Alice\"\n",
    "                    except:\n",
    "                        save_path = \"Alice\"\n",
    "\n",
    "                torch.save({\n",
    "                    'Epoch': epoch,\n",
    "                    'Vagus_Params': best_params,\n",
    "                    'Vagus_LR': scheduler.get_last_lr()[0]\n",
    "                }, f\"{save_path}/Alice_Voice_checkpoint.json\")\n",
    "\n",
    "                print(\"Model saved!\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    sample_audio = Talker(text, spectrogram)\n",
    "\n",
    "                    plt.figure(figsize=(5, 5))\n",
    "                    display.specshow(sample_audio[0].cpu().numpy(), sr=44100)\n",
    "                    plt.colorbar()\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    Audio(sample_audio[0].cpu(), rate=44100)\n",
    "\n",
    "                    torchaudio.save(f'{save_path}/Alice_Voice_sample.wav', sample_audio[0].cpu(), sample_rate=44100, channels_first=True)\n",
    "                \n",
    "                print(\"Audio sample saved!\")\n",
    "\n",
    "        \n",
    "        scheduler.step()\n",
    "        VLR.append(scheduler.get_last_lr()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3\n",
      "Current Loss: 46762.17643333139\tBest Loss: 46762.17643333139\n",
      "Current LR: 1\n",
      "Grads Average: 0.0\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n",
      "0/3\n",
      "Current Loss: 14429.706583459989\tBest Loss: 14429.706583459989\n",
      "Current LR: 1\n",
      "Grads Average: -1.2092777356103535\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n",
      "1/3\n",
      "Current Loss: 87602.54853548965\tBest Loss: 87602.54853548965\n",
      "Current LR: 0.1\n",
      "Grads Average: -1.1941901558788557\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n",
      "1/3\n",
      "Current Loss: 26620.2577855224\tBest Loss: 26620.2577855224\n",
      "Current LR: 0.1\n",
      "Grads Average: -1.1941795597961513\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n",
      "2/3\n",
      "Current Loss: 115706.10841878023\tBest Loss: 115706.10841878023\n",
      "Current LR: 0.010000000000000002\n",
      "Grads Average: -1.1941795594297557\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n",
      "2/3\n",
      "Current Loss: 59473.1912198435\tBest Loss: 59473.1912198435\n",
      "Current LR: 0.010000000000000002\n",
      "Grads Average: -1.1941795594295215\n",
      "Model saved!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio sample saved!\n"
     ]
    }
   ],
   "source": [
    "train_Vagus(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    epochs=3,\n",
    "    checkpoint=100,\n",
    "    lr=1,\n",
    "    beta=1e-3,\n",
    "    grad_clip=None,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "display.specshow(dataset.spectrograms[0].numpy(), sr=44100)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(VGrads) ### ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import tensorboard\n",
    "\n",
    "Talker = Vagus().to(device).double()\n",
    "\n",
    "for name, param in Vagus.named_parameters():\n",
    "    if \"weight\" in name:\n",
    "        nn.init.normal_(param, 0, 0.01)\n",
    "    \n",
    "    if \"bias\" in name:\n",
    "        nn.init.constant_(param, 0.0)\n",
    "\n",
    "writer = tensorboard.SummaryWriter(log_dir='Vagus', comment=\"Testing Alice's Voice Generator\")\n",
    "\n",
    "writer.add_graph(Vagus, [torch.randn((1, 77, 1)).to(device).double(), torch.randn((1, 1333, 80).to(device).double()])\n",
    "\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for a, b, c in zip(costAudio, VLR, VGrads):\n",
    "\n",
    "        writer.add_scalar('Vagus Loss', a, global_step=epoch)\n",
    "        writer.add_scalar('Vagus Learning Rate', b[0], global_step=epoch)\n",
    "        writer.add_scalar('Vagus Gradients Average', c, global_step=epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "def evaluate_Vagus(\n",
    "    dataset=None,\n",
    "    batch_size=16,\n",
    "    model_path=None\n",
    "):\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    Talker = Vagus().to(device).double()\n",
    "\n",
    "    params = torch.load(f\"{model_path}/Alice_Voice_checkpoint.json\")\n",
    "\n",
    "    Talker.load_state_dict(params['Vagus_Params'])\n",
    "\n",
    "    Talker.eval()\n",
    "\n",
    "    _, (_, spectrogram, text) = next(enumerate(dataloader))\n",
    "\n",
    "    spectrogram = spectrogram.to(device)\n",
    "    text = text.to(device)\n",
    "\n",
    "    sample_audio = Talker(text, spectrogram)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    display.specshow(sample_audio[0].detach().cpu().numpy(), sr=44100)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    Audio(sample_audio[0].detach().cpu(), rate=44100)\n",
    "\n",
    "    torchaudio.save(f'{model_path}/Alice_Voice_sample.wav', sample_audio[0].detach().cpu(), sample_rate=44100, channels_first=True)\n",
    "    winsound.PlaySound(f'{model_path}/Alice_Voice_sample.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_Vagus(dataset=dataset, batch_size=1, model_path=\"Alice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
