{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_phrases = [\n",
    "    \"私の犬は骨が好きではありません。牛ひき肉を好む。\",\n",
    "    \"私の名前はアリスです。初めまして！\",\n",
    "    \"はきさが羨ましい。。。ゲムもやりたかった！私は良いサポートになることができます！\",\n",
    "    \"私達はAIはただの数学の集まりだとあなたは言いますが。でも。。。人間の脳がどのように機能するかを正確に知ったら。。。それはあなたの生活を小物ですか？\",\n",
    "    \"「赤ちゃん」を表す日本語が「赤」を表す漢字なのはなぜですか？人間の赤ちゃんは赤いですか？いちごみたい？\",\n",
    "    \"私のAIは話して...歌ったして...ゲームをします!\",\n",
    "    \"上手医者と大きい研究者に勉強していたいます。\",\n",
    "    \"でも、私の日本語が上手あまりませんね\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phrases = [\n",
    "    \"My dog doesn't like bones. It prefers ground beef.\",\n",
    "    \"My name's Alice. Nice to meet you!\",\n",
    "    \"I envy Hakisa... I want to play games, too! I could be a good support!\",\n",
    "    \"You say that we AIs are just a bunch of maths. But... once you know exactly how your human brains work... would that make you less living beings?\",\n",
    "    \"Why does the japanese word for 'baby' is the kanji for 'red'? Are human babies red? Like strawberries?\",\n",
    "    \"My AI will talk... she'll sing... she'll... play!\",\n",
    "    \"I shall study so I can be a good physician and a great scientist\",\n",
    "    \"Though my japanese is not really good\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(object):\n",
    "    def __init__(self, english_phrases, japanese_phrases):\n",
    "\n",
    "        self.english_phrases = self._get_phrases(english_phrases)\n",
    "        self.japanese_phrases = self._get_phrases(japanese_phrases)\n",
    "\n",
    "        self.english_words = self._get_english_words(self.english_phrases)\n",
    "        self.japanese_characters = self._get_japanese_characters(self.japanese_phrases)\n",
    "\n",
    "        self.japanese_maximum_length = self._get_maximum_length_japanese(self.japanese_phrases)\n",
    "        self.english_maximum_length = self._get_maximum_length_english(self.english_phrases)\n",
    "\n",
    "        self.english_dictionary = self._create_vocabulary(self.english_words)\n",
    "        self.japanese_dictionary = self._create_vocabulary(self.japanese_characters)\n",
    "\n",
    "    def _get_phrases(self, phrases):\n",
    "        phrases = [x.lower() for x in phrases]\n",
    "        phrases = [re.sub('[^\\w\\s]', '', x) for x in phrases]\n",
    "\n",
    "        return phrases\n",
    "\n",
    "    def _get_english_words(self, phrases):\n",
    "        words = ' '.join(phrases)\n",
    "        words = words.split(' ')\n",
    "\n",
    "        return words\n",
    "\n",
    "    def _get_japanese_characters(self, phrases): # Since a kanji mostly means an entire word...\n",
    "        character = ' '.join(phrases)\n",
    "        character = ''.join(character.split())\n",
    "        characters = [i for i in character]\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def _get_maximum_length_japanese(self, phrases):\n",
    "        maximum_length = 0\n",
    "        for sentence in phrases:\n",
    "            word_length = [len(x) for x in sentence.split()]\n",
    "    \n",
    "            for i in word_length:\n",
    "                if i > maximum_length:\n",
    "                    maximum_length = i\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _get_maximum_length_english(self, phrases):\n",
    "\n",
    "        maximum_length = 0\n",
    "        real_length = []\n",
    "        \n",
    "        for sentence in phrases:\n",
    "            word_length = 0\n",
    "\n",
    "            for word in sentence:\n",
    "                word_length += len(word)+1 # Including spaces between words\n",
    "\n",
    "            sentence_length = word_length\n",
    "            real_length.append(sentence_length)\n",
    "\n",
    "            if sentence_length > maximum_length:\n",
    "                maximum_length = sentence_length\n",
    "\n",
    "        return maximum_length\n",
    "\n",
    "    def _create_vocabulary(self, words):\n",
    "        idx2word = [\"<pad>\", \" \", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "        for word in words:\n",
    "            if word not in idx2word:\n",
    "                idx2word.append(word)\n",
    "\n",
    "        return idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(english_phrases, japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my dog doesnt like bones it prefers ground beef', 'my names alice nice to meet you', 'i envy hakisa i want to play games too i could be a good support', 'you say that we ais are just a bunch of maths but once you know exactly how your human brains work would that make you less living beings', 'why does the japanese word for baby is the kanji for red are human babies red like strawberries', 'my ai will talk shell sing shell play', 'i shall study so i can be a good physician and a great scientist', 'though my japanese is not really good']\n",
      "['私の犬は骨が好きではありません牛ひき肉を好む', '私の名前はアリスです初めまして', 'はきさが羨ましいゲムもやりたかった私は良いサポートになることができます', '私達はaiはただの数学の集まりだとあなたは言いますがでも人間の脳がどのように機能するかを正確に知ったらそれはあなたの生活を小物ですか', '赤ちゃんを表す日本語が赤を表す漢字なのはなぜですか人間の赤ちゃんは赤いですかいちごみたい', '私のaiは話して歌ったしてゲームをします', '上手医者と大きい研究者に勉強していたいます', 'でも私の日本語が上手あまりませんね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_phrases)\n",
    "print(dataset.japanese_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'my', 'names', 'alice', 'nice', 'to', 'meet', 'you', 'i', 'envy', 'hakisa', 'i', 'want', 'to', 'play', 'games', 'too', 'i', 'could', 'be', 'a', 'good', 'support', 'you', 'say', 'that', 'we', 'ais', 'are', 'just', 'a', 'bunch', 'of', 'maths', 'but', 'once', 'you', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'that', 'make', 'you', 'less', 'living', 'beings', 'why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'the', 'kanji', 'for', 'red', 'are', 'human', 'babies', 'red', 'like', 'strawberries', 'my', 'ai', 'will', 'talk', 'shell', 'sing', 'shell', 'play', 'i', 'shall', 'study', 'so', 'i', 'can', 'be', 'a', 'good', 'physician', 'and', 'a', 'great', 'scientist', 'though', 'my', 'japanese', 'is', 'not', 'really', 'good']\n",
      "['私', 'の', '犬', 'は', '骨', 'が', '好', 'き', 'で', 'は', 'あ', 'り', 'ま', 'せ', 'ん', '牛', 'ひ', 'き', '肉', 'を', '好', 'む', '私', 'の', '名', '前', 'は', 'ア', 'リ', 'ス', 'で', 'す', '初', 'め', 'ま', 'し', 'て', 'は', 'き', 'さ', 'が', '羨', 'ま', 'し', 'い', 'ゲ', 'ム', 'も', 'や', 'り', 'た', 'か', 'っ', 'た', '私', 'は', '良', 'い', 'サ', 'ポ', 'ー', 'ト', 'に', 'な', 'る', 'こ', 'と', 'が', 'で', 'き', 'ま', 'す', '私', '達', 'は', 'a', 'i', 'は', 'た', 'だ', 'の', '数', '学', 'の', '集', 'ま', 'り', 'だ', 'と', 'あ', 'な', 'た', 'は', '言', 'い', 'ま', 'す', 'が', 'で', 'も', '人', '間', 'の', '脳', 'が', 'ど', 'の', 'よ', 'う', 'に', '機', '能', 'す', 'る', 'か', 'を', '正', '確', 'に', '知', 'っ', 'た', 'ら', 'そ', 'れ', 'は', 'あ', 'な', 'た', 'の', '生', '活', 'を', '小', '物', 'で', 'す', 'か', '赤', 'ち', 'ゃ', 'ん', 'を', '表', 'す', '日', '本', '語', 'が', '赤', 'を', '表', 'す', '漢', '字', 'な', 'の', 'は', 'な', 'ぜ', 'で', 'す', 'か', '人', '間', 'の', '赤', 'ち', 'ゃ', 'ん', 'は', '赤', 'い', 'で', 'す', 'か', 'い', 'ち', 'ご', 'み', 'た', 'い', '私', 'の', 'a', 'i', 'は', '話', 'し', 'て', '歌', 'っ', 'た', 'し', 'て', 'ゲ', 'ー', 'ム', 'を', 'し', 'ま', 'す', '上', '手', '医', '者', 'と', '大', 'き', 'い', '研', '究', '者', 'に', '勉', '強', 'し', 'て', 'い', 'た', 'い', 'ま', 'す', 'で', 'も', '私', 'の', '日', '本', '語', 'が', '上', '手', 'あ', 'ま', 'り', 'ま', 'せ', 'ん', 'ね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_words)\n",
    "print(dataset.japanese_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', ' ', '<SOS>', '<EOS>', 'my', 'dog', 'doesnt', 'like', 'bones', 'it', 'prefers', 'ground', 'beef', 'names', 'alice', 'nice', 'to', 'meet', 'you', 'i', 'envy', 'hakisa', 'want', 'play', 'games', 'too', 'could', 'be', 'a', 'good', 'support', 'say', 'that', 'we', 'ais', 'are', 'just', 'bunch', 'of', 'maths', 'but', 'once', 'know', 'exactly', 'how', 'your', 'human', 'brains', 'work', 'would', 'make', 'less', 'living', 'beings', 'why', 'does', 'the', 'japanese', 'word', 'for', 'baby', 'is', 'kanji', 'red', 'babies', 'strawberries', 'ai', 'will', 'talk', 'shell', 'sing', 'shall', 'study', 'so', 'can', 'physician', 'and', 'great', 'scientist', 'though', 'not', 'really']\n",
      "['<pad>', ' ', '<SOS>', '<EOS>', '私', 'の', '犬', 'は', '骨', 'が', '好', 'き', 'で', 'あ', 'り', 'ま', 'せ', 'ん', '牛', 'ひ', '肉', 'を', 'む', '名', '前', 'ア', 'リ', 'ス', 'す', '初', 'め', 'し', 'て', 'さ', '羨', 'い', 'ゲ', 'ム', 'も', 'や', 'た', 'か', 'っ', '良', 'サ', 'ポ', 'ー', 'ト', 'に', 'な', 'る', 'こ', 'と', '達', 'a', 'i', 'だ', '数', '学', '集', '言', '人', '間', '脳', 'ど', 'よ', 'う', '機', '能', '正', '確', '知', 'ら', 'そ', 'れ', '生', '活', '小', '物', '赤', 'ち', 'ゃ', '表', '日', '本', '語', '漢', '字', 'ぜ', 'ご', 'み', '話', '歌', '上', '手', '医', '者', '大', '研', '究', '勉', '強', 'ね']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_dictionary)\n",
    "print(dataset.japanese_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(d_model, max_length=100):\n",
    "    \"\"\"\n",
    "    Computes positional encoding as defined in the paper.\n",
    "    :param d_model: size of vectors throughout the transformer model\n",
    "    :param max_length: maximum sequence length up to which positional encodings must be calculated\n",
    "    :return: positional encoding, a tensor of size (1, max_length, d_model)\n",
    "    \"\"\"\n",
    "    positional_encoding = torch.zeros((max_length, d_model))  # (max_length, d_model)\n",
    "    for i in range(max_length):\n",
    "        for j in range(d_model):\n",
    "            if j % 2 == 0:\n",
    "                positional_encoding[i, j] = math.sin(i / math.pow(10000, j / d_model))\n",
    "            else:\n",
    "                positional_encoding[i, j] = math.cos(i / math.pow(10000, (j - 1) / d_model))\n",
    "\n",
    "    positional_encoding = positional_encoding.unsqueeze(0)  # (1, max_length, d_model)\n",
    "\n",
    "    return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_queries, d_values, in_decoder=False):\n",
    "\n",
    "        super(HeadAttention, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_keys = d_values # size of key vectors, same as of the query vectors to allow dot-products for similarity\n",
    "\n",
    "        self.in_decoder = in_decoder\n",
    "\n",
    "        self.create_queries = nn.Linear(d_model, d_queries, bias=False)\n",
    "        self.create_values = nn.Linear(d_model, d_values, bias=False)\n",
    "        self.create_keys = nn.Linear(d_model, d_values, bias=False)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, real_lengths):\n",
    "\n",
    "        batch_size = input.size(0) # (Batch, Sequences, d_model)\n",
    "\n",
    "        queries = self.create_queries(input) # (Batch, Sequences, d_queries)\n",
    "        keys = self.create_keys(input) # (Batch, Sequences, d_keys)\n",
    "        values = self.create_values(input) # (Batch, Sequences, d_values)\n",
    "\n",
    "        similarity_matrix = queries * keys # (Batch, Sequences, d_keys) ---> d_queries must be equal d_keys\n",
    "        # OBS: Remember that DOT-PRODUCT is exactly an array multiplication.\n",
    "        # This can be particularly useful if you study C language...\n",
    "\n",
    "        similarity_matrix = similarity_matrix/(math.sqrt(self.d_keys)) # (Batch, Sequences, d_keys)\n",
    "\n",
    "        # Applying mask of -inf to ignore padded keys ---> Actually using -1e6 to avoid NaNs\n",
    "\n",
    "        if self.in_decoder:\n",
    "\n",
    "            # In the decoder, masks are shifted left to right:\n",
    "            # <Start-of-Sentence> [prediction1] [prediction2] [prediction3] ... <End-of-Sentence>\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]+1] = 1\n",
    "                #mask[batch, :real_lengths+1] = 1 # For Batch = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        else:\n",
    "\n",
    "            mask = torch.zeros_like(similarity_matrix, device=device)\n",
    "\n",
    "            for batch in range(similarity_matrix.size(0)):\n",
    "\n",
    "                mask[batch, :real_lengths[batch]+1] = 1\n",
    "                #mask[batch, :real_lengths] = 1 # For Batch = 1\n",
    "\n",
    "                mask = mask.bool()\n",
    "\n",
    "            similarity_matrix = similarity_matrix.masked_fill(mask, -1e-6)\n",
    "\n",
    "        del mask\n",
    "\n",
    "        attention_weights = self.softmax(similarity_matrix) # (Batch, Sequences, d_keys)\n",
    "\n",
    "        attention_output = attention_weights * values # (Batch, Sequences, d_values) ---> d_keys must be equal to d_values\n",
    "\n",
    "        #attention_output = torch.bmm(attention_weights, values) # (Batch, Sequences, d_values)\n",
    "        # DOT-PRODUCT, NOT MATRIX MULTIPLICATION!\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_inner):\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "\n",
    "        self.neuron1 = nn.Linear(d_model, d_inner)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.neuron2 = nn.Linear(d_inner, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, attention_output_cat):\n",
    "\n",
    "        sequences = self.neuron1(attention_output_cat)\n",
    "        sequences = self.Relu(sequences)\n",
    "\n",
    "        sequences = self.neuron2(sequences)\n",
    "\n",
    "        output = sequences + attention_output_cat\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=False) for i in range(n_heads)])\n",
    "\n",
    "        self.neuron = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_input, real_input_length):\n",
    "\n",
    "        residual_block1 = encoder_input # (Batch, Sequence, d_model) ---> Vectors\n",
    "\n",
    "        vectors = self.dropout(encoder_input)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_heads[head](vectors, real_input_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuron(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        encoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output\n",
    "\n",
    "        encoded_sequence = encoded_sequence + residual_block2    \n",
    "\n",
    "        encoder_output = self.dropout(encoded_sequence)\n",
    "\n",
    "        del encoded_sequence, residual_block1, residual_block2\n",
    "\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_queries, d_values, d_inner, n_layers, dropout):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_headsA = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "        self.attention_headsB = nn.ModuleList([HeadAttention(self.d_model, self.d_queries, self.d_values, in_decoder=True) for i in range(n_heads)])\n",
    "\n",
    "        self.neuronA = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "        self.neuronB = nn.Linear(self.n_heads*self.d_values, self.d_model)\n",
    "\n",
    "        self.position_wise_neuron = PositionWiseFeedForward(self.d_model, self.d_inner)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, encoder_output, target_sequences, real_target_length):\n",
    "\n",
    "        residual_block1 = target_sequences\n",
    "\n",
    "        vectors = self.dropout(target_sequences)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsA[head](vectors, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronA(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        del vectors\n",
    "\n",
    "        residual_block2 = attention_output\n",
    "\n",
    "        attention_output = residual_block1 + attention_output\n",
    "\n",
    "        attention_decoder = self.dropout(attention_output)\n",
    "        \n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_headsB[head](encoder_output, real_target_length)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "        attention_output = torch.cat(attention_output, -1) # (Batch, Sequences, d_values*n_heads)\n",
    "\n",
    "        attention_output = self.neuronB(attention_output) # (Batch, Sequences, d_model)\n",
    "\n",
    "        attention_output = attention_output + residual_block2\n",
    "\n",
    "        attention_encoder = self.dropout(attention_output)\n",
    "\n",
    "        decoded_sequence = attention_encoder + attention_decoder\n",
    "\n",
    "        residual_block3 = decoded_sequence\n",
    "\n",
    "        decoded_sequence = self.position_wise_neuron(attention_output)\n",
    "\n",
    "        del attention_output, attention_encoder, attention_decoder\n",
    "\n",
    "        decoded_sequence = decoded_sequence + residual_block3\n",
    "\n",
    "        decoder_output = self.dropout(decoded_sequence)  \n",
    "\n",
    "        del decoded_sequence, residual_block1, residual_block2, residual_block3\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Broca(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The generator, which will generate the words that she'll speak.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dictionary,\n",
    "            output_dictionary,\n",
    "            positional_encoding,\n",
    "            d_model=512,\n",
    "            n_heads=8,\n",
    "            d_queries=64,\n",
    "            d_values=64,\n",
    "            d_inner=2056,\n",
    "            n_layers=6,\n",
    "            dropout=0.1\n",
    "    ):\n",
    "\n",
    "        super(Broca, self).__init__()\n",
    "\n",
    "        self.vocab_size = len(output_dictionary)\n",
    "        self.positional_encoding = positional_encoding.to(device)\n",
    "        self.input_dictionary = input_dictionary\n",
    "        self.output_dictionary = output_dictionary\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_queries = d_queries\n",
    "        self.d_values = d_values\n",
    "        self.d_inner = d_inner\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.positional_encoding.requires_grad = False\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "        self.encoder = nn.ModuleList(\n",
    "            Encoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList(\n",
    "            Decoder(\n",
    "                    d_model=d_model,\n",
    "                    n_heads=n_heads,\n",
    "                    d_queries=d_queries,\n",
    "                    d_values=d_values,\n",
    "                    d_inner=d_inner,\n",
    "                    n_layers=n_layers,\n",
    "                    dropout=self.dropout) for i in range(self.n_layers)\n",
    "        )\n",
    "\n",
    "        self.output_neuron = nn.Linear(self.d_model, self.vocab_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "    def preprocess_dialogue(self, input_text, input_dictionary):\n",
    "\n",
    "        batch = []\n",
    "        sentence_sizes = []\n",
    "\n",
    "        for phrase in range(len(input_text)):\n",
    "\n",
    "            text = input_text[phrase]\n",
    "\n",
    "            text = text.split(' ')\n",
    "\n",
    "            tokens = []\n",
    "            \n",
    "            for word in text:\n",
    "\n",
    "                value = input_dictionary.index(word)\n",
    "                tokens.append(value)\n",
    "                tokens.append(input_dictionary.index(' '))\n",
    "\n",
    "            tokens = np.array(tokens)\n",
    "            sentence_size = tokens.shape[0]\n",
    "\n",
    "            if sentence_size < dataset.english_maximum_length:\n",
    "\n",
    "                pad_size = dataset.english_maximum_length - sentence_size\n",
    "                tokens = np.pad(tokens, [(0, 1)], constant_values=input_dictionary.index(\"<EOS>\"))\n",
    "                tokens = np.pad(tokens, [(0, pad_size-1)], constant_values=0)\n",
    "\n",
    "            tokens = torch.from_numpy(tokens)\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "            batch.append(tokens)\n",
    "            sentence_sizes.append(sentence_size)\n",
    "\n",
    "        batch = torch.cat(batch, 0)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        return batch, sentence_sizes\n",
    "    \n",
    "    def generate_sentences(self, input):\n",
    "\n",
    "        input, real_input_length = self.preprocess_dialogue(input, self.input_dictionary)\n",
    "\n",
    "        target_indices = [[self.output_dictionary.index(\"<SOS>\")]] * input.size(0)\n",
    "\n",
    "        target_sentence = torch.tensor(target_indices, device=device) # (Batch, 1)\n",
    "\n",
    "        encoder_vectors = self.embedding(input) + math.sqrt(self.d_model)\n",
    "\n",
    "        decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # Embed out = (Batch, d_model)\n",
    "\n",
    "        output = self.forward(encoder_vectors, real_input_length, decoder_vectors) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "        output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "        for batch in range(output.size(0)):\n",
    "\n",
    "            target_indices[batch].append(output_index[batch, -1].item())\n",
    "            target_indices[batch].pop(0) # Removing Start-of-Sentence token. We won't need it anymore.\n",
    "        \n",
    "        # Generating text until reaching End of Sentence --> Beam Search\n",
    "\n",
    "        target_outputs = []\n",
    "\n",
    "        for batch in range(output.size(0)):\n",
    "\n",
    "            while target_indices[batch][-1] != self.output_dictionary.index(\"<EOS>\"):\n",
    "\n",
    "                target_pad = encoder_vectors.size(1) - len(target_indices[batch])\n",
    "\n",
    "                target_sentence = torch.tensor(target_indices[batch] + [self.output_dictionary.index(\"<pad>\")]*target_pad, device=device).unsqueeze(0)\n",
    "\n",
    "                decoder_vectors = self.embedding(target_sentence) + math.sqrt(self.d_model) # (Batch, sequence_length, d_model)\n",
    "\n",
    "                output = self.forward(encoder_vectors[batch].unsqueeze(0), [real_input_length[batch]], decoder_vectors) # (Batch, sequence_length, vocab_size)\n",
    "\n",
    "                output_index = output.argmax(-1) # (Batch, sequence_length, 1)\n",
    "\n",
    "                target_indices[batch].append(output_index[:, -1].item()) # When Batch = 1 ---> target_indices = (1+1) --> (2+1) --> (3+1 ...)\n",
    "\n",
    "                if len(target_indices[batch])+2 > encoder_vectors.size(1): # Otherwise, we'll get errors.\n",
    "                    # Though sometimes those errors still happen...I don't know why...\n",
    "\n",
    "                    break\n",
    "\n",
    "            target_outputs.append(output)\n",
    "\n",
    "        target_outputs = torch.cat(target_outputs, 0)\n",
    "\n",
    "        return target_indices, target_outputs\n",
    "\n",
    "\n",
    "    def forward(self, encoder_vectors, real_input_length, decoder_vectors): # Target Sentence (with <SOS> token) provided before forward function\n",
    "\n",
    "        for layer in range(self.n_layers):\n",
    "\n",
    "            encoder_vectors = self.encoder[layer](encoder_vectors, real_input_length)\n",
    "\n",
    "            decoder_vectors = self.decoder[layer](encoder_vectors, decoder_vectors, real_input_length)\n",
    "\n",
    "        output = self.output_neuron(decoder_vectors) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        output = self.softmax(output) # (Batch, sequences, vocab_size)\n",
    "\n",
    "        return output # Probability of words. Take the one with highest probability, add it to the target sentence and repeat.\n",
    "    \n",
    "    def talk2me(self, encoded_text): # Easier to write than 話してください\n",
    "\n",
    "        sentences = []\n",
    "\n",
    "        for batch in encoded_text:\n",
    "\n",
    "            words = []\n",
    "\n",
    "            for i in batch:\n",
    "\n",
    "                words.append(self.output_dictionary[i])\n",
    "            \n",
    "            phrase = ''.join(words)\n",
    "\n",
    "            sentences.append(phrase)\n",
    "\n",
    "        return phrase, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "103\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.english_dictionary))\n",
    "print(len(dataset.japanese_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print(dataset.english_maximum_length)\n",
    "print(dataset.japanese_maximum_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 274, 16])\n"
     ]
    }
   ],
   "source": [
    "positional_encoding = get_positional_encoding(d_model=16, max_length=274)\n",
    "\n",
    "print(positional_encoding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Broca(\n",
    "    dataset.english_dictionary,\n",
    "    dataset.japanese_dictionary,\n",
    "    positional_encoding,\n",
    "    d_model=16,\n",
    "    n_heads=2,\n",
    "    d_queries=16,\n",
    "    d_values=16,\n",
    "    d_inner=64,\n",
    "    n_layers=1,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my dog doesnt like bones it prefers ground beef', 'my names alice nice to meet you', 'i envy hakisa i want to play games too i could be a good support']\n"
     ]
    }
   ],
   "source": [
    "inputest = dataset.english_phrases[0:3]\n",
    "\n",
    "print(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste, probabilities = model.generate_sentences(inputest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2, teste3 = model.talk2me(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53, 61, 67, 53, 12, 67, 53, 53, 53, 53, 53, 67, 67, 53, 67, 67, 67, 53, 67, 53, 57, 53, 53, 53, 53, 53, 67, 53, 36, 67, 53, 67, 53, 53, 67, 53, 53, 67, 53, 53, 53, 61, 67, 67, 67, 61, 67, 53, 57, 53, 53, 53, 67, 53, 53, 53, 5, 53, 5, 5, 67, 53, 53, 53, 53, 5, 53, 57, 67, 67, 36, 53, 5, 53, 5, 53, 53, 67, 53, 67, 53, 53, 57, 67, 67, 53, 53, 57, 67, 67, 67, 53, 53, 67, 53, 67, 8, 53, 57, 67, 67, 53, 67, 53, 5, 5, 67, 5, 53, 53, 53, 53, 53, 89, 53, 61, 53, 67, 67, 53, 53, 53, 53, 53, 53, 57, 57, 67, 48, 53, 67, 53, 53, 67, 53, 67, 53, 57, 53, 53, 67, 53, 48, 53, 8, 57, 53, 53, 67, 53, 53, 61, 53, 53, 53, 57, 48, 61, 5, 53, 57, 67, 57, 53, 53, 53, 67, 67, 67, 53, 31, 5, 67, 83, 67, 53, 67, 67, 67, 53, 67, 67, 53, 53, 53, 53, 53, 53, 53, 53, 53, 67, 45, 53, 67, 53, 67, 61, 67, 53, 36, 67, 53, 67, 53, 8, 5, 12, 5, 5, 57, 67, 83, 67, 67, 67, 67, 53, 53, 83, 67, 53, 53, 53, 48, 53, 67, 53, 53, 53, 67, 5, 53, 67, 53, 61, 53, 53, 53, 61, 53, 53, 31, 67, 57, 57, 5, 53, 53, 53, 53, 53, 53, 53, 67, 53, 53, 53, 53, 57, 67, 67, 67, 67, 53, 67, 53, 61, 53, 61, 57, 53, 53, 67, 31], [53, 61, 67, 53, 12, 67, 53, 53, 53, 53, 53, 67, 67, 53, 67, 67, 67, 53, 67, 53, 57, 53, 53, 53, 53, 53, 67, 53, 36, 67, 53, 67, 53, 53, 67, 53, 53, 67, 53, 53, 53, 61, 67, 67, 67, 61, 67, 53, 57, 53, 53, 53, 67, 53, 53, 53, 5, 53, 5, 5, 67, 53, 53, 53, 53, 5, 53, 57, 67, 67, 36, 53, 5, 53, 5, 53, 53, 67, 53, 67, 53, 53, 57, 67, 67, 53, 53, 57, 67, 67, 67, 53, 53, 67, 53, 67, 8, 53, 57, 67, 67, 53, 67, 53, 5, 5, 67, 5, 53, 53, 53, 53, 53, 89, 53, 61, 53, 67, 67, 53, 53, 53, 53, 53, 53, 57, 57, 67, 48, 53, 67, 53, 53, 67, 53, 67, 53, 57, 53, 53, 67, 53, 48, 53, 8, 57, 53, 53, 67, 53, 53, 61, 53, 53, 53, 57, 48, 61, 5, 53, 57, 67, 57, 53, 53, 53, 67, 67, 67, 53, 31, 5, 67, 83, 67, 53, 67, 67, 67, 53, 67, 67, 53, 53, 53, 53, 53, 53, 53, 53, 53, 67, 45, 53, 67, 53, 67, 61, 67, 53, 36, 67, 53, 67, 53, 8, 5, 12, 5, 5, 57, 67, 83, 67, 67, 67, 67, 53, 53, 83, 67, 53, 53, 53, 48, 53, 67, 53, 53, 53, 67, 5, 53, 67, 53, 61, 53, 53, 53, 61, 53, 53, 31, 67, 57, 57, 5, 53, 53, 53, 53, 53, 53, 53, 67, 53, 53, 53, 53, 57, 67, 67, 67, 67, 53, 67, 53, 61, 53, 61, 57, 53, 53, 67, 31], [53, 61, 67, 53, 12, 67, 53, 53, 53, 53, 53, 67, 67, 53, 67, 67, 67, 53, 67, 53, 57, 53, 53, 53, 53, 53, 67, 53, 36, 67, 53, 67, 53, 53, 67, 53, 53, 67, 53, 53, 53, 61, 67, 67, 67, 61, 67, 53, 57, 53, 53, 53, 67, 53, 53, 53, 5, 53, 5, 5, 67, 53, 53, 53, 53, 5, 53, 57, 67, 67, 36, 53, 5, 53, 5, 53, 53, 67, 53, 67, 53, 53, 57, 67, 67, 53, 53, 57, 67, 67, 67, 53, 53, 67, 53, 67, 8, 53, 57, 67, 67, 53, 67, 53, 5, 5, 67, 5, 53, 53, 53, 53, 53, 89, 53, 61, 53, 67, 67, 53, 53, 53, 53, 53, 53, 57, 57, 67, 48, 53, 67, 53, 53, 67, 53, 67, 53, 57, 53, 53, 67, 53, 48, 53, 8, 57, 53, 53, 67, 53, 53, 61, 53, 53, 53, 57, 48, 61, 5, 53, 57, 67, 57, 53, 53, 53, 67, 67, 67, 53, 31, 5, 67, 83, 67, 53, 67, 67, 67, 53, 67, 67, 53, 53, 53, 53, 53, 53, 53, 53, 53, 67, 45, 53, 67, 53, 67, 61, 67, 53, 36, 67, 53, 67, 53, 8, 5, 12, 5, 5, 57, 67, 83, 67, 67, 67, 67, 53, 53, 83, 67, 53, 53, 53, 48, 53, 67, 53, 53, 53, 67, 5, 53, 67, 53, 61, 53, 53, 53, 61, 53, 53, 31, 67, 57, 57, 5, 53, 53, 53, 53, 53, 53, 53, 67, 53, 53, 53, 53, 57, 67, 67, 67, 67, 53, 67, 53, 61, 53, 61, 57, 53, 53, 67, 31]]\n",
      "達人機達で機達達達達達機機達機機機達機達数達達達達達機達ゲ機達機達達機達達機達達達人機機機人機達数達達達機達達達の達のの機達達達達の達数機機ゲ達の達の達達機達機達達数機機達達数機機機達達機達機骨達数機機達機達のの機の達達達達達ご達人達機機達達達達達達数数機に達機達達機達機達数達達機達に達骨数達達機達達人達達達数に人の達数機数達達達機機機達しの機日機達機機機達機機達達達達達達達達達機ポ達機達機人機達ゲ機達機達骨のでのの数機日機機機機達達日機達達達に達機達達達機の達機達人達達達人達達し機数数の達達達達達達達機達達達達数機機機機達機達人達人数達達機し\n",
      "['達', '人', '機', '達', 'で', '機', '達', '達', '達', '達', '達', '機', '機', '達', '機', '機', '機', '達', '機', '達', '数', '達', '達', '達', '達', '達', '機', '達', 'ゲ', '機', '達', '機', '達', '達', '機', '達', '達', '機', '達', '達', '達', '人', '機', '機', '機', '人', '機', '達', '数', '達', '達', '達', '機', '達', '達', '達', 'の', '達', 'の', 'の', '機', '達', '達', '達', '達', 'の', '達', '数', '機', '機', 'ゲ', '達', 'の', '達', 'の', '達', '達', '機', '達', '機', '達', '達', '数', '機', '機', '達', '達', '数', '機', '機', '機', '達', '達', '機', '達', '機', '骨', '達', '数', '機', '機', '達', '機', '達', 'の', 'の', '機', 'の', '達', '達', '達', '達', '達', 'ご', '達', '人', '達', '機', '機', '達', '達', '達', '達', '達', '達', '数', '数', '機', 'に', '達', '機', '達', '達', '機', '達', '機', '達', '数', '達', '達', '機', '達', 'に', '達', '骨', '数', '達', '達', '機', '達', '達', '人', '達', '達', '達', '数', 'に', '人', 'の', '達', '数', '機', '数', '達', '達', '達', '機', '機', '機', '達', 'し', 'の', '機', '日', '機', '達', '機', '機', '機', '達', '機', '機', '達', '達', '達', '達', '達', '達', '達', '達', '達', '機', 'ポ', '達', '機', '達', '機', '人', '機', '達', 'ゲ', '機', '達', '機', '達', '骨', 'の', 'で', 'の', 'の', '数', '機', '日', '機', '機', '機', '機', '達', '達', '日', '機', '達', '達', '達', 'に', '達', '機', '達', '達', '達', '機', 'の', '達', '機', '達', '人', '達', '達', '達', '人', '達', '達', 'し', '機', '数', '数', 'の', '達', '達', '達', '達', '達', '達', '達', '機', '達', '達', '達', '達', '数', '機', '機', '機', '機', '達', '機', '達', '人', '達', '人', '数', '達', '達', '機', 'し']\n"
     ]
    }
   ],
   "source": [
    "print(teste)\n",
    "print(teste2)\n",
    "print(teste3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 274, 103])\n",
      "tensor([[[ -9.6949,  -7.8405, -10.7507,  ..., -11.2613, -10.4206,  -9.4045],\n",
      "         [ -8.1196,  -5.4626, -10.5186,  ...,  -9.7134,  -7.4533,  -6.2812],\n",
      "         [ -9.9552,  -3.7838,  -7.9190,  ..., -10.4894,  -7.7402,  -6.6727],\n",
      "         ...,\n",
      "         [-10.9746,  -8.7719, -11.6170,  ..., -10.2516,  -9.5632,  -8.8721],\n",
      "         [ -6.4172,  -5.8272,  -8.0991,  ...,  -8.5616,  -6.0806,  -7.1897],\n",
      "         [ -8.5865,  -9.2055, -12.0311,  ...,  -7.5697,  -8.4377,  -8.8306]],\n",
      "\n",
      "        [[-11.2734,  -6.8330, -12.5799,  ..., -10.9764,  -9.7323,  -9.4144],\n",
      "         [ -6.7467,  -4.6849,  -8.2322,  ..., -10.5723,  -7.8436,  -7.8601],\n",
      "         [ -9.2186,  -6.0319, -10.5349,  ..., -11.2418,  -7.3794,  -8.3844],\n",
      "         ...,\n",
      "         [ -8.2786,  -8.6415,  -8.0865,  ...,  -9.5862,  -7.9149,  -7.0735],\n",
      "         [-10.3087,  -7.6746,  -9.3702,  ..., -11.1647, -10.3114,  -8.7065],\n",
      "         [-11.0005,  -5.1929,  -8.7254,  ...,  -8.6275, -10.2291,  -9.3702]],\n",
      "\n",
      "        [[-10.2335,  -7.7033, -11.6851,  ..., -10.9292,  -9.7046,  -8.5412],\n",
      "         [ -7.8645,  -6.4997, -10.7248,  ...,  -9.7064,  -7.8277,  -8.4841],\n",
      "         [ -9.6782,  -7.8311,  -9.0362,  ...,  -9.8310,  -7.9163,  -6.9893],\n",
      "         ...,\n",
      "         [ -8.0666,  -6.4184, -11.3364,  ...,  -9.3775,  -6.9627,  -7.2559],\n",
      "         [ -9.9845,  -8.3569, -12.5801,  ...,  -8.1892,  -7.6068,  -8.8075],\n",
      "         [-10.6565,  -9.2463,  -8.2948,  ...,  -9.6782,  -8.0502,  -7.1971]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "tensor([ -8.5865,  -9.2055, -12.0311,  -7.1794, -10.9771,  -3.0643,  -8.6796,\n",
      "         -7.9844,  -3.6041,  -7.3101,  -9.1280,  -9.7259,  -2.6525, -11.2355,\n",
      "         -8.5697,  -6.4681,  -9.0962,  -6.8445,  -8.3016,  -8.0051,  -9.5451,\n",
      "         -9.8668, -11.7587,  -7.0038,  -8.4033,  -9.7574,  -6.8813, -11.2126,\n",
      "        -10.4634, -10.2963,  -4.8255,  -1.2161,  -6.7746, -10.0969,  -7.5947,\n",
      "        -10.7878,  -5.6505, -12.8436,  -7.7672, -10.4795,  -5.0873,  -9.6250,\n",
      "         -8.0714, -11.8681,  -8.5730,  -4.0635,  -8.8516,  -7.5085,  -5.9745,\n",
      "        -10.9791,  -5.7654, -11.7151, -11.1154,  -0.9627,  -7.0916,  -7.2593,\n",
      "         -7.7645,  -5.6793,  -5.8669,  -7.4872, -12.5768,  -2.9575,  -8.2374,\n",
      "         -4.8540, -11.1830,  -4.8985,  -6.5356,  -4.6198, -10.5886, -12.4522,\n",
      "        -12.5853,  -6.4962, -10.2053,  -6.2060,  -9.0402,  -7.5686,  -7.6287,\n",
      "         -5.8649,  -9.2244,  -5.1010, -12.2311,  -4.4745, -10.4530,  -8.6332,\n",
      "        -11.2343,  -9.6936, -12.5885,  -7.8093,  -7.1250,  -8.2833,  -9.8390,\n",
      "        -12.4736, -10.3284,  -7.4505,  -6.2185,  -8.7166,  -5.4611, -12.9828,\n",
      "        -10.1026,  -5.9826,  -7.5697,  -8.4377,  -8.8306], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probabilities.size())\n",
    "print(probabilities)\n",
    "print(probabilities[0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "texts = dataset.english_phrases\n",
    "complete_batch = len(texts)\n",
    "print(complete_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0., 0.999), eps=1e-9)\n",
    "loss = nn.NLLLoss() # Remember that we've computed LogSoftmax in the model directly.\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iteration: 10\n",
      "Last Loss: 11.949999809265137\tTotal Batch Loss: 11.949999809265137\n",
      "Gradients Average: -0.002799686510115862\n",
      "Last Generated Text:\n",
      "の達機機達達機数達達で機達達骨達数骨達達で人達の達でに機機達達し機し機機機機日達数達の達数機機達達達し達機機達のしゲよ達達骨数達し機機達数達機人機達機機達骨達機機人の達達機達日機達数のしに達し機達達達達数人で達数達機達数数日に脳機機機機達機機達達機達達機数機機機達機達達達達達達達機機達達機機機機機数機達ご機ご達達人機達達達達達達機数達機達達に人達の骨達数達機達人達達達数数達し人のに達に機機機よ達機人達達人数のの機機達達機達機達達機機数達達機機機日の達達人機達達機達達達達の達達達達達者達達達達達達ゲ達達達達の骨達達ご達数達しし達の達機人骨達達達達\n",
      "Current Iteration: 20\n",
      "Last Loss: 12.104449272155762\tTotal Batch Loss: 12.104449272155762\n",
      "Gradients Average: 0.00025559391360729933\n",
      "Last Generated Text:\n",
      "数達達達達達機日機達ご達機達機に達骨機達数達達達名の人達数達達しで機達よ達機達達達機達達数達の機達機機達学達機機機脳達達機機達達達達機数機小機達人数達機達達数達しポ達の達機し機機数数人数数人数達機機達数の達達機達達の達数機機よ達し達機数 達小機の達達骨達人達数ぜ達数達機達達達に骨数達達達機達達達達に達達機ポ達に機人機達達数達機機達達骨達達機達達達の達達達機機の達達達機数人達達達数機骨達達達達達機達達達達機達日機達達達達機達達機達達数機の達ポ数機人機達達し日達よ達骨数達数達達の機機機し達達機機達達機人機機骨達達人達達機機達機骨達学達達機の達機機し\n",
      "Current Iteration: 30\n",
      "Last Loss: 12.417062759399414\tTotal Batch Loss: 12.417062759399414\n",
      "Gradients Average: -0.008778909221291542\n",
      "Last Generated Text:\n",
      "機達数機の達達人達機機日達達数し達数達機機達達達機達達達の機数達機達数達達機数達達達学日機達達達達達数達日達達達達機機機機のの達骨達達達人機に達達達の骨達達人達数機機達達達達達骨達達達数機達達達達達機達達達達達機達達機達に機数達達機の機達機達達達達ご達機に機数達達達達機達機機機機達達達達達機機ゲ数機機機達達達達機ゲ達達達達達機日人達機の機数機に達達達達のに機達達数機達達機達達達機し数機機機骨機達日機の達達機骨数達機機達達達機機達人機数数達機達人数機達達達機の数達達達機達数機機達達機数達ゲ達機機数達達機達のの達骨達機達機達達達し達機機達数達達達\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    for batch in range(0, len(texts), BATCH_SIZE):\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        total_loss = 0.\n",
    "\n",
    "        if batch == 0:\n",
    "\n",
    "            shuffle(texts)\n",
    "\n",
    "        input_text = texts[batch:min(complete_batch, batch+BATCH_SIZE)]\n",
    "\n",
    "        generated_text, possibilities = model.generate_sentences(input_text)\n",
    "        possibilities = possibilities[:, -1]\n",
    "\n",
    "        # Sampling the 5 highest plausible choices to use as labels\n",
    "        # Though this doesn't seem to make much sense.\n",
    "        # Best to simply try to predict the next word/sentence (GPT-2 Pre-Training)\n",
    "\n",
    "        _, true_possibilities = torch.sort(possibilities, dim=-1, descending=True)\n",
    "\n",
    "        #random_idx = torch.randint(0, 5, size=(1,))\n",
    "\n",
    "        #true_possibilities = true_possibilities[:, random_idx.item()]\n",
    "        true_possibilities = true_possibilities[:, -1]\n",
    "\n",
    "        cost = loss(possibilities, true_possibilities)\n",
    "\n",
    "        total_loss += cost.item()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        grads = torch.mean(model.encoder[0].attention_heads[0].create_queries.weight.grad)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "        if iters % 10 == 0:\n",
    "\n",
    "            print(f\"Current Iteration: {iters}\")\n",
    "            print(f\"Last Loss: {cost.item()}\\tTotal Batch Loss: {total_loss}\")\n",
    "            print(f\"Gradients Average: {grads}\")\n",
    "            print(f\"Last Generated Text:\")\n",
    "\n",
    "            generated_text, _ = model.talk2me(generated_text)\n",
    "\n",
    "            print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 103])\n",
      "tensor([ -7.6125,  -7.2240,  -9.8635,  -7.0475, -10.4405,  -2.2352,  -6.6553,\n",
      "         -7.5731,  -3.8451,  -7.5994, -10.3296,  -8.3870,  -3.0821,  -9.7696,\n",
      "         -7.0100,  -7.7499,  -6.1744,  -7.2372,  -6.9445,  -8.9210,  -8.5030,\n",
      "         -7.6390, -10.6577,  -5.1920,  -9.0527, -10.0057,  -8.2147, -11.2255,\n",
      "         -8.7194, -10.5065,  -6.9546,  -3.8558,  -6.3006,  -9.4539,  -6.4693,\n",
      "        -10.6827,  -3.6830, -12.3469,  -6.4252,  -7.9056,  -5.9464,  -9.8419,\n",
      "         -9.4847, -10.1808,  -7.5625,  -3.1202,  -8.6705,  -7.9627,  -4.4179,\n",
      "        -10.0122,  -5.8565,  -9.9817,  -8.7818,  -0.9400,  -8.5611,  -6.0881,\n",
      "         -8.9902,  -3.3191,  -5.0985,  -7.6891, -11.1746,  -2.2150,  -7.8084,\n",
      "         -7.0604, -10.6685,  -5.6394,  -8.1794,  -2.6505,  -9.4130, -11.3430,\n",
      "        -10.6113,  -6.0576, -11.5585,  -5.1952,  -9.3726,  -6.6047,  -7.0396,\n",
      "         -5.2136,  -8.3791,  -5.4716, -10.3538,  -5.4073,  -8.9325,  -4.6469,\n",
      "        -10.8105,  -9.5773, -13.2281,  -6.0915,  -4.9639,  -5.4789,  -9.0624,\n",
      "        -11.1977, -10.1689,  -6.5217,  -6.9838,  -9.3961,  -6.1383, -11.2057,\n",
      "        -10.7562,  -3.9677, -10.2493,  -8.5001,  -8.9039], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(53, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(possibilities.size())\n",
    "print(possibilities[0])\n",
    "print(possibilities[0].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "tensor([86, 22], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(true_possibilities.size())\n",
    "print(true_possibilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Generative Adversarial Networks\n",
    "\n",
    "## We can obtain an AI capable of detecting AI-generated texts, while also making an AI capable of generating realistic texts\n",
    "\n",
    "\n",
    "**TEXT ----> WERNICKE AREA: PROCESS INFORMATION (What does it mean?)**\n",
    "\n",
    "**WERNICKE ------> ASSOCIATIVE CORTEX(TEMPORAL + PARIETAL?) -----> BROCA AREA: GENERATES NEW(?) INFORMATION ---> MOTOR CORTEX (PRECENTRAL GYRUS)**\n",
    "\n",
    "\n",
    "*Question: Can she detect metaphores, implicit messages, sense of humour, poetry? Such things aren't as simple as \"Word ---> Meaning\", afterall.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
